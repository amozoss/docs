<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="File Redundancy #  Durability and expansion factor  #  In a decentralized storage network, any storage node could go offline permanently at any time. A storage network’s redundancy strategy must store data in a way that provides access with high probability, even though any given number of individual nodes may be in an offline state. To achieve a specific level of durability (defined as the probability that data remains available in the face of failures), many products in this space (Filecoin, MaidSafe, Siacoin, GFS, Ceph, IPFS, etc."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content="File Redundancy"><meta property="og:description" content="File Redundancy #  Durability and expansion factor  #  In a decentralized storage network, any storage node could go offline permanently at any time. A storage network’s redundancy strategy must store data in a way that provides access with high probability, even though any given number of individual nodes may be in an offline state. To achieve a specific level of durability (defined as the probability that data remains available in the face of failures), many products in this space (Filecoin, MaidSafe, Siacoin, GFS, Ceph, IPFS, etc."><meta property="og:type" content="article"><meta property="og:url" content="/dcs/concepts/file-redundancy/"><meta property="article:section" content="dcs"><title>File Redundancy | Storj Docs</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.f89e84d24aa67a9627cfb7a3b446b3b062a451eebceb8d5f060601553c0a6cfb.css><script defer src=/flexsearch.min.js></script>
<script defer src=/en.search.min.afc156b59ccdf08ee689010e2667621d6ac7b00f30a969882371f26dd69b9b61.js></script>
<script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js></script>
<link rel=subresource href=/fonts/Inter-roman.var.woff2><link rel=subresource href=/fonts/Inter-italic.var.woff2></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Storj Docs</span></a></h2><div class=book-search><input type=text id=book-search-input data-filter=/dcs/ placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul class=book-first-sections><li class=active><a href=/dcs/>DCS</a></li><li><a href=/node/>Node Operator</a></li></ul><ul><li class=book-section-flat><span>Decentralized Cloud Storage</span><ul><li><a href=/dcs/storage/considerations/>Product Overview</a></li></ul></li><li class=book-section-flat><span>Downloads</span><ul><li><a href=/dcs/downloads/download-uplink-cli/>Download Uplink CLI</a></li><li><a href=/dcs/downloads/download-self-hosted-s3-compatible-gateway/>Download Self-hosted S3 Compatible Gateway</a></li><li><a href=/dcs/downloads/download-storj-client-libraries/>Download Storj Client Libraries</a></li></ul></li><li class=book-section-flat><span>Getting Started</span><ul><li><a href=/dcs/getting-started/quickstart-guide/>Quickstart Guide</a></li><li><a href=/dcs/getting-started/quickstart-objectbrowser/>Quickstart - Object Browser</a></li><li><input type=checkbox id=section-8a2bc98c91c60155c8bcc6967e5b3cf7 class=toggle>
<label for=section-8a2bc98c91c60155c8bcc6967e5b3cf7 class="flex justify-between"><a href=/dcs/getting-started/gateway-mt/>Quickstart - AWS CLI and Hosted Gateway MT</a></label><ul><li><a href=/dcs/getting-started/gateway-mt/aws-cli-advanced-options/>AWS CLI Advanced Options</a></li></ul></li><li><a href=/dcs/getting-started/quickstart-aws-sdk-and-hosted-gateway-mt/>Quickstart - AWS SDK and Hosted Gateway MT</a></li><li><input type=checkbox id=section-eb51eff27101399663fec0a5feb790b7 class=toggle>
<label for=section-eb51eff27101399663fec0a5feb790b7 class="flex justify-between"><a href=/dcs/getting-started/quickstart-uplink-cli/>Quickstart - Uplink CLI</a></label><ul><li><a href=/dcs/getting-started/quickstart-uplink-cli/prerequisites/>Prerequisites</a></li><li><input type=checkbox id=section-1cc3e1bb03e2e308966123bb1cc84dfc class=toggle>
<label for=section-1cc3e1bb03e2e308966123bb1cc84dfc class="flex justify-between"><a href=/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/>Uploading Your First Object CLI</a></label><ul><li><a href=/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/create-first-access-grant/>Create an Access Grant</a></li><li><a href=/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/set-up-uplink-cli/>Set Up Uplink CLI with Access Grant</a></li><li><a href=/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/create-a-bucket/>Create a Bucket</a></li><li><a href=/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/upload-an-object/>Upload an Object</a></li><li><a href=/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/view-distribution-of-an-object/>View Distribution of an Object</a></li></ul></li><li><input type=checkbox id=section-ef98e9c48f14a1e6e3291b7ffdd021df class=toggle>
<label for=section-ef98e9c48f14a1e6e3291b7ffdd021df class="flex justify-between"><a href=/dcs/getting-started/quickstart-uplink-cli/interacting-with-your-first-object/>Interacting With Your First Object CLI</a></label><ul><li><a href=/dcs/getting-started/quickstart-uplink-cli/interacting-with-your-first-object/list-an-object/>List an Object</a></li><li><a href=/dcs/getting-started/quickstart-uplink-cli/interacting-with-your-first-object/download-an-object/>Download an Object</a></li><li><a href=/dcs/getting-started/quickstart-uplink-cli/interacting-with-your-first-object/delete-an-object/>Delete an Object</a></li></ul></li><li><input type=checkbox id=section-583035221fd089be3bccd9566e3d01dd class=toggle>
<label for=section-583035221fd089be3bccd9566e3d01dd class="flex justify-between"><a href=/dcs/getting-started/quickstart-uplink-cli/sharing-your-first-object/>Sharing Your First Object</a></label><ul><li><a href=/dcs/getting-started/quickstart-uplink-cli/sharing-your-first-object/prerequisites/>Prerequisites</a></li><li><a href=/dcs/getting-started/quickstart-uplink-cli/sharing-your-first-object/generate-access/>Create an Access to an Object</a></li><li><a href=/dcs/getting-started/quickstart-uplink-cli/sharing-your-first-object/import-access/>Import an Access to an Object</a></li><li><a href=/dcs/getting-started/quickstart-uplink-cli/sharing-your-first-object/revoke-an-access-to-an-object/>Revoke an Access to an Object</a></li></ul></li><li><input type=checkbox id=section-5272dcaf3a9682609c7d3fdd368cf961 class=toggle>
<label for=section-5272dcaf3a9682609c7d3fdd368cf961 class="flex justify-between"><a href=/dcs/getting-started/quickstart-uplink-cli/generate-access-grants-and-tokens/>Advanced Usage</a></label><ul><li><a href=/dcs/getting-started/quickstart-uplink-cli/generate-access-grants-and-tokens/generate-a-token/>Create Access Grant in CLI</a></li></ul></li></ul></li><li><input type=checkbox id=section-e035376383ed4301f1acc99452a9495f class=toggle>
<label for=section-e035376383ed4301f1acc99452a9495f class="flex justify-between"><a href=/dcs/getting-started/satellite-developer-account/>Quickstart - Satellite Admin Console</a></label><ul><li><a href=/dcs/getting-started/satellite-developer-account/creating-your-account/>Creating Your Account</a></li><li><a href=/dcs/getting-started/satellite-developer-account/manage-projects/>Manage Projects</a></li><li><a href=/dcs/getting-started/satellite-developer-account/dashboard/>Dashboard</a></li><li><a href=/dcs/getting-started/satellite-developer-account/objects/>Buckets</a></li><li><a href=/dcs/getting-started/satellite-developer-account/access-grants/>Access</a></li><li><a href=/dcs/getting-started/satellite-developer-account/users/>Users</a></li><li><a href=/dcs/getting-started/satellite-developer-account/billing/>Billing</a></li><li><a href=/dcs/getting-started/satellite-developer-account/resources/>Resources</a></li><li><a href=/dcs/getting-started/satellite-developer-account/quick-start/>Quick Start</a></li><li><a href=/dcs/getting-started/satellite-developer-account/my-account/>My Account</a></li></ul></li></ul></li><li class=book-section-flat><span>SDK & Reference</span><ul><li><input type=checkbox id=section-e71eaa406f51abd64ce74a83dbe5dd13 class=toggle>
<label for=section-e71eaa406f51abd64ce74a83dbe5dd13 class="flex justify-between"><a href=/dcs/api-reference/s3-compatible-gateway/>Storj-hosted S3 Compatible Gateway</a></label><ul><li><input type=checkbox id=section-5fc59ab320b4677ae3ebd53fe7b10db5 class=toggle>
<label for=section-5fc59ab320b4677ae3ebd53fe7b10db5 class="flex justify-between"><a href=/dcs/api-reference/s3-compatible-gateway/multipart-upload/>Multipart Upload</a></label><ul><li><a href=/dcs/api-reference/s3-compatible-gateway/multipart-upload/multipart-part-size/>Multipart Part Size</a></li></ul></li><li><a href=/dcs/api-reference/s3-compatible-gateway/using-presigned-urls/>Using presigned URLs</a></li><li><a href=/dcs/api-reference/s3-compatible-gateway/supported-s3-commands/>Supported S3 Commands</a></li></ul></li><li><input type=checkbox id=section-e534a50d47604ebc17cdb3d09a7bbe5c class=toggle>
<label for=section-e534a50d47604ebc17cdb3d09a7bbe5c class="flex justify-between"><a href=/dcs/api-reference/uplink-cli/>Uplink CLI</a></label><ul><li><input type=checkbox id=section-ab2e55a365604a01b5cad7151a01b544 class=toggle>
<label for=section-ab2e55a365604a01b5cad7151a01b544 class="flex justify-between"><a href=/dcs/api-reference/uplink-cli/access-command/>access</a></label><ul><li><a href=/dcs/api-reference/uplink-cli/access-command/access-inspect-command/>access inspect</a></li><li><a href=/dcs/api-reference/uplink-cli/access-command/access-list-command/>access list</a></li><li><a href=/dcs/api-reference/uplink-cli/access-command/access-register/>access register</a></li></ul></li><li><a href=/dcs/api-reference/uplink-cli/cp-command/>cp</a></li><li><a href=/dcs/api-reference/uplink-cli/import-command/>import</a></li><li><a href=/dcs/api-reference/uplink-cli/ls-command/>ls</a></li><li><a href=/dcs/api-reference/uplink-cli/uplink-mb-command/>mb</a></li><li><input type=checkbox id=section-12823ba9c45a10b81b0eaecb4dfc8ebb class=toggle>
<label for=section-12823ba9c45a10b81b0eaecb4dfc8ebb class="flex justify-between"><a href=/dcs/api-reference/uplink-cli/meta-command/>meta</a></label><ul><li><a href=/dcs/api-reference/uplink-cli/meta-command/meta-get-command/>meta get</a></li></ul></li><li><a href=/dcs/api-reference/uplink-cli/mv/>mv</a></li><li><a href=/dcs/api-reference/uplink-cli/rb-command/>rb</a></li><li><a href=/dcs/api-reference/uplink-cli/rm-command/>rm</a></li><li><a href=/dcs/api-reference/uplink-cli/setup-command/>setup</a></li><li><a href=/dcs/api-reference/uplink-cli/share-command/>share</a></li></ul></li><li><input type=checkbox id=section-03ffd000c84f3189e27b539e51443e62 class=toggle>
<label for=section-03ffd000c84f3189e27b539e51443e62 class="flex justify-between"><a href=/dcs/api-reference/s3-gateway/>Self-hosted S3 Compatible Gateway</a></label><ul><li><a href=/dcs/api-reference/s3-gateway/gateway-st-advanced-usage/>Gateway ST Advanced Usage</a></li></ul></li><li><a href=/dcs/api-reference/linksharing-service/>Linksharing Service</a></li><li><input type=checkbox id=section-0848317479e2fc58b812eefd4912b4a4 class=toggle>
<label for=section-0848317479e2fc58b812eefd4912b4a4 class="flex justify-between"><a href=/dcs/api-reference/storj-client-libraries/>Storj Client Libraries</a></label><ul><li><a href=https://github.com/storj/uplink class=book-external-link target=_blank rel=noopener>Go<img src=/svg/external-link.svg class=book-icon alt="External Link"></a></li><li><a href=https://github.com/storj/uplink-c class=book-external-link target=_blank rel=noopener>C<img src=/svg/external-link.svg class=book-icon alt="External Link"></a></li><li><a href=https://github.com/storj/uplink-android class=book-external-link target=_blank rel=noopener>Android<img src=/svg/external-link.svg class=book-icon alt="External Link"></a></li><li><a href=https://github.com/storj/uplink-java class=book-external-link target=_blank rel=noopener>Java<img src=/svg/external-link.svg class=book-icon alt="External Link"></a></li><li><a href=https://www.npmjs.com/package/uplink-nodejs class=book-external-link target=_blank rel=noopener>NodeJS<img src=/svg/external-link.svg class=book-icon alt="External Link"></a></li></ul></li></ul></li><li class=book-section-flat><span>How To's</span><ul><li><a href=/dcs/how-tos/container-registry-docker/>Container Registry - Docker</a></li><li><input type=checkbox id=section-591e24e11327becce048ce6a5d0b3e18 class=toggle>
<label for=section-591e24e11327becce048ce6a5d0b3e18 class="flex justify-between"><a href=/dcs/how-tos/host-a-static-website/>Host a Static Website</a></label><ul><li><a href=/dcs/how-tos/host-a-static-website/host-a-static-website-with-the-cli-and-linksharing-service/>Host a Static Website with the Uplink CLI and Linksharing Service</a></li></ul></li><li><a href=/dcs/how-tos/configure-tools-for-the-partner-program/>Configure Tools for the Partner Program</a></li><li><a href=/dcs/how-tos/fastly-integration/>Fastly Integration</a></li><li><a href=https://filebase.com class=book-external-link target=_blank rel=noopener>Configure Filebase for easy upload<img src=/svg/external-link.svg class=book-icon alt="External Link"></a></li><li><a href=/dcs/how-tos/backup-with-duplicati/>Backup With Duplicati</a></li><li><a href=/dcs/how-tos/nft-storage/>NFT storage for OpenSea</a></li><li><input type=checkbox id=section-8fe4791a1117b6b147a551b2324e5838 class=toggle>
<label for=section-8fe4791a1117b6b147a551b2324e5838 class="flex justify-between"><a href=/dcs/how-tos/sync-files-with-rclone/>Sync Files With Rclone</a></label><ul><li><a href=/dcs/how-tos/sync-files-with-rclone/rclone-with-native-integration/>Rclone with Native Integration</a></li><li><a href=/dcs/how-tos/sync-files-with-rclone/rclone-with-hosted-gateway/>Rclone with Hosted Gateway</a></li></ul></li><li><a href=/dcs/how-tos/backup-with-restic/>Backup With Restic</a></li><li><a href=/dcs/how-tos/set-up-filezilla-for-decentralized-file-transfer/>FileZilla Native Integration</a></li><li><a href=/dcs/how-tos/filezilla-pro-integration-guide/>Filezilla Pro Integration Guide</a></li><li><a href=/dcs/how-tos/mongodb-ops-manager-backup/>MongoDB Ops Manager Backup</a></li><li><a href=/dcs/how-tos/tesla-sentry-mode-teslausb/>Store Tesla Sentry Mode & Dashcam videos on Storj DCS</a></li><li><a href=/dcs/how-tos/how-to-connect-s3fs-to-storj-dcs/>How to connect s3fs to Storj DCS</a></li><li><a href=/dcs/how-tos/how-to-set-up-video-analytics-and-video-management-with-kerberos-vault/>How to set up video analytics and video management with Kerberos Vault</a></li><li><a href=/dcs/how-tos/how-to-hod-rod-file-transfer-performance-on-storj-dcs/>How to Hod Rod File Transfer Performance on Storj DCS</a></li><li><a href=/dcs/how-tos/backups-with-hashbackup/>Backups with HashBackup</a></li><li><a href=/dcs/how-tos/how-to-use-plex-and-storj-dcs-as-a-private-streaming-service/>How to use Plex and Storj DCS as a private streaming service</a></li><li><a href=/dcs/how-tos/how-to-use-cyberduck-and-storj-dcs/>How to use Cyberduck and Storj DCS</a></li><li><a href=/dcs/how-tos/storj-ipfs-pinning-service-beta/>Storj IPFS Pinning Service (Beta)</a></li><li><a href=/dcs/how-tos/simplify-file-management-with-s3-browser-and-storj-dcs/>Simplify File Management with S3 Browser and Storj DCS</a></li><li><a href=/dcs/how-tos/kubernetes-backup-via-velero/>Kubernetes Backup via Velero</a></li></ul></li><li class=book-section-flat><span>Solution Architectures</span><ul><li><a href=/dcs/solution-architectures/common-use-cases/>Common Use Cases</a></li><li><a href=/dcs/solution-architectures/common-architectural-patterns/>Common Architectural Patterns</a></li></ul></li><li class=book-section-flat><span>Concepts</span><ul><li><a href=/dcs/concepts/overview/>Understanding Storj DCS</a></li><li><a href=/dcs/concepts/definitions/>Definitions</a></li><li><a href=/dcs/concepts/key-architecture-constructs/>Key Architecture Constructs</a></li><li><input type=checkbox id=section-e3983b48cc8585311f4fd92a3441c885 class=toggle>
<label for=section-e3983b48cc8585311f4fd92a3441c885 class="flex justify-between"><a href=/dcs/concepts/access/>Access Management</a></label><ul><li><input type=checkbox id=section-2aaccce9e50f91dfc9ae1f4f2919e73d class=toggle>
<label for=section-2aaccce9e50f91dfc9ae1f4f2919e73d class="flex justify-between"><a href=/dcs/concepts/access/access-grants/>Access Grants</a></label><ul><li><input type=checkbox id=section-6532f8fab18a0bf2730b54084407ef27 class=toggle>
<label for=section-6532f8fab18a0bf2730b54084407ef27 class="flex justify-between"><a href=/dcs/concepts/access/access-grants/api-key/>API Key</a></label><ul><li><a href=/dcs/concepts/access/access-grants/api-key/restriction/>Access Restrictions</a></li></ul></li><li><a href=/dcs/concepts/access/access-grants/when-to-use-the-satellite-web-interface-and-when-to-use-the-cli/>When to use the Satellite Web Interface and When to use the CLI</a></li></ul></li><li><input type=checkbox id=section-d17166aeaa257fe6cfe34e9469861364 class=toggle>
<label for=section-d17166aeaa257fe6cfe34e9469861364 class="flex justify-between"><a href=/dcs/concepts/access/encryption-and-keys/>Encryption Keys</a></label><ul><li><a href=/dcs/concepts/access/encryption-and-keys/key-management/>Key Management</a></li><li><a href=/dcs/concepts/access/encryption-and-keys/when-to-use-different-encryption-keys/>When to use different encryption keys</a></li></ul></li><li><a href=/dcs/concepts/access/access-revocation/>Access Revocation</a></li><li><a href=/dcs/concepts/access/capability-based-access-control/>Capability Based Access vs Access Control Lists</a></li><li><a href=/dcs/concepts/access/access-management-at-the-edge/>Access Management at the Edge</a></li></ul></li><li><input type=checkbox id=section-32bee2047eb722c84da55e9b4fbd0a7e class=toggle>
<label for=section-32bee2047eb722c84da55e9b4fbd0a7e class="flex justify-between"><a href=/dcs/concepts/edge-services/>Edge Services</a></label><ul><li><a href=/dcs/concepts/edge-services/auth-service/>Auth Service</a></li></ul></li><li><a href=/dcs/concepts/connectors/>Connectors</a></li><li><a href=/dcs/concepts/data-structure/>Data Structure</a></li><li><input type=checkbox id=section-273072f64771f53acb81a101acf9cdb9 class=toggle>
<label for=section-273072f64771f53acb81a101acf9cdb9 class="flex justify-between"><a href=/dcs/concepts/encryption-key/>Encryption</a></label><ul><li><a href=/dcs/concepts/encryption-key/how-encryption-is-implemented/>How Encryption is Implemented</a></li><li><a href=/dcs/concepts/encryption-key/design-decision-end-to-end-encryption/>Design Decision: End-to-end Encryption</a></li><li><a href=/dcs/concepts/encryption-key/design-decision-server-side-encryption/>Design Decision: Server-side Encryption</a></li></ul></li><li><input type=checkbox id=section-390f7f447019f4755db05aad8ca13661 class=toggle>
<label for=section-390f7f447019f4755db05aad8ca13661 class="flex justify-between"><a href=/dcs/concepts/decentralization/>Decentralization</a></label><ul><li><a href=/dcs/concepts/decentralization/coordination-avoidance/>Coordination Avoidance</a></li></ul></li><li><a href=/dcs/concepts/file-redundancy/ class=active>File Redundancy</a></li><li><a href=/dcs/concepts/multiregion-availability/>Multiregion Availability</a></li><li><a href=/dcs/concepts/limits/>Usage Limits</a></li><li><a href=/dcs/concepts/satellite/>Satellite (Metadata Region)</a></li><li><a href=/dcs/concepts/s3-compatibility/>S3 Compatibility</a></li><li><a href=/dcs/concepts/file-repair/>File Repair</a></li></ul></li><li class=book-section-flat><span>Support</span><ul><li><a href=/dcs/support/support-process-overview/>Support Overview</a></li><li><a href=/dcs/support/faqs/>FAQ</a></li><li><a href=https://forum.storj.io/c/engineer-amas/dev-category class=book-external-link target=_blank rel=noopener>Community Forum<img src=/svg/external-link.svg class=book-icon alt="External Link"></a></li><li><a href=https://status.tardigrade.io class=book-external-link target=_blank rel=noopener>Status Page<img src=/svg/external-link.svg class=book-icon alt="External Link"></a></li><li><a href=https://support.tardigrade.io class=book-external-link target=_blank rel=noopener>Help Desk<img src=/svg/external-link.svg class=book-icon alt="External Link"></a></li></ul></li><li class=book-section-flat><span>Billing, Payment & Accounts</span><ul><li><input type=checkbox id=section-e86e5160b6cbbb979535071ad9af4793 class=toggle>
<label for=section-e86e5160b6cbbb979535071ad9af4793 class="flex justify-between"><a href=/dcs/billing-payment-and-accounts-1/pricing/>Billing, Payment and Accounts</a></label><ul><li><a href=/dcs/billing-payment-and-accounts-1/pricing/billing-and-payment/>How Billing is Calculated</a></li><li><a href=/dcs/billing-payment-and-accounts-1/pricing/free-tier/>Free Plan</a></li><li><a href=/dcs/billing-payment-and-accounts-1/pricing/usage-limit-increases/>Usage Limit Increases</a></li></ul></li><li><input type=checkbox id=section-4e3c83ea13e2e7ae2025639618d7fec3 class=toggle>
<label for=section-4e3c83ea13e2e7ae2025639618d7fec3 class="flex justify-between"><a href=/dcs/billing-payment-and-accounts-1/storj-token/>Payment Methods</a></label><ul><li><a href=/dcs/billing-payment-and-accounts-1/storj-token/promotional-credits/>Promotional Credits</a></li><li><a href=/dcs/billing-payment-and-accounts-1/storj-token/debits-against-payment-methods/>Debits Against Payment Methods</a></li><li><a href=/dcs/billing-payment-and-accounts-1/storj-token/changing-payment-methods/>Changing Payment Methods</a></li><li><a href=/dcs/billing-payment-and-accounts-1/storj-token/deleting-a-payment-method/>Deleting a Payment Method</a></li><li><a href=/dcs/billing-payment-and-accounts-1/storj-token/expired-credit-card/>Expired Credit Card</a></li><li><a href=/dcs/billing-payment-and-accounts-1/storj-token/reporting-a-payment-problem/>Reporting a Payment Problem</a></li></ul></li><li><a href=/dcs/billing-payment-and-accounts-1/requesting-a-refund/>Requesting a Refund</a></li><li><a href=/dcs/billing-payment-and-accounts-1/closing-an-account/>Closing an Account</a></li><li><a href=/dcs/billing-payment-and-accounts-1/data-retention-policy/>Data Retention Policy</a></li></ul></li><li class=book-section-flat><span>Resources</span><ul><li><a href=https://www.storj.io/storjv3.pdf class=book-external-link target=_blank rel=noopener>Whitepaper<img src=/svg/external-link.svg class=book-icon alt="External Link"></a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label>
<strong>File Redundancy</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><div class=toc-title>Contents</div><nav id=TableOfContents><ul><li><ul><li><a href=#durability-and-expansion-factor-a-hrefdurability-and-expansion-factor-iddurability-and-expansion-factora>Durability and expansion factor <a href=durability-and-expansion-factor id=durability-and-expansion-factor></a></a></li><li><a href=#erasure-code-a-hreferasure-codes-iderasure-codesa>Erasure Code <a href=erasure-codes id=erasure-codes></a></a></li><li><a href=#okay-erasure-codes-take-less-disk-space-but-isnt-repairing-data-more-expensive-a-hrefokay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensive-idokay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensivea>Okay, erasure codes take less disk space. But isn’t repairing data more expensive? <a href=okay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensive id=okay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensive></a></a></li><li><a href=#downsides-a-hrefdownsides-iddownsidesa>Downsides? <a href=downsides id=downsides></a></a></li><li><a href=#upsides-a-hrefupsides-idupsidesa>Upsides? <a href=upsides id=upsides></a></a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=file-redundancy>File Redundancy
<a class=anchor href=#file-redundancy>#</a></h1><h3 id=durability-and-expansion-factor-a-hrefdurability-and-expansion-factor-iddurability-and-expansion-factora>Durability and expansion factor <a href=durability-and-expansion-factor id=durability-and-expansion-factor></a>
<a class=anchor href=#durability-and-expansion-factor-a-hrefdurability-and-expansion-factor-iddurability-and-expansion-factora>#</a></h3><p>In a decentralized storage network, any storage node could go offline permanently at any time. A storage network’s redundancy strategy must store data in a way that provides access with high probability, even though any given number of individual nodes may be in an offline state. To achieve a specific level of <em>durability</em> (defined as the probability that data remains available in the face of failures), many products in this space (Filecoin, MaidSafe, Siacoin, GFS, Ceph, IPFS, etc.) by default use replication, which means simply having multiple copies of the data stored on different nodes.</p><p>Unfortunately, replication anchors durability to the network <em>expansion factor</em>, which is the storage overhead for reliably storing data. If you want more durability, you need more copies. For every increase of durability you desire, you must spend another multiple of the data size in bandwidth when storing or repairing the data, as nodes churn in and out of the network.</p><p>For example, suppose your desired durability level requires a replication strategy that makes eight copies of the data. This yields an expansion factor of 8x, or 800%. This data then needs to be stored on the network, using bandwidth in the process. Thus, more replication results in more bandwidth usage for a fixed amount of data.</p><p>On the one hand, replication does make network maintenance simpler. If a node goes offline, only one of the other storage nodes is needed to bring a new replacement node into the fold. On the other hand, for every node that is added to the redundancy pool, 100% of the replicated data must be transferred.</p><h3 id=erasure-code-a-hreferasure-codes-iderasure-codesa>Erasure Code <a href=erasure-codes id=erasure-codes></a>
<a class=anchor href=#erasure-code-a-hreferasure-codes-iderasure-codesa>#</a></h3><p>Erasure codes are another redundancy approach, and importantly, they do not tie durability to the expansion factor. You can tune your durability without increasing the overall network traffic!</p><p>Erasure codes are widely used in both distributed and peer-to-peer storage systems. While they are more complicated and possess trade-offs of their own, the scheme we adopt, Reed-Solomon, has been around since 1960 and is used everywhere from CDs, deep space communication, barcodes, advanced RAID-like applications–you name it.</p><p>An erasure code is often described by two numbers, <em>k</em> and <em>n</em>. If a block of data is encoded with a <em>k</em>, <em>n</em> erasure code, there are <em>n</em> total generated erasure shares, where only any <em>k</em> of them are required to recover the original block of data! It doesn’t matter if you recover all of the even numbered shares, all of the odd numbered shares, the first <em>k</em> shares, the last <em>k</em> shares, whatever. Any <em>k</em> shares can recover the original block.</p><p>If a block of data is <em>s</em> bytes large, each of the <em>n</em> erasure shares are roughly <em>s</em>/<em>k</em> bytes. For example, if a block is 10 MB, and you’re using a <em>k</em> = 10, <em>n</em> = 20 erasure code scheme, each erasure share of that block will only be 1 MB. This means that with <em>k</em> = 10, <em>n</em> = 20, the expansion factor is only 2x. For a 10 MB block, only 20 MB total is used, because there are twenty 1-MB shares. The same expansion factor holds with <em>k</em> = 20, <em>n</em> = 40, where there are forty 512-KB shares.</p><p>Interestingly, the durability of a <em>k</em> = 20, <em>n</em> = 40 erasure code is better than a <em>k</em> = 10, <em>n</em> = 20 erasure code, even though the expansion factor (2x) is the same for both. This is because the risk is spread across more nodes in the <em>k</em> = 20, <em>n</em> = 40 case. To help drive this point home, we calculated the durability for various erasure code configuration choices in a network with a churn of 10%. We talked more about the math behind this table in section 3.4 of
<a href=https://storj.io/storjv3.pdf>our paper</a>, and we’ll discuss more about churn in an upcoming blog post, but suffice it to say, we hope these calculated values are illustrative:</p><p><img src=https://storj.io/blog/img/stats1.png alt></p><p>Notice how increasing the amount of storage nodes involved increases the amount of durability significantly (each new 9 is 10x more durable), without a change in the expansion factor. We also put this data together in a graph:</p><p><img src=https://storj.io/blog/img/graph.png alt></p><p>Admittedly, this graph is a little disingenuous: the chances of you caring about having thirty-two 9s of durability is… low, to say the least. The National Weather Service
<a href=https://www.weather.gov/safety/lightning-odds>estimates</a> the likelihood of you not getting hit by lightning this year at only six 9s after all. But you should still be able to see that a <em>k</em> = 2, <em>n</em> = 4 is less durable than a <em>k</em> = 16, <em>n</em> = 32 configuration.</p><p>In contrast, replication requires significantly higher expansion factors for the same durability. The following table shows durability with a replication scheme:</p><p><img src=https://storj.io/blog/img/stats2.png alt></p><p>Comparing the two tables, notice that replicating data at 10x can’t beat erasure codes with <em>k</em> = 16, <em>n</em> = 32, which is an expansion factor of only two. For durable storage, erasure codes simply require ridiculously less disk space than replication.</p><p>If you want to learn more about how erasure codes work, you can read
<a href=https://innovation.vivint.com/introduction-to-reed-solomon-bc264d0794f8>this introductory tutorial I co-wrote last year.</a></p><h3 id=okay-erasure-codes-take-less-disk-space-but-isnt-repairing-data-more-expensive-a-hrefokay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensive-idokay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensivea>Okay, erasure codes take less disk space. But isn’t repairing data more expensive? <a href=okay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensive id=okay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensive></a>
<a class=anchor href=#okay-erasure-codes-take-less-disk-space-but-isnt-repairing-data-more-expensive-a-hrefokay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensive-idokay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensivea>#</a></h3><p>It’s true that replication makes repair <em>simpler</em>. Every time a node is lost, only one of the remaining nodes is necessary for recovery. On the flip side, erasure codes require several nodes to be involved for each repair. Though this feels like a problem, it’s actually not.</p><p>To understand why, let’s set up both scenarios, replication at 9x and erasure codes at <em>k</em> = 18, <em>n</em> = 36, and consider what it costs us. These numbers are chosen because they have similar durability (9x replication has six 9s of durability assuming 10% of node churn, and <em>k</em> = 18, <em>n</em> = 36 erasure coding has seven). We’ll consider what happens when we are storing a data block that is 18 MB and we suddenly lose one-third of our nodes.</p><p>At 9x, replication in our model of course has an expansion factor of 9. Once again, replication is the simplest to implement. If we lose one-third of our nine nodes we will need to spin up three new nodes. Each new node transfers a copy of the lost data, which means that each node must transfer 18 MB. That’s a total of 54 MB of bandwidth usage for repair. No intensive CPU processing was needed.</p><p>With <em>k</em> = 18, <em>n</em> = 36 erasure codes (with an expansion factor of only two), losing one-third of our nodes means we now only have 24 nodes still available and need to repair to twelve new nodes. The data each node is storing is only 1 MB each, but eighteen nodes must be contacted to rebuild the data. Let’s designate one of the nodes to rebuild the data. It will download eighteen 1 MB pieces, reconstruct the original file, then store the missing twelve 1 MB pieces on new nodes. If this designated node is one of the new nodes, we can avoid one of the transfers. The total overall bandwidth used is at most 30 MB, which is almost half of the replication scenario. This advantage in bandwidth savings becomes even wider with higher durabilities.</p><h3 id=downsides-a-hrefdownsides-iddownsidesa>Downsides? <a href=downsides id=downsides></a>
<a class=anchor href=#downsides-a-hrefdownsides-iddownsidesa>#</a></h3><p>Erasure coding did require more CPU time, that’s true. Still, a reasonable erasure encoding library can generate encoded data at at least 650 MB/s, which is unlikely to be the major throughput bottleneck over a wide-area network with unreliable storage nodes.</p><p>Erasure coding also required a designated node to do the repair. While this complicates architectures in untrusted environments, it is not an unsolvable problem. It simply requires the addition of hashes, signatures, and retries in a few new places. This is something we’ll talk about more down the road. We have a lot of blog posts to write!</p><p>Notably, erasure coding does <em>not</em> complicate streaming. Remember how I said erasure codes are used for satellite communication and CDs? As long as erasure coding is batched into small operations, streaming continues to work just fine. See Figure 4.2 and sections 4.1.2 and 4.8 in
<a href=https://storj.io/storjv3.pdf>our white paper</a> for more details about how we can pull native video streaming off.</p><h3 id=upsides-a-hrefupsides-idupsidesa>Upsides? <a href=upsides id=upsides></a>
<a class=anchor href=#upsides-a-hrefupsides-idupsidesa>#</a></h3><p>Comparing 9x replication and <em>k</em> = 18, <em>n</em> = 36 erasure coding, the latter uses less than half the overall bandwidth for repair. It also uses less than a third of the bandwidth for storage and takes up less than a third of the disk space. It is roughly ten times more durable! Holy crap!</p><p>Oh, and did I mention this also enables us to pay storage node operators more? Specifically over three times more? Because the disk-space usage is more efficient, there is more money available for each storage node operator. The income is less diluted across storage nodes, you could say.</p><p>It’s worth re-reading those last two paragraphs. These gains are significant. Hopefully by this point you’re convinced that erasure codes are better.</p><p>In this edition we dive deeper into why nodes joining and leaving the network - also known as churn - has a much more significant (and also bad) impact on a redundancy strategy that relies on replication. We make the case that using replication in a high-churn environment is not only impractical, but inevitably doomed to fail. Quoting
<a href=http://www.eecs.harvard.edu/%5c~margo/cs261/papers-a1/blake.pdf>Blake and Rodrigues</a>, “Data redundancy is the key to any data guarantees. However, preserving redundancy in the face of highly dynamic membership is costly.”</p><h4 id=an-aside-on-dynamics-a-hrefan-aside-on-dynamics-idan-aside-on-dynamicsa>An Aside on Dynamics <a href=an-aside-on-dynamics id=an-aside-on-dynamics></a>
<a class=anchor href=#an-aside-on-dynamics-a-hrefan-aside-on-dynamics-idan-aside-on-dynamicsa>#</a></h4><p>Before diving into the exciting math parts, we need to quickly define a couple of concepts relating to network dynamics. The lifetime of a node is the duration between it joining and leaving the system for whatever reason. A network made of several nodes has an average lifetime, commonly called the mean time to failure (MTTF). The inverse of mean time to failure is churn rate or frequency of failure-per-unit-of-time. It’s an important relationship to understand, especially when MTTF is a unit of time much greater than the units needed for a specific problem.</p><p>Distributed storage systems have mechanisms to repair data by replacing the pieces that become unavailable due to node churn. However, in distributed cloud storage systems, file repair incurs a cost for the bandwidth utilized during the repair process. Regardless of whether file pieces are simply replicated, or whether erasure coding is used to recreate missing pieces, the file repair process requires pieces to be downloaded from available nodes and uploaded to other uncorrelated and available nodes.</p><p>After reading part one of this series, it’s clearly not feasible to rely on replication alone, but some projects have proposed combining erasure coding and replication. Once you’ve erasure coded a file and distributed it across a set of nodes, it’s going to have a defined durability for a given level of node churn. If you want to increase that durability for that given level of node churn, you have two choices: increase the erasure code k/n ratio or use replication to make copies of the erasure coded pieces. These two strategies are very different and have a huge impact on the network beyond just increasing durability.</p><h4 id=our-hypothetical-networks-a-hrefour-hypothetical-networks-idour-hypothetical-networksa>Our Hypothetical Networks <a href=our-hypothetical-networks id=our-hypothetical-networks></a>
<a class=anchor href=#our-hypothetical-networks-a-hrefour-hypothetical-networks-idour-hypothetical-networksa>#</a></h4><p>So, let’s define two hypothetical distributed storage networks, one using erasure coding alone for redundancy (the approach used on Storj’s V3 network), and one using erasure coding plus replication for redundancy (which is the approach used by Filecoin as well as the previous, depreciated Storj network). We will assume nodes on both networks are free to join and leave at any time, and that uptime for nodes can be highly variable based on the hardware, OS, available bandwidth, and a variety of other factors. When a node leaves a network, the pieces of data on that node become permanently unavailable. Of course, if nodes fall below a certain threshold of availability in a given month, the impact on the overall availability of files is effectively equivalent to the node leaving the network altogether.</p><p>Let’s also assume both hypothetical networks use a 4⁄8 Reed-Solomon erasure code ratio and have 99.9% durability with node churn at 10%. Both networks want to achieve eleven 9s of durability though. One is going to achieve it through erasure coding alone, and the other is going to combine erasure coding with replication.</p><h4 id=and-now-some-math-a-hrefand-now-some-math-idand-now-some-matha>And Now, Some Math <a href=and-now-some-math id=and-now-some-math></a>
<a class=anchor href=#and-now-some-math-a-hrefand-now-some-math-idand-now-some-matha>#</a></h4><p>As it turns out, if you know the target durability, you know the MTTF for nodes, and you know the erasure coding scheme, you can calculate the amount of data churn in a given time period. The formula for calculating data churn is:</p><p><img src=https://storj.io/blog/img/rvsec-formula-1.png alt></p><p>where is the churn rate, B is the number of bytes on the network, n is the total number of erasure shares, m is the repair threshold, and k is the number of pieces needed for rebuild.</p><p>For example, on our hypothetical erasure-coding network, even if we use a 30⁄80 Reed-Solomon scheme (which is much more durable than the 4⁄8 scenario listed above), a MTTF of 9 months would mean you would have to <strong>repair 35% of your data every month to achieve a durability of 99.999999999%!</strong></p><p>This shows node churn is the single most impactful factor in file availability. Increasing node churn dramatically decreases file availability and durability. Strategies like erasure coding and replication are means of insulating against the impact of node churn, but without a mechanism to replace the data, file loss is simply a factor of the rate of churn.</p><p>So, let’s take that math and apply it to our two hypothetical networks. The first thing we need to do is calculate how we get to eleven 9s of durability for each of the two scenarios:</p><ol><li>For the erasure code-only scenario, calculate the k/n ratio that will deliver the target durability for the defined rate of churn.</li><li>For the erasure code+replication scenario, calculate the number of times the erasure coded pieces need to be replicated to deliver the target durability for the defined rate of churn.</li></ol><p>To calculate the durability of a replicated or erasure-coded file, we consider the CDF of the Poisson distribution, given by the formula:</p><p><img src=https://storj.io/blog/img/rvsec-formula-2.png alt></p><p>where D is the event that the most n-k pieces have been lost. In the case of simple replication, k=1, so a file is still recoverable when at most n-1 of the pieces have been lost; that is, if at least one of the copies is still on the network, the data is still accessible. When considering replication on a file that has already been subjected to erasure encoding, the calculation changes.</p><p>Suppose that a file undergoes k=4, n=8 erasure-encoding (where 8 pieces are created and only 4 are needed for a rebuild), and then suppose further that each of the 8 erasure shares are replicated r=10 times each, for a total of 80 pieces. These 80 pieces are special in that not any 4 can be used to rebuild the file, so they should really be thought of as 80 pieces contained in 8 sets of 10 copies each. To rebuild a file, 4 of the sets must still each have at least 1 piece each.</p><p>Thus, rather than having a single factor of P(D) determining the durability (with at most n-1 pieces being lost), one has a factor of P(D) for each unique set required for rebuild, since there are now k sets of which each one must not have lost more than r-1 pieces, where the expansion factor r determines the number of copies that are made (with there being r-1 copies made to achieve an expansion factor of r, including the original file). Calculating this probability requires the use of the Binomial distribution, where we let p be the probability that at most r-1 copies have been lost from a set. Then, to calculate the probability that there are at least k sets containing at least 1 copy each, we find the area of the upper tail of the Binomial CDF:</p><p><img src=https://storj.io/blog/img/rvsec-formula-3.png alt></p><p>Let’s first look at the impact of node churn on durability based on the two hypothetical scenarios, one using replication+erasure coding, and the other optimizing for erasure coding alone. Based on the above formulas, the results are as follows:</p><p><img src=https://storj.io/blog/img/table-1.png alt></p><p>As it turns out (predictably) the increased durability can be achieved in the erasure-code-only scenario with no increase in expansion factor. Adding replication to already-erasure-coded data is much more efficient that just straight up replicating the original file, (which requires 17 copies to achieve), but has triple the expansion factor of erasure codes alone.</p><p>In an environment where churn is even higher, or highly variable, durability is impacted significantly in either scenario:</p><p><img src=https://storj.io/blog/img/table-2.png alt></p><p>In these unpredictable or highly variable environments, it becomes necessary to address the worst case scenario in order to maintain a constant level of durability. Again, as is clear from the table below, node churn has a massive impact, and when using replication, that massive impact translates directly into increases in the expansion factor. In the table below you can see the impact of churn on expansion factor when trying to maintain a minimum durability of eleven 9s:</p><p><img src=https://storj.io/blog/img/table-3.png alt></p><p>So, what do these tables tell us? Well, there are a number of interesting observations to be drawn:</p><ul><li>At higher rates of churn, replication dramatically increases the expansion factor and, as we learned in
<a href=https://storj.io/blog/2018/11/replication-is-bad-for-decentralized-storage-part-1-erasure-codes-for-fun-and-profit/>the previous blog post</a>, requires much higher bandwidth utilization for repair.</li><li>Erasure coding can be used to achieve higher rates of durability without increasing either the expansion factor or the amount of bandwidth used for repair.</li></ul><p>Just to drive that point home, let’s first look at how a file actually exists on the two hypothetical networks:</p><p><img src=https://storj.io/blog/img/table-4.png alt></p><p>It is worth understanding the differences in how repair actually behaves on our two networks, because the process for replication is very different compared to erasure codes. Continuing the example of the 1 TB file above, let’s examine how repair actually looks when 1⁄3 of nodes storing the data exit the network:</p><p><img src=https://storj.io/blog/img/table-5.png alt></p><p>One other important thing to remember about distributed storage networks is that the amount of data the network can store isn’t constrained by the amount of available hard drive space on the nodes. It’s constrained by the amount of bandwidth available to nodes. Allow me to explain.</p><p>The following variables and calculated values are used in determining the amount of data and bandwidth a storage node operator can contribute:</p><p><strong>Variables</strong></p><ol><li><p><strong>Storage per storage node operator</strong> - The amount of hard drive space available to share by a storage node.</p></li><li><p><strong>Download speed</strong> - The downstream bandwidth available on the network on which the storage node is operating, measured in Mbps.\</p></li><li><p><strong>Upload speed</strong> - The upstream bandwidth available on the network on which the storage node is operating, measured in Mbps.</p></li><li><p><strong>ISP bandwidth cap</strong> - The maximum amount of bandwidth a storage node operator can utilize in a month before being subjected to a bandwidth cap enforcement action such as incurring a financial penalty or being subjected to bandwidth throttling from an ISP.</p></li><li><p><strong>Storage node operator bandwidth utilization percentage</strong> - The percentage of the total monthly bandwidth cap that a user will dedicate to be used by their storage node, assuming some percentage of bandwidth will be utilized for other services.</p></li><li><p><strong>Egress bandwidth percentage</strong> - The average amount of egress traffic from client downloads based on the use cases we support.\</p></li><li><p><strong>Repair bandwidth ratio (as a percent of storage)</strong> - The percentage amount of repair traffic on the network, primarily driven by node churn, software or hardware failure. While actual nodes may experience higher or lower repair traffic based on the pieces they hold, this is the average across the network.</p></li><li><p><strong>Ingress bandwidth percentage</strong> - The amount of bandwidth available for uploads of new data from clients.\</p></li></ol><p><strong>Calculations</strong></p><ol><li><strong>Total available upload bandwidth based on download speed (excluding cap)</strong> - The maximum amount of data available for ingress, based on download speed in Mbps multiplied by number of seconds in a month.</li><li><strong>Total available download bandwidth based on upload speed (excluding cap)</strong> - This calculation is the percent of the bandwidth cap a user is willing to dedicate to the Storj network multiplied by the bandwidth cap for ingress.</li><li><strong>Maximum data uploaded per month (TB) based on BW cap x percent available for upload</strong> - This calculation is the amount of data that can be uploaded irrespective of the cap, based on download speed in Mbps multiplied by seconds in a month.</li><li><strong>Maximum data uploaded per month (TB) based on download speed x seconds in month</strong> - This calculation is the percent of the bandwidth cap a user is willing to dedicate to the Storj network multiplied by the bandwidth cap.</li><li><strong>Maximum data downloaded per month (TB) based on BW cap x percent available for download</strong> - This calculation is the amount of data that can be downloaded irrespective of the cap, based on upload speed in Mbps times seconds in a month.</li><li><strong>Maximum data downloaded per month (TB) based on upload speed</strong> - This calculation is the percentage of the bandwidth cap required to dedicate to Storj repair traffic times the bandwidth cap.</li><li><strong>Maximum repair traffic per month (TB) based on BW cap</strong> - This calculation is the amount of data for repair traffic irrespective of the cap based on upload speed in Mbps times seconds in a month.</li><li><strong>Maximum repair traffic per month (TB) based on upload speed</strong> - This is how many months it will take to fill the available hard drive space at the lesser ingress rate of percent of available BW cap or actual throughput.</li></ol><p>Although download speed is typically higher in asynchronous internet connections, from the perspective of a person uploading a file to, or downloading a file from, a decentralized file system, the upload and download from the client’s perspective are the inverse of the storage node. For example, when a client uploads data to the network, it is technically downloaded to the storage node. Similarly, when data is downloaded by a client, it is technically being uploaded by the storage node.</p><p>The following examples are based on two different storage nodes with different bandwidth caps. Note that the amount of data stored is inclusive of the expansion factor.</p><p><img src=https://storj.io/blog/img/table-6.png alt></p><p>Bandwidth has a significant impact across the board. It’s generally finite and has to be split between ingress, egress and repair. As the expansion factor increases, the actual amount of bandwidth consumed for those functions increases at the same rates. Lower bandwidth caps further lower the actual amount of data the network can store with a given number of nodes. Increase the amount of bandwidth required for file repair and that number gets lower still.</p><p>Let’s look at the impact on bandwidth available for repair if you also constrain nodes practical limits of shared storage space. In the scenario above where nodes have:</p><ul><li><p>2 TB bandwidth cap</p></li><li><p>100 Mbps down/10 Mbps up asynchronous bandwidth</p></li><li><p>2 TB of shared storage on average\</p></li><li><p>50% of data downloaded per month</p></li><li><p>40% data uploaded per month</p></li><li><p>10% churn</p></li><li><p>Nodes operate at 100% bandwidth capacity and storage</p></li></ul><p>Each node has less than 0.12 TB of bandwidth available for repair. And that’s in an environment storing archival data without a lot of download bandwidth. When scaling that distributed storage network up to an exabyte of data stored, you really see the impact of the expansion factor.</p><p><img src=https://storj.io/blog/img/table-7.png alt></p><p>Ultimately, the result is an exponential increase in the number of nodes required to support a given network size. Higher bandwidth use cases further exacerbate the problem by increasing the number of nodes required to service a given amount of stored data. That given network size has a finite amount of revenue associated with it, which is then spread over an increasing number of storage node operators, meaning that over time the amount of money earned by storage node operators decreases.</p><p>Rapidly increasing demand for more storage node operators, combined with decreasing payouts per node, results in increased node churn, which only accelerates the cycle. Once again, increased churn increases the expansion factor from replication, increasing bandwidth utilized for repair, which further erodes the bandwidth available for storage and egress.</p><p>What this means is that in the debate over any reliance on replication versus erasure coding alone, in an environment that must relentlessly optimize for bandwidth conservation, erasure coding without replication is the clear winner. Replication, and proof-of-replication approaches like that used in the Filecoin network, simply cannot sustain an acceptable level of durability with a corresponding expansion factor and repair rate that can operate in a bandwidth constrained environment. Just imagine that same network above with 25% churn where the replication example requires a 1,400% expansion factor to maintain sufficient durability. Sorry if I scared you with that one.</p><p>When you have to factor in the reality that in the current storage market customers only pay for storage based on the actual, pre-erasure coded or replicated volume of data, and egress bandwidth, when it comes to the dollars, replication makes even less sense.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(n){const e=window.getSelection(),t=document.createRange();t.selectNodeContents(n),e.removeAllRanges(),e.addRange(t)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><div class=toc-title>Contents</div><nav id=TableOfContents><ul><li><ul><li><a href=#durability-and-expansion-factor-a-hrefdurability-and-expansion-factor-iddurability-and-expansion-factora>Durability and expansion factor <a href=durability-and-expansion-factor id=durability-and-expansion-factor></a></a></li><li><a href=#erasure-code-a-hreferasure-codes-iderasure-codesa>Erasure Code <a href=erasure-codes id=erasure-codes></a></a></li><li><a href=#okay-erasure-codes-take-less-disk-space-but-isnt-repairing-data-more-expensive-a-hrefokay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensive-idokay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensivea>Okay, erasure codes take less disk space. But isn’t repairing data more expensive? <a href=okay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensive id=okay-erasure-codes-take-less-disk-space-but-isn-t-repairing-data-more-expensive></a></a></li><li><a href=#downsides-a-hrefdownsides-iddownsidesa>Downsides? <a href=downsides id=downsides></a></a></li><li><a href=#upsides-a-hrefupsides-idupsidesa>Upsides? <a href=upsides id=upsides></a></a></li></ul></li></ul></nav></div></aside></main></body></html>