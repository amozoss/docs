[{"id":0,"href":"/dcs/api-reference/uplink-cli/access-command/","title":"access","first":"/dcs/","section":"Uplink CLI","content":"access #  Usage #  Windows ./uplink.exe access [command] Linux uplink access [command] macOS uplink access [command]  Child commands #     Command Description     create Create an access from a setup token. uplink setup is an alias for this.   delete Delete an access from local store   import Save an existing access. uplink import is an alias for this.    inspect Inspect allows you to expand a serialized access into its constituent parts    list Prints name and associated satellite of all available accesses    register Register an access grant for use with a hosted S3 compatible gateway and linksharing   restrict Restrict an access   revoke Revoke an access   use Set default access to use    Flags #     Flag Description     --help, -h help for access    "},{"id":1,"href":"/dcs/concepts/access/access-grants/","title":"Access Grants","first":"/dcs/","section":"Access Management","content":"Access Grants #  An Access Grant is a security envelope that contains a satellite address, a restricted API Key, and a set of one or more restricted prefix-based encryption keys—everything an application needs to locate an object on the network, access that object, and decrypt it.\nSimple Developer Tool for Access Management #  Access Grants coordinate two parallel constructs—encryption and authorization in a way that makes it easy to share data without having to manage access control lists or use complex encryption tools. Both of these constructs work together to provide a client-side access management framework that’s secure and private, as well as extremely flexible for application developers.\nAccess Grants are used for access management for client applications using the libuplink library, the CLI, as well as for generating credentials for the S3 compatible gateway (both the hosted GatewayMT and the self-hosted GatewayST).  To make the implementation of these constructs as easy as possible for developers, the Storj DCS developer tools abstract the complexity of encoding objects for access management and encryption/decryption. A simple share command encapsulates an encryption key, an API Key (a bearer token), and the appropriate Satellite address into an encoded string called an Access Grant.\nAccess Grants can be imported easily into an Uplink client, whether it\u0026rsquo;s the CLI, developer library, or a client application. Imported Access Grants are managed client-side and may be leveraged in applications via the uplink client library.\nAccess Grants can be restricted both from the server side (at the Satellite) and from the client side using the CLI or libuplink library, a serialized, hierarchically derived structure. When creating restricted access grants, both the API Key and the encryption key are hierarchically derived automatically from the parent Access Grant.\nLearn how to create an Access Grant using the Satellite Admin Console in the Satellite Admin Console Quickstart.\nLearn how to create an Access Grant using the Uplink CLI in the Uplink CLI Quickstart.\nStorj DCS satellites never come in contact with encryption keys. When you use an access grant with the CLI, libuplink library, or the self-hosted Gateway, encryption keys are managed client-side using a serialized, hierarchically derived structure for end-to-end encryption. With the cloud-hosted Gateway-MT, your data is server-side encrypted, since Storj is hosting the gateway.  "},{"id":2,"href":"/dcs/api-reference/uplink-cli/access-command/access-inspect-command/","title":"access inspect","first":"/dcs/","section":"access","content":"access inspect #  Usage #  Windows ./uplink.exe access inspect [ACCESS-GRANT] [flags] Linux uplink access inspect [ACCESS-GRANT] [flags] macOS uplink access inspect [ACCESS-GRANT] [flags]  Flags #     Flag Description     --access string the serialized access, or name of the access to use   --help, -h help for inspect    Examples #  Windows ./uplink.exe access inspect cheesecake Linux uplink access inspect cheesecake macOS uplink access inspect cheesecake  is equivalent to:\nWindows ./uplink.exe access --access cheesecake inspect Linux uplink access --access cheesecake inspect macOS uplink access --access cheesecake inspect  and will output something like:\n{ \u0026#34;satellite_addr\u0026#34;: \u0026#34;12EayRS2V1kEsWESU9QMRseFhdxYxKicsiFmxrsLZHeLUtdps3S@us1.storj.io:7777\u0026#34;, \u0026#34;encryption_access\u0026#34;: { \u0026#34;default_path_cipher\u0026#34;: \u0026#34;ENC_AESGCM\u0026#34; }, \u0026#34;api_key\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;macaroon\u0026#34;: { \u0026#34;head\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;caveats\u0026#34;: [], \u0026#34;tail\u0026#34;: \u0026#34;...\u0026#34; } } "},{"id":3,"href":"/dcs/concepts/access/access-grants/api-key/restriction/","title":"Access Restrictions","first":"/dcs/","section":"API Key","content":"Access Restrictions #  Granular Access Control with Caveats #  Access restrictions are encoded into the API Key within and Access Grant automatically when creating an Access Grant via the Satellite Admin Console, via the CLI, or libuplink library, when using the Share command. While the possibilities for access controls that can be encoded in a Caveat are virtually unlimited, the specific Caveats supported on Storj DCS today are as follows:\nSpecific Operations: Caveats can restrict whether an Access Grant can permit any of the following operations:\n Read Write Delete List  Bucket: Caveats can restrict whether an Access Grant can permit operations on one or more Buckets.\nPath and path prefix: Caveats can restrict whether an Access Grant can permit operations on Objects within a specific path in the object hierarchy.\nTime Window: Caveats can restrict when an Access Grant can permit operations on objects stored on the platform (before or after a date and time or a range of time between two dates/times.\nThe code related to the supported Caveats on the Satellite is available for review on GitHub.\nWhen an Access Grant is created to share access to an object, it creates an Access Grant because the object will need to be retrieved using the API Key in the Access Grant and decrypted using the encryption key.\nWhen an Uplink Client makes a request to a Satellite to perform an action on an object, the Satellite will evaluate the validity of the Access Grant and allow the action if the Access Grant is valid for the action and object.\nIn the case of sharing read access to an object, the Access Grant is used to allow an Uplink Client to download the pieces of a file and re-encode the pieces into a complete file, but the Uplink Client must also be able to decrypt the encrypted file for file sharing to be actually useful.\n"},{"id":4,"href":"/dcs/concepts/access/access-grants/api-key/","title":"API Key","first":"/dcs/","section":"Access Grants","content":"API Key #  The API Key encoded into an Access Grants is based on a type of token called a Macaroon. A Macaroon is essentially a cookie with an internal structure for encoding logic, in this case, access restrictions. A Macaroon embeds the logic for the access it allows and can be restricted, simply by embedding the path restrictions and any additional restrictions within the string that represents the Macaroon. Unlike a typical cookie, a Macaroon is not a random string of bytes, but rather is an envelope with access logic encoded in it.\nStorj DCS make it easy to share access to objects securely and privately. You don\u0026rsquo;t need to know how to construct an API Key, but understanding how they work and what the capabilities are provide you with a better understanding of the tools Storj DCS provides you to build more private and secure applications.  About API Keys #  For a more complete review of Macaroons, please check out the Google paper. This documentation will provide enough information to effectively use the access management and object sharing functionality of Storj DCS, but is not intended to be an exhaustive explanation on the full functionality of Macaroons.\nAlthough this documentation uses the terms “API Key” and “Macaroon” interchangeably, only the term “Access Grant” is referenced on the service, through the libraries, and in the documentation. The API Key is embedded inside of an Access Grant, and is not something you need to manage separately.\nAccess Management Starts at the Project Level #  Each Project has a Root API Key Secret that is issued by the Satellite. This Root API Key Secret is used to create other API Keys. Since all Access Grants are derived from the same Root API Key Secret, they all have the same level of access. By default, all API Keys have complete, unrestricted access to perform all functions against all objects within a project.\nAccess Encoding by Uplink Client #  When an Uplink Client (LibUplink, Uplink CLI, or Uplink S3 Gateway) is configured to use an Access Grant, the Uplink Client automatically creates an HMAC signature that is encoded in the metadata of the object stored. As objects and their path metadata are created, the hierarchy is encoded in the object metadata. Based on where an object falls in the hierarchy, If the parent Access Grant is known, a restricted Access Grant can be derived for any level of the hierarchy that is valid from that point in the hierarchy and below to any child objects below it in the hierarchy.\nSharing Access to Objects #  When the Access Grant is created by the Uplink Client, that Access Grant can be passed to a peer (another Uplink Client). When that peer Uplink Client uses that Access Grant to access an object, it passes only the API Key to the appropriate Satellite to request access to the object (never the encryption key). The Satellite can determine the validity of the API Key passed to it (along with any Caveats as described below) without needing access to the actual metadata. Since the metadata is also encrypted client-side, this is extremely important.\nEffectively, the Satellite does not need to know which user or application is attempting an object or what the object is; The Uplink Client provides only the minimum information that allows the Satellite to determine the validity of the request without knowing anything about the requestor or the object being requested.  Encoding Restrictions in an Access Grant #  It is also possible to restrict an Access Grant to provide a limited level of access to an object. Access restrictions are accomplished through the use of Caveats. Caveats are conditional access restrictions that are encoded into the body of an API Key.\nAn API Key has three parts, a head, a list of caveats, and a tail. These are concatenated and serialized together. An unrestricted API Key has no caveats, so it’s just a head and a tail. The head is a random nonce, and the tail of the unrestricted API Key is the HMAC of the root secret and the head.\nThe next section will detail the specific restrictions on the bucket and object constructs.\\\n"},{"id":5,"href":"/dcs/concepts/edge-services/auth-service/","title":"Auth Service","first":"/dcs/","section":"Edge Services","content":"Auth Service #  Overview #  When objects are shared via the Linksharing service or via the S3-compatible hosted gateway, an Access Grant is automatically registered with the Auth Service. The Access Grant used in conjunction with the edge services does contain encryption information for the objects that are within the scope of the Access Grant. Access Grants registered with the Auth Service are encrypted with an encryption key that is not stored or persisted by any Storj Service. The encryption key used to encrypt the Access Grants are held by the user or the user\u0026rsquo;s application.\nIn all cases, the encryption key used to encrypt the Access Grants registered with the Auth Service is managed in the same way and treated the same way by the code. There are three different ways the users and applications interact with the Auth Service:\n Object Browser - users must enter their encryption passphrase that derives the appropriate encryption keys each time they access the Object Browser in the Satellite Admin Console. Hosted S3 Gateway - the encryption key used to encrypt the Access Grant is the Access Key in the S3 credentials generated from registering the Access Grant. Linkshare Service - the encryption key used to encrypt the Access Grant is embedded in the URL. Note that in the case of Access Keys specifically registered for the linksharing service, the Access Key does not require a Secret Key for authorization and use.  Note: The encryption information generated for use in conjunction with the Linksharing Service or S3-compatible gateway credentials follows the same hierarchically deterministic derived method as all Access Grants and the encryption information is limited in scope to the level of access provided at the path-key or object-key as defined in the access Grant.\nAll of the Edge Services use server-side encryption.  Secure Handling of Encryption Information #  Where customers elect to utilize server-side encryption in conjunction with Storj Edge Services, Storj Edge Services only hold customer encryption information during the duration of an operation but not longer. Such customer encryption information is stored encrypted at rest. This data is encrypted with a key given to the user that Storj does not keep. Without an encryption key, no one, including Storj, can access customer encrypted information. The user must provide this key as part of their operation authentication.\nRegistering an Access Grant #  Access Grants are registered with the Auth Service either via the Object Browser in the Satellite Admin Console or via the CLI. The CLI may be used to generate Linkshare links or S3-compatible gateway credentials.\\\n"},{"id":6,"href":"/node/before-you-begin/auth-token/","title":"Authorization Token","first":"/node/","section":"Before You Begin","content":"Authorization Token #  Authorization Token #  Make sure you have copied your personal single-use authorization token from the Node Hosting signup page. It looks like this:\nIf you do not have an authorization token yet, sign up to Hosting a node.  By using your authorization token, you agree to the Storage Node Operator Terms and Conditions.\n"},{"id":7,"href":"/dcs/getting-started/gateway-mt/aws-cli-advanced-options/","title":"AWS CLI Advanced Options","first":"/dcs/","section":"Quickstart - AWS CLI and Hosted Gateway MT","content":"AWS CLI Advanced Options #  Define an endpoint with AWS CLI #  AWS CLI v1.x Installing pluginawscli-plugin-endpoint will also install the AWS CLI v1  Install awscli-plugin-endpoint plugin:\n~ % pip3 install awscli-plugin-endpoint Configure plugin in your ~/.aws/config file:\n~ % aws configure set plugins.endpoint awscli_plugin_endpoint AWS CLI v2.x    Install the AWS CLI v2.x\n  Install awscli-plugin-endpoint plugin:\n~ % pip3 install --no-deps awscli-plugin-endpoint Remember the path where the plugin is installed, you will need it in the next step.\n  Configure the path in your ~/.aws/config file (replace the site-packages-path to your path from a previous step):\n~ % aws configure set plugins.cli_legacy_plugin_path site-packages-path   Configure plugin in your ~/.aws/config file:\n~ % aws configure set plugins.endpoint awscli_plugin_endpoint    ..and configure the default profile (see Storj-hosted S3 Compatible Gateway to choose a correct endpoint):\n~ % aws configure set default.s3.endpoint_url https://gateway.us1.storjshare.io ~ % aws configure set default.s3.multipart_threshold 60MB ~ % aws configure set default.s3.multipart_chunksize 60MB The resulting file would look like:\nAWS CLI v1.x [plugins] endpoint = awscli_plugin_endpoint [default] s3 = endpoint_url = https://gateway.us1.storjshare.io multipart_threshold = 60MB multipart_chunksize = 60MB AWS CLI v2.x [plugins] cli_legacy_plugin_path = c:\\users\\USER\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages endpoint = awscli_plugin_endpoint [default] s3 = endpoint_url = https://gateway.us1.storjshare.io multipart_threshold = 60MB multipart_chunksize = 60MB  You can now use the AWS CLI without specifying an endpoint:\n~ % aws s3 ls 2021-01-08 19:41:13 demo To configure s3api endpoint you can use this command:\n~ % aws configure set default.s3api.endpoint_url https://gateway.us1.storjshare.io You can also use a different profile for Storj:\n~ % aws configure set profile.storj.s3.endpoint_url https://gateway.us1.storjshare.io ~ % aws configure set profile.storj.s3.multipart_threshold 1TB To use AWS CLI with a separate profile storj:\n~ % aws s3 --profile storj ls 2021-01-08 19:41:13 demo "},{"id":8,"href":"/dcs/billing-payment-and-accounts-1/pricing/","title":"Billing, Payment and Accounts","first":"/dcs/","section":"Billing, Payment \u0026 Accounts","content":"Billing, Payment and Accounts #  Understanding Your Storj DCS Billing and Usage for Your Satellite Account #  This document explains how registered Storj DCS users are billed for the data they store and the bandwidth they use on Storj DCS. The information provided in the documentation below is not intended to modify or supersede the Terms of Service. In the event of a conflict between this documentation and Terms of Service, the Terms of Service shall govern.\nAll examples of price calculations provided in this Documentation are specific to the usage exactly as described in the example. The examples are meant to illustrate how billing is calculated and your actual bill will be based on your actual usage pattern.\nOverview #  The Storj DCS service provides secure, private and reliable cloud object storage at a much lower cost than the cost of the big cloud providers. Billing is simple. We charge for we will call “metered service types.” The metered service types are: static object storage, download bandwidth, and, under some circumstances, a per-segment fee.\nStatic Object Storage #  Static object storage is charged at $4 per terabyte per month, or $0.004 per gigabyte. Static object storage is calculated in “terabyte months,” which is a prorated calculation of how much storage is used throughout the month, broken down by hour. Storing one terabyte of data for 30 days, two terabytes of data for 15 days, or three terabytes of data for 10 days, would each be the equivalent of one terabyte month.\nDownload Bandwidth #  Download bandwidth is charged at $7 per terabyte, or $0.007 per gigabyte of bandwidth consumed. There is no charge for ingress bandwidth, so uploading files to the network doesn’t incur any cost.\nSegments #  With default usage limits, there is no per-segment fee. If you request a usage limit increase for segments, you may be charged an additional fee.\nUsage Limits #   Usage Limits allow us to set boundaries on the amount of data users can upload to the Storj network and the amount of download bandwidth available to users. We have separate limits for storage and egress bandwidth per project on all Storj DCS Satellites. All limits are set to the Free Tier level until a method of payment is added. If you would like to increase your limits to higher values, you can always contact our support team through the Storj DCS support portal.​\nStorage and bandwidth limits are imposed by most cloud infrastructure providers as a normal part of capacity planning and to ensure the achievement of SLAs. In distributed and decentralized storage systems they are equally important, if not more so. Just like any provider, the aggregate amount of available storage and bandwidth must be shared across all users. With a distributed and decentralized storage system like Storj, the storage and bandwidth are provided by a network of third parties running storage node software. One of the key drivers of the success of the Storj DCS service is the balance of supply and demand. If there are too many users over-utilizing available resources, the user experience will be poor.\nSimilarly, if there are too many storage nodes, there won’t be enough use to provide a meaningful ROI for Storage Node Operators. This can lead to storage node churn, increasing load on the network, and potentially impacting durability. Usage limits are one part of the toolkit that allows us to maintain the balance.\nWe introduced rate limits between the uplink and the satellite to ensure a good quality of service for all uplink users. Without the rate limit, it would be possible for users to inadvertently consume most of the database resources available on the satellite and cause issues for other users. The intention was to pick a default limit that would remain mostly unnoticed by end users (as the typical use case shouldn’t hit the limit).\nWe also have usage limits for some other platform features or constructs, including projects, buckets and segments.\nCustomers can request a usage limit increase when needed by filling out the limit increase request form on our Storj DCS support portal. Please only make such requests if your use case really requires more than the current default limits. Requests will be evaluated taking into account the intended use case and availability on the network.\nRead more about Usage Limits under Concepts.\nHow Billing is Calculated Payment Methods "},{"id":9,"href":"/node/setup/cli/","title":"CLI Install","first":"/node/","section":"Setup","content":"CLI Install #  Before a CLI Install, you must: #  Receive an Authorization Token #  Authorization Token Generate an Identity #  Identity Set Up Port Forwarding #  Port Forwarding Failure to complete these steps will prevent your storage node from working.  "},{"id":10,"href":"/dcs/solution-architectures/common-use-cases/","title":"Common Use Cases","first":"/dcs/","section":"Solution Architectures","content":"Common Use Cases #  Optimal object storage uses for Storj DCS #  Object storage is ideal for files that are written once and read many times (WORM - Write Once, Read Many data). Object storage has formed the backbone of many different applications and use cases. Distributed and decentralized object storage is optimized for larger files, especially where those files are accessed regularly from geographically diverse locations.\nObject storage, in general, has a wide range of use cases. The use cases that are the best fit for Storj DCS are outlined below. If you don\u0026rsquo;t see your use case listed, don\u0026rsquo;t worry - this will just give you a sense of how Storj DCS is meant to be used.\nCommon use cases for decentralized and distributed storage #     Platform/Service Description Decentralized Advantage     General Backup Long term storage of large files required for business continuity or disaster recovery needs Low cost and always available high-throughput bandwidth means storage is economical and recovery is rapid   Database Backup Regular snapshot backups of databases for backup or testing are an entrenched part of infrastructure management Streaming backup eliminates the need to write large database snapshots to local disk before backup or for recovery   Private Data Data that is highly sensitive and an attractive target for ransomware attacks or other attempts to compromise or censor the data Client side encryption and industry-leading access management controls and highly distributed network of storage nodes reduce attack surface and risk   Multimedia Storage Storage of large numbers of large multimedia files, especially data produced at the edge from sources like security cameras that must be stored for long periods of time with low access Rapid transit leveraging parallelism makes distributed storage effective for integrating with video compression systems to reduce volume of data stored   Multimedia Streaming Fluid delivery of multimedia files with the ability to seek to specific file ranges and support for large number of concurrent downloads Native file streaming support and distributed bandwidth load across highly distributed nodes reduce bottlenecks   Large File Transfer Transiting large amounts of data point to point over the internet High-throughput bandwidth takes advantage of parallelism for rapid transit; Client-side encryption ensures privacy during transit   Hybrid Cloud Flexible ability to provide elastic capacity to on-premise data storage Enables enterprises to monetize excess storage capacity when not needed and provides secure, private cloud storage on demand   Machine Learning Storage transit for processing of large data sets from disparate data sources and types Decentralized architecture provides better response times for data processing, which can translate into the ability to process more data within time limits, as well as efficiency in transport and peering costs   VR/AR Virtual reality and augmented reality are both latency sensitive and bandwidth demanding with large file sets. Distributed storage provides better response times toward end users, as well as efficiency in transport and decreased peering costs   IoT Data Connected devices generate massive amounts of data Small IoT files can be packed into large blocks for efficient storage while individual message files can be accessed via streaming to specific data ranges   NFT Related Digital Assets Storage digital assets associated with non-fungible tokens (NFTs) Provide cost-effective storage with edge-based delegated authorization for managing access to digital content    "},{"id":11,"href":"/node/dependencies/storage-node-operator-payout-information/zk-sync-opt-in-for-snos/","title":"Configuring zkSync Payments","first":"/node/","section":"Storage Node Operator Payout Information","content":"Configuring zkSync Payments #  Background #  Storj is continuously innovating to bring the decentralized cloud to the masses\nWe are excited to bring Layer 2 (L2) ethereum scaling through ZK Rollups to our community. L2 scaling aligns with our goal of bringing decentralized cloud storage to the masses - via more efficient and autonomous payments.\nHere are a few of the benefits to receiving payouts through this approach:\nBetter Scalability\nWith zkSync, payouts will consume less block space on the Ethereum network because transactions are bundled together and processed in batches. The system is currently capable of processing 2000 transactions per second!\nLower Layer 2 Transfer Fees\nZkSync also dramatically lowers network transfer fees (compared to Layer 1 fees) for operators sharing their hard drive space and bandwidth on the Storj network. ZkSync accounts are tied to your existing Ethereum keys and current transaction fees on L2 are as low as ~0.000543 ETH at 210 Gwei. As the zkSync ecosystem grows, interoperability between projects and exchanges means even more savings. These fees can be reinvested in the community, creating new incentives for network operators to drive growth.\nPay Network Fees in STORJ Token\nOne of the most interesting things about zkSync is it supports \u0026ldquo;gasless meta-transactions\u0026rdquo; that allow users to pay transaction fees in the tokens being transferred. For example, if you want to transfer STORJ from L2 to an exchange, smart contract, or other address, there is no need for you to own ETH or any other tokens.\nGet Started and Opt-in #  To opt-in to zkSync you need to do a simple change in your Node configuration by following these steps:\nBinary versions (include Windows/Linux GUI) #  Open your storage node\u0026rsquo;s config.yaml (see Where can I find the config.yaml?)) and add the line\noperator.wallet-features: [\u0026#34;zksync\u0026#34;] Please enter everything in lowercase and double-check for spelling mistakes. This is a very basic implementation without any validations.  Once you have added the line to your config file, save it and restart your node.\nDocker versions #  If you use a docker version, you can also specify the zksync wallet feature as an option after the image name, for example:\ndocker run ... storjlabs/storagenode:latest --operator.wallet-features=zksync Please enter everything in lowercase and double-check for spelling mistakes. This is a very basic implementation without any validations.  If you decided to specify the zksync wallet feature as an option, you need to stop and remove the container and run it back with all your parameters include added option for wallet feature, otherwise you can just restart the container.\nHow to check the opt-in for zkSync #  Navigate to your personal web-dashboard, you should see an indication of zkSync enabled:\nAfter opting-in for zkSync payouts for STORJ payments, gather your ETH address and private key, navigate to the zkSync Wallet ( https://wallet.zksync.io/), and connect your L1 ethereum wallet. If you have problems accessing your wallet, you might want to change your payout address to an address that you can access (for which you control the private keys).\nSupported Wallets: zkSync supports WalletConnect, an open source protocol for connecting decentralized applications to mobile wallets. The protocol also supports hardware wallets like Trezor/Ledger and software wallets like Metamask, Fortmatic, Portis, Oper, Dapper, Lattice and Torus.\nFinally, zkSync enables our Storage Node Operators to more easily interact directly with the world of DeFi through solutions like Curve, Numio, Uniswap V3, and others.\nWe are excited to share this update around payment scaling with our community of operators. If you have any questions about using zkSync, check out our documentation. If you have ideas, or would like to talk with the team, please feel free to reach out on our forum.\n You can read more about our approach to storage node payouts in general here.\nUnderstanding zkSync fees #  Matter Labs runs an API for calculating transfer fees. Here\u0026rsquo;s an example session to determine the Layer 2 to Layer 1 withdrawal fee:\ncurl -X POST -H \u0026#39;Content-type: application/json\u0026#39; -d \u0026#39;{ \u0026#34;jsonrpc\u0026#34;:\u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;:1, \u0026#34;method\u0026#34;: \u0026#34;get_tx_fee\u0026#34;, \u0026#34;params\u0026#34;: [\u0026#34;Withdraw\u0026#34;, \u0026#34;0x80a52B7F26426d2b16578FC5f376c349F54772A7\u0026#34;, \u0026#34;STORJ\u0026#34;] }\u0026#39; https://api.zksync.io/jsrpc | jq Sample output:\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;feeType\u0026#34;: \u0026#34;Withdraw\u0026#34;, \u0026#34;gasTxAmount\u0026#34;: \u0026#34;52700\u0026#34;, \u0026#34;gasPriceWei\u0026#34;: \u0026#34;121000000437\u0026#34;, \u0026#34;gasFee\u0026#34;: \u0026#34;3197359019\u0026#34;, \u0026#34;zkpFee\u0026#34;: \u0026#34;1192017\u0026#34;, \u0026#34;totalFee\u0026#34;: \u0026#34;3190000000\u0026#34; }, \u0026#34;id\u0026#34;: 1 } This tells us that at a gas price of approximately 121 GWei, the withdrawal fee is about 31.9 STORJ. Note that this is only an example, actual fees will vary depending on current STORJ-ETH exchange rates.\nHere is an example of a Layer 2 to Layer 2 transfer fee:\ncurl -X POST -H \u0026#39;Content-type: application/json\u0026#39; -d \u0026#39;{ \u0026#34;jsonrpc\u0026#34;:\u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;:1, \u0026#34;method\u0026#34;: \u0026#34;get_tx_fee\u0026#34;, \u0026#34;params\u0026#34;: [\u0026#34;Transfer\u0026#34;, \u0026#34;0x80a52B7F26426d2b16578FC5f376c349F54772A7\u0026#34;, \u0026#34;STORJ\u0026#34;] }\u0026#39; https://api.zksync.io/jsrpc | jq Sample output:\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;feeType\u0026#34;: \u0026#34;TransferToNew\u0026#34;, \u0026#34;gasTxAmount\u0026#34;: \u0026#34;1980\u0026#34;, \u0026#34;gasPriceWei\u0026#34;: \u0026#34;121000000437\u0026#34;, \u0026#34;gasFee\u0026#34;: \u0026#34;120128480\u0026#34;, \u0026#34;zkpFee\u0026#34;: \u0026#34;1192017\u0026#34;, \u0026#34;totalFee\u0026#34;: \u0026#34;121300000\u0026#34; }, \u0026#34;id\u0026#34;: 1 } This tells us that at a gas price of approximately 121 Gwei, the transfer fee would be about 1.2 STORJ.\nYou can learn more in the zkSync Documentation.\nSend tokens from zkSync (L2) to Ethereum (L1) #  If you want to withdraw your tokens from your zkSync L2 wallet address to any L1 address, you need to reconnect your L1 Ethereum wallet to zkSync with https://wallet.zksync.io/ - once connected, it will show your L2 token balance as shown below.\nNow click the Send button, and in the next window you will be able to send to Ethereum (L1).\nClick the Send to Ethereum (L1) link.\nIn the Send to Ethereum window you can select or specify the L1 address that should receive the tokens you are sending in the Address field. This field will display your own L1 wallet address by default (which is the same as on L2), but you can change it to any other L1 address. For example, if you plan to immediately sell the tokens, you could choose an exchange\u0026rsquo;s STORJ deposit address. This will allow you to directly withdraw STORJ from your L2 zkSync wallet address to an exchange L1 deposit address in one single transaction.\nTo select a different address from your Contacts, you can click on the Own account[v] link.\nThis window also allows you to change which type of token to use to pay for the withdrawal fee. So if you are sending STORJ, by default part of your STORJ balance will be used for paying the withdrawal fee, but you can also select ETH for example, if you have sufficient ETH in the same wallet address to pay the withdrawal fee. To change the type of token to use for paying the gas fees, click the Change fee token button and select a different token.\nCurrently, zkSync requires users to pay a one-time activation fee for registering your address to zkSync, as this requires an on-chain transaction (you can elect to pay it with your STORJ tokens rather than ETH). See What is the activation fee on zkSync.  Complete Send to Ethereum #   Replace the destination address with the one you want to send the tokens to. You can save the entered address to Contacts later. Click the Select token button and select STORJ.  3. Enter the amount you want to send into the Amount field. You will see a transaction fee, one-time activation fee (only the first time you make a send to Ethereum), and the estimated fee values expressed in the token you selected (STORJ) and its equivalent value in USD.\n4. Authorize the Send to Ethereum by pressing the Authorize to Send to Ethereum button. You will be asked for confirmation in your Wallet.\n5. Confirm sending by pressing the Send to Ethereum button. You should be able to verify the withdrawal transaction was successful in History.\nSee also Send Funds to Ethereum for details.\n"},{"id":12,"href":"/dcs/how-tos/container-registry-docker/","title":"Container Registry - Docker","first":"/dcs/","section":"How To's","content":"Container Registry - Docker #  Containers are wonderful: Containers provide a powerful way to package and deploy an application and make the runtime environment immutable and reproducible. But using containers also requires more infrastructure - distributing containers requires a docker registry, either a public one (like Dockerhub) or a private instance.\nUnder the hood, the container registry serves simple REST requests. As Storj DCS also can serve files via HTTP, it can be used as a container registry if the pieces are uploaded in the right order and mode.\nThe structure of a container registry  #  But what is the right order? The container registry API follows a simple structure:\nHere we pull the elek/herbsttag image with the latest tag. The manifest can be found under https://host/v2/elek/herbsttag/manifests/latest.\nThe first item is a manifest JSON file that defines the required blobs and layer descriptors. For example:\n{ \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.container.image.v1+json\u0026#34;, \u0026#34;size\u0026#34;: 4416, \u0026#34;digest\u0026#34;: \u0026#34;sha256:833c7a986ed965eec8fe864223920c366fb0a25dd23edd0bdd2a4428fd0ce1e2\u0026#34; }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.image.rootfs.diff.tar.gzip\u0026#34;, \u0026#34;size\u0026#34;: 5865472, \u0026#34;digest\u0026#34;: \u0026#34;sha256:e2eb06d8af8218cfec8210147357a68b7e13f7c485b991c288c2d01dc228bb68\u0026#34; } ]}Both layers should be found under https://host/v2/elek/herbsttag/blobs. The layer in the manifest is a simple tar.gz file with the file system containing the first one, which is another JSON descriptor that includes all the container settings (labels), endpoints, environment variables, default volumes, etc.\nIf we upload the layers and metadata files in the same structure, Docker pull will be able to download our Docker images directly from the Storj decentralized cloud. But there are some catches:\n The /v2 endpoint should return with 200 (Docker uses this to double-check if registry implements v2 container registry). We will solve this by uploading an empty HTML file to /v2/index.html. A specific Content-Type should be returned for each manifest. Fortunately, Storj linksharing service supports custom content-type. We will solve this with uploading the manifest file with custom metadata:\nuplink cp /tmp/1 sj://dockerrepo/v2/elek/herbsttag/latest \u0026ndash;metadata \u0026lsquo;{\u0026ldquo;Content-Type\u0026rdquo;:\u0026ldquo;application/vnd.docker.distribution.manifest.v2+json\u0026rdquo;}\u0026rsquo; The container registry should be served under the root path of the domain (https://host/v2 is correct, while https://host/some/dir/deeper/v2 is incorrect). It can be resolved by assigning a custom domain name for the Storj bucket.  Publish the container  #  So let’s see an example. What is the publishing process, assuming we have a local docker container (elek/herbsttag in this example)?\nThe process is simple:\n Create/prepare all the JSON / blob files to upload to Storj DCS with uplink CLI (or UI) During the upload, define the custom HTTP header for the manifests Upload blobs Upload an empty index.html to make the `/v2` endpoint work Define custom domain name for your registry.  The first step can be done with skopeo, which is a command-line utility to copy container images between different types of registries (including local Docker daemon, and directories):\nskopeo copy docker-daemon:elek/herbsttag:latest dir:/tmp/container\nThe result is very close to what we really need:\n manifests.json should be uploaded to /v2/elek/herbsttag/manifests/latest The two hash-named layers are blobs to /v2/elek/herbsttag/blobs/ version file is not required  Let’s upload them with uplink cli:\n#just to make /v2 work touch index.html uplink cp index.html sj://registry/v2/index.html  #upload manifest with custom content type uplink cp manifest.json sj://registry/v2/elek/herbsttag/manifests/latest --metadata \u0026#39;{\u0026#34;Content-Type\u0026#34;:\u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;}\u0026#39;  #note: sha256: prefix is added to the filenames uplink cp 833c7a986ed965eec8fe864223920c366fb0a25dd23edd0bdd2a4428fd0ce1e2 sj://registry/v2/elek/herbsttag/blobs/sha256:833c7a986ed965eec8fe864223920c366fb0a25dd23edd0bdd2a4428fd0ce1e2 uplink cp e2eb06d8af8218cfec8210147357a68b7e13f7c485b991c288c2d01dc228bb68 sj://registry/v2/elek/herbsttag/blobs/sha256:e2eb06d8af8218cfec8210147357a68b7e13f7c485b991c288c2d01dc228bb68  #it\u0026#39;s a good idea to support sha256 based pulls sha256sum manifest.json #output: be9eeb0e64046a25df3df2df0eb2577ea11a9e521733b6e10df37914cddc7bcb manifest.json uplink cp manifest.json sj://registry/v2/elek/herbsttag/manifests/sha256sum:be9eeb0e64046a25df3df2df0eb2577ea11a9e521733b6e10df37914cddc7bcb --metadata \u0026#39;{\u0026#34;Content-Type\u0026#34;:\u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;} Now we are almost there. There are two remaining parts. First, we need to register a custom domain for our sj://registry bucket. The exact process is documented here, in short, an uplink command can be used to save the access grant and assign it to a domain:\nuplink share --public sj://registry/ --base-url https://link.eu1.storjshare.io --dns registry.anzix.net --auth-service https://auth.eu1.storjshare.io The command returns all the important information to modify the DNS zones. The usage of this information depends on the DNS registrar.\n# DNS INFO  # Remember to update the $ORIGIN with your domain name. You may also change the $TTL. $ORIGIN example.com. $TTL 3600 registry.anzix.net IN CNAME link.eu1.storjshare.io. txt-registry.anzix.net IN TXT storj-root:docker txt-registry.anzix.net IN TXT storj-access:jvydrxvotvks3tulle6pkvzmsgza The last piece is the configuration of your local docker daemon. The Storj linksharing service doesn’t support custom SSL certificates (yet), therefore Docker should use HTTP to access it (another workaround here is terminating the SSL with a CDN service like Cloudflare).\nModify /etc/docker/daemon/json and add the following section:\n{  \u0026#34;insecure-registries\u0026#34; : [\u0026#34;registry.anzix.net\u0026#34;] } And now we can test the pull command:\nSummary  #  Storj supports custom Content-Type for any key, so it can be used as a container registry to distribute container images. Currently, it has some limitations (no custom SSL, push is a manual process, no easy way to share layers), but distributing static container images for a large-scale audience can be done with all the advantages of a real Decentralized Cloud Storage.\n"},{"id":13,"href":"/dcs/concepts/decentralization/coordination-avoidance/","title":"Coordination Avoidance","first":"/dcs/","section":"Decentralization","content":"Coordination Avoidance #  Coordination Avoidance #  Rather than coming to a global consensus around the entire state of the network (as in blockchains like ethereum, etc.) The Storj Network is Coordination Avoidant. The network does not need global consistency (as per CAP Theorem) as each uplink user only needs to be able to recover their files.\nAdvantages Over Coordination-dependant Systems #  By ensuring coordination avoidance within Storj DCS, we’re able to deliver better performance and scalability over other decentralized systems—two issues that are critical to achieving broad adoption with traditional storage users. Decentralized systems that are coordination dependent, like Bitcoin, require an increasing number of resources as they scale. To compete with centralized cloud storage platforms like Amazon S3, Microsoft Azure, and Google Cloud, Storj DCS must be able to scale into the exabyte range, and beyond—something we feel confident it will be able to achieve.\nWe believe our approach of decentralizing both storage and metadata tiers in the network allows greater scalability, performance, and reliability than systems that rely on seeking consensus.\nComparison to Blockchain Networks #  Blockchain consensus offers very strong guarantees, but this comes at a heavy cost in coordination overhead. Coordination is not always necessary for correctness and minimizing coordination is key to maximizing scalability, availability, and high performance in database systems. One fundamental design decision of the Storj network was not to utilize blockchain consensus for file transfers to increase those properties of the Storj network. Storj takes a pragmatic approach to avoiding blockchain consensus while still maintaining correctness for file transfers. But, at the same time by default, Storj uses blockchain consensus with the Ethereum-based STORJ token for payment processing to storage node operators.\nStorj DCS is an enterprise, production-ready version of the Storj network, complete with guaranteed SLAs. All user uploads and downloads on Storj DCS uses metainformation from Storj DCS Satellites which are special nodes on the network that audit storage nodes and ensure Storage Nodes properly storing files and managing metadata for users storing data on the network.\nAs shown in the above architecture, there are three primary peer classes within the Storj network. All are open source, and capable of being run by any third party, making the network architecture fundamentally decentralized.\nThe Storj network can leverage the decentralized nature of storage nodes and Satellites to create partitions in the network to isolate users and file transfers from each other, which helps minimize coordination across the Storj network. For extremely high throughput demands, organizations can run their own Satellite. This avoids coordination overhead with the rest of Storj DCS and allows users to make their own decisions about what database infrastructure their Satellite will use and relax consistency guarantees if they wish.\n"},{"id":14,"href":"/dcs/getting-started/quickstart-uplink-cli/generate-access-grants-and-tokens/generate-a-token/","title":"Create Access Grant in CLI","first":"/dcs/","section":"Advanced Usage","content":"Create Access Grant in CLI #    You need to have a satellite account and installed Uplink CLI. To start, proceed through the initial steps of creating a new Access Grant.  3. When given the options to either continue in the browser or in the CLI, select Continue in CLI.\n4. Copy and save the Satellite Address and API Key as it will only appear once.\n5. Make sure you\u0026rsquo;ve already downloaded the Uplink CLI and run uplink setup.\nWindows ./uplink.exe access create Linux uplink access create macOS uplink access create  For anyone who has previously configured an uplink please use a named access. If you want to replace the default access, you should Create an Access Grant and use --overwrite flag.  6. Follow the prompts. When asked for your API Key, enter it (you should have saved it in step 4 above).\n7. Generate the Access Grant by running uplink share with no restrictions.\nIf you chose an access name, you\u0026rsquo;ll need to specify it in the following command as --access=name  Windows ./uplink.exe share --readonly=false Linux uplink share --readonly=false macOS uplink share --readonly=false  Keep your full-rights Access Grant in secret, it contains encryption key and it will allow to upload, download or delete your data from the entire project!  8. Your Access Grant should have been output.\n"},{"id":15,"href":"/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/create-first-access-grant/","title":"Create an Access Grant","first":"/dcs/","section":"Uploading Your First Object CLI","content":"Create an Access Grant #   You need to have a satellite account and installed Uplink CLI.\nNavigate to the Access page within your project and then click on Continue.\nGive your new Access Grant a name.\nAssign permissions to the Access Grant.\nIf you click Continue in Browser, our client-side javascript will finalize your access grant with your encryption passphrase. Your data will remain end-to-end encrypted until you explicitly register your access grant with Gateway MT for S3 compatibility. Only then will your access grant be shared with our servers. Storj does not know or store your encryption passphrase.\nHowever, if you are still reluctant to enter your passphrase into our web application, that\u0026rsquo;s completely understandable, and you should select Continue in CLI and follow these instructions.\n**The instructions below assume you selected **Continue in Browser.\n Select a Passphrase type: Either Enter your own Encryption Passphrase or Generate a 12-Word Mnemonic Passphrase. Make sure you save your encryption passphrase as you\u0026rsquo;ll not be able to reset this after it\u0026rsquo;s created.\nThis passphrase is important! Encryption keys derived from it are used to encrypt your data at rest, and your data will have to be re-uploaded if you want it to change!\nImportantly, if you want two access grants to have access to the same data, they must use the same passphrase. You won\u0026rsquo;t be able to access your data if the passphrase in your access grant is different than the passphrase you uploaded the data with.\nPlease note that Storj does not know or store your encryption passphrase, so if you lose it, you will not be able to recover your files.\n Access Grant is generated. The Access Grant will only display once. Save this information in a password manager or wherever you prefer to store sensitive information.\n"},{"id":16,"href":"/dcs/getting-started/satellite-developer-account/creating-your-account/","title":"Creating Your Account","first":"/dcs/","section":"Quickstart - Satellite Admin Console","content":"Creating Your Account #  The first step in creating your developer account on Storj DCS is to select a Satellite. A Satellite is a set of hosted services that is responsible for a range of functions on the network, including the node discovery system, node address information caching, per-object metadata storage, storage node reputation management, billing data aggregation, storage node payment, data audit and repair, as well as user account and authorization management.\nWhen selecting the Satellite for your project, you\u0026rsquo;ll want to choose the geographic region where the majority of the end users of your service who will be interacting with the objects on Storj DCS will be located.\nLearn more about Satellites under Concepts.\n To register for an Account, go to Storj.io and choose Start for Free\nNext, select your metadata region, in this example, the Americas region:\nChoose your account type - Personal if you\u0026rsquo;re an individual with a smaller project\nOr Professional if you are part of a business interested in leveraging Storj DCS in an application or service\nFill out the form and sign up. You\u0026rsquo;ll receive an email asking you to verify your email address:\nClick confirm to verify your email, then log into the Satellite Admin Console with your username and password.\nNext we\u0026rsquo;ll learn about using the Satellite Admin Console, starting with the Project Dashboard.\n"},{"id":17,"href":"/dcs/","title":"DCS","first":"/dcs/","section":"Storj Docs","content":"Introduction #  Storj DCS #  For developers who demand ownership of their data and who want to build with confidence, Storj Decentralized Cloud Storage (DCS) is private by design and secure by default—delivering unparalleled data protection and privacy when compared to traditional centralized cloud object storage alternatives.\nStorj DCS offers affordable and predictable pricing, S3 compatibility, a robust library of open source technical documentation and familiar development tools - along with a vibrant user community - which enables developers to economically and easily learn and leverage decentralized cloud storage technology to take control of their data when building the next great application or service.\nStorj Decentralized Cloud Storage (DCS): The Secure and Private Decentralized Cloud Storage Service For Developers #  Storj DCS is the world’s first open-source, decentralized cloud storage layer that’s private by design and secure by default - enabling developers to build in the best data protection and privacy into their applications as possible. The zero trust architecture, multi-region high availability, default encryption and edge-based access controls minimize risk and give only you, or those you grant permission to, access to your files. The result is that you take back full ownership and control of your data.\nHow it Works #  When an object is uploaded into Storj DCS, it is default encrypted, split up into 80 or more pieces and distributed across thousands of diverse nodes and ISPs in nearly 100 countries. There is no single point of failure or centralized location meaning outages, downtime, bitrot, ransomware and data breaches are virtually impossible. If a node goes offline for any reason, your file can be reconstituted from just 29 of its distributed pieces. And, in addition to user-assigned access grants to ensure privacy, our edge-based security model with delegated authorization provides flexible and ultra-secure capabilities for access management.\nWhy Choose Storj DCS? #  Developers have a wide range of choices for S3-compatible object storage, but Storj DCS provides a number of key advantages to help developers build more secure and private applications:\nSecurity #  Confidently store your data with files split, distributed, and stored multi-regionally across the globe providing no single point of failure, resistance to hacking, tampering, and bitrot, and an edge-based security model.\nPrivacy #  Own your data with default encryption and user-assigned access grants so no one can view or compromise your data without permission.\nPlug-in S3 Compatibility #  Swap out a few lines of code, and you’ll be up and running in minutes.\nAvailability #  Download your data anytime you need it with multi-region architecture, no single point of failure, and satellite-automated data orchestration. No downtime, no bitrot, and no lost files.\nDurability #  Automate file repair and know that Reed-Solomon erasure coding enables the highest levels of durability for all files uploaded to Storj DCS.\nCost Efficiency #  Get high availability multi-region cloud object storage for a fraction of the price of a single availability zone from centralized providers like AWS. And, get started for free! Create a Storj DCS account and take advantage of a free tier of decentralized cloud object storage that includes three projects with 50 GB each and 50 GB bandwidth each.\nOpen Source Freedom and Flexibility #  Take advantage of absolute transparency through our open source code. You are not locked-in to our technology or cost structure - giving you the freedom of choice.\nDeveloper Friendly #  Take advantage of familiar development tools and robust technical documentation, tutorials and videos to help get you started and leverage all the benefits of decentralized cloud object storage.\nCommunity Driven #  Share, discuss and collaborate with other open source developers in the Storj DCS community and find answers to questions in our very active forum.\nExperience the Benefits of Storj DCS Today #  It’s easy to take advantage of all the privacy and security gains delivered through decentralized cloud storage. Storj DCS is S3 compatible and you start for free!\n"},{"id":18,"href":"/node/setup/cli/docker/","title":"Docker","first":"/node/","section":"CLI Install","content":"Docker #  Install Docker #  To setup a Storage Node, you first must have Docker installed. Install Docker by following the appropriate installation guide for your OS.\nWindows  Windows Docker Installation\nPlease, install version 2.1.0.5 if your Windows doesn\u0026rsquo;t support WSL2: https://docs.docker.com/docker-for-windows/release-notes/#docker-desktop-community-2105\nAll newer versions for Hyper-V have various issues, such as losing network connection as described in this thread: https://forum.storj.io/t/latest-docker-desktop-for-windows-compatibility/6045\n Docker Toolbox is not supported  Linux Ubuntu Docker Installation\nCentOS Docker Installation\nDebian Docker Installation\nFedora Docker InstallationmacOS  MacOS Docker Installation\nPlease, install version 2.1.0.5: https://docs.docker.com/docker-for-windows/release-notes/#docker-desktop-community-2105\nAll newer versions have various issues, such as losing network connection, have false disk errors and so on as described in this thread: https://forum.storj.io/t/nodes-offline-for-3-4-days-is-it-possible-to-recover/11697/16\n Docker Toolbox is not supported.   "},{"id":19,"href":"/dcs/downloads/download-uplink-cli/","title":"Download Uplink CLI","first":"/dcs/","section":"Downloads","content":"Download Uplink CLI #  Creating Your Account Install the binary for your OS:\nWindows Download the Windows Uplink Binary zip file #  In the Downloads folder, right-click and select \u0026ldquo;Extract all\u0026rdquo;\nExtract to your user\u0026rsquo;s folder (\u0026quot;Alexey\u0026quot; in this example):\nOnce extracted, do not try to open the file, as it can only be accessed via command line.\nOpen Windows PowerShell and continue on to the next step.\nLinux AMD64 #  Curl Download #  curl -L https://github.com/storj/storj/releases/latest/download/uplink_linux_amd64.zip -o uplink_linux_amd64.zip unzip -o uplink_linux_amd64.zip sudo install uplink /usr/local/bin/uplink Direct Download #   Linux AMD64 Uplink Binary\nARM #  Curl Download #  curl -L https://github.com/storj/storj/releases/latest/download/uplink_linux_arm.zip -o uplink_linux_arm.zip unzip -o uplink_linux_arm.zip sudo install uplink /usr/local/bin/uplink Direct Download #   Linux ARM Uplink Binary\nARM64 #  Curl Download #  curl -L https://github.com/storj/storj/releases/latest/download/uplink_linux_arm64.zip -o uplink_linux_arm64.zip unzip -o uplink_linux_arm64.zip sudo install uplink /usr/local/bin/uplink Direct Download #   Linux ARM64 Uplink Binary\nmacOS Curl Download #  curl -L https://github.com/storj/storj/releases/latest/download/uplink_darwin_amd64.zip -o uplink_darwin_amd64.zip unzip -o uplink_darwin_amd64.zip sudo install uplink /usr/local/bin/uplink Direct Download #   macOS Uplink Binary\n Quickstart - Uplink CLI "},{"id":20,"href":"/node/resources/faq/","title":"FAQ's","first":"/node/","section":"Resources","content":"FAQ\u0026rsquo;s #  How is the online score calculated? Where can I check for a new version? How do I check my node when I\u0026#39;m away from my machine? What if my machine restarts or shuts down? How do I shutdown my node for system maintenance? What other commands can I run? How do I check my logs? How do I redirect my logs to a file? How do I check my L2 zkSync payouts? How does held back amount work? Where can I find the config.yaml? How do I change values like wallet address or storage capacity? How do I setup static mount via /etc/fstab for Linux? How do I estimate my potential earnings for a given amount of space and bandwidth? Why am I not storing more data? Why is the network not using all of my storage and bandwidth? Why are my payouts so low? How do I estimate my payouts per Satellite? How do I migrate my node to a new device? How to migrate the Windows GUI node from one physical location to another? What if I\u0026#39;m using a remote connection? How to remote access the web dashboard What if I\u0026#39;m using the CLI Install on Windows / MacOS? How to add an additional drive? Can\u0026rsquo;t find an answer to your question? Visit our Community. #  "},{"id":21,"href":"/dcs/api-reference/s3-gateway/gateway-st-advanced-usage/","title":"Gateway ST Advanced Usage","first":"/dcs/","section":"Self-hosted S3 Compatible Gateway","content":"Gateway ST Advanced Usage #  Configuration options #    Adding Access Grants  Adding Access Grants #  You can add several access grants to the config.yaml. using this format:\naccess: 14aV.... # default Access accesses.site: 26NBm..... # the Access with name \u0026#34;site\u0026#34; You can see the path to the default config file config.yaml with this command:\nWindows ./gateway help Linux gateway help macOS gateway help  Running options #    Running Gateway ST with an Access Grant  Running Gateway ST to host a static website  Running the Gateway to host a static website with cache  Running Gateway ST with an Access Grant #  You can run a gateway with specifying the access grant (or its name) with the option --access, for example:\nWindows ./gateway run --access 14aV.... or with the name of the access grant from your config (see Add multiple Access grants)\n./gateway run --access site Linux gateway run --access 14aV.... or with name of the access grant from your config (see Add multiple Access grants)\ngateway run --access site macOS gateway run --access 14aV.... or with name of the access grant from your config (see Add multiple Access grants)\ngateway run --access site Docker docker run -it --rm -p 127.0.0.1:7777:7777 --mount type=bind,source=/path/to/gateway-config-dir/,destination=/root/.local/share/storj/gateway/ --name gateway storjlabs/gateway run --access 14aV.... or with name of the access grant from your config (see Add multiple Access grants)\ndocker run -it --rm -p 127.0.0.1:7777:7777 --mount type=bind,source=/path/to/gateway-config-dir/,destination=/root/.local/share/storj/gateway/ --name gateway storjlabs/gateway run --access site  Running Gateway ST to host a static website #  You can also run a gateway to handle a bucket as a static website. Make sure to limit the access to the exposed buckets.\nWindows ./gateway run --access 14aV.... --website or with the name of the access grant from your config (see Add multiple Access grants)\n./gateway run --access site --website Linux gateway run --access 14aV.... --website or with name of the access grant from your config (see Add multiple Access grants)\ngateway run --access site --website macOS gateway run --access 14aV.... --website or with name of the access grant from your config (see Add multiple Access grants)\ngateway run --access site --website Docker docker run -it --rm -p 127.0.0.1:7777:7777 --mount type=bind,source=/path/to/gateway-config-dir/,destination=/root/.local/share/storj/gateway/ --name gateway storjlabs/gateway run --access 14aV.... --website or with name of the access grant from your config (see Add multiple Access grants)\ndocker run -it --rm -p 127.0.0.1:7777:7777 --mount type=bind,source=/path/to/gateway-config-dir/,destination=/root/.local/share/storj/gateway/ --name gateway storjlabs/gateway run --access site --website  Now you can navigate to http://localhost:7777/site/ to see the bucket site as XML or to http://localhost:7777/site/index.html to see a static page, uploaded to the bucket site.\nYou can publish this page to the internet, but in this case, you should run your gateway with the option --server.address local_IP:local_Port (replacelocal_IPwith the local IP of your PC andlocal_Port with the port you want to expose).\nIf you uselocalhost or 127.0.0.1 as your local_IP, you will not be able to publish it directly (via port forwarding for example), instead, you will have to use a reverse proxy here.\n Running Gateway ST to host a static website with cache #  You can use the Minio caching technology in conjunction with the hosting of a static website.\n The following example uses /mnt/drive1, /mnt/drive2 ,/mnt/cache1 \u0026hellip; /mnt/cache3 for caching, while excluding all objects under bucket mybucket and all objects with \u0026lsquo;.pdf\u0026rsquo; extensions on a S3 Gateway setup. Objects are cached if they have been accessed three times or more. Cache max usage is restricted to 80% of disk capacity in this example. Garbage collection is triggered when the high watermark is reached (i.e. at 72% of cache disk usage) and will clear the least recently accessed entries until the disk usage drops to the low watermark - i.e. cache disk usage drops to 56% (70% of 80% quota).\n Export the environment variables before running the Gateway:\nWindows Cache disks are not supported, because caching requires the atime function to be enabled.\n$env:MINIO_CACHE=\u0026#34;on\u0026#34; $env:MINIO_CACHE_EXCLUDE=\u0026#34;*.pdf,mybucket/*\u0026#34; $env:MINIO_CACHE_QUOTA=80 $env:MINIO_CACHE_AFTER=3 $env:MINIO_CACHE_WATERMARK_LOW=70 $env:MINIO_CACHE_WATERMARK_HIGH=90 Linux export MINIO_CACHE=\u0026#34;on\u0026#34; export MINIO_CACHE_DRIVES=\u0026#34;/mnt/drive1,/mnt/drive2,/mnt/cache{1...3}\u0026#34; export MINIO_CACHE_EXCLUDE=\u0026#34;*.pdf,mybucket/*\u0026#34; export MINIO_CACHE_QUOTA=80 export MINIO_CACHE_AFTER=3 export MINIO_CACHE_WATERMARK_LOW=70 export MINIO_CACHE_WATERMARK_HIGH=90 macOS export MINIO_CACHE=\u0026#34;on\u0026#34; export MINIO_CACHE_DRIVES=\u0026#34;/mnt/drive1,/mnt/drive2,/mnt/cache{1...3}\u0026#34; export MINIO_CACHE_EXCLUDE=\u0026#34;*.pdf,mybucket/*\u0026#34; export MINIO_CACHE_QUOTA=80 export MINIO_CACHE_AFTER=3 export MINIO_CACHE_WATERMARK_LOW=70 export MINIO_CACHE_WATERMARK_HIGH=90 Docker You can create a file with environment variables, for example - minio_vars with such content:\nMINIO_CACHE=\u0026#34;on\u0026#34; MINIO_CACHE_DRIVES=\u0026#34;/mnt/drive1,/mnt/drive2,/mnt/cache{1...3}\u0026#34; MINIO_CACHE_EXCLUDE=\u0026#34;*.pdf,mybucket/*\u0026#34; MINIO_CACHE_QUOTA=80 MINIO_CACHE_AFTER=3 MINIO_CACHE_WATERMARK_LOW=70 MINIO_CACHE_WATERMARK_HIGH=90 For Windows, the option -e MINIO_CACHE_DRIVES is useless due to the lack of an atime function, and can be removed with mounts those drives to the docker container.\nThen add parameters --env-file ./minio_vars --mount type=bind,src=/mnt/drive1,dst=/mnt/drive1 --mount type=bind,src=/mnt/drive2,dst=/mnt/drive2 --mount type=bind,src=/mnt/cache1,dst=/mnt/cache1 --mount type=bind,src=/mnt/cache2,dst=/mnt/cache2 --mount type=bind,src=/mnt/cache3,dst=/mnt/cache3 to the docker run section, for example:\ndocker run -it --rm -p 127.0.0.1:7777:7777 --env-file ./minio_vars --mount type=bind,src=/mnt/drive1,dst=/mnt/drive1 --mount type=bind,src=/mnt/drive2,dst=/mnt/drive2 --mount type=bind,src=/mnt/cache1,dst=/mnt/cache1 --mount type=bind,src=/mnt/cache2,dst=/mnt/cache2 --mount type=bind,src=/mnt/cache3,dst=/mnt/cache3 --mount type=bind,source=/path/to/gateway-config-dir/,destination=/root/.local/share/storj/gateway/ --name gateway storjlabs/gateway run --access site --website  Setting MINIO_BROWSER=off env variable would disable the Minio browser. This would make sense if running the gateway as a static website in production.  Then run the gateway with static site enabled.\n"},{"id":22,"href":"/dcs/how-tos/host-a-static-website/host-a-static-website-with-the-cli-and-linksharing-service/","title":"Host a Static Website with the Uplink CLI and Linksharing Service","first":"/dcs/","section":"Host a Static Website","content":"Host a Static Website with the Uplink CLI and Linksharing Service #  You can use your own domain name and host your own static website on Storj DCS with the following setup.\nStatic websites serve files, including HTML, CSS, and Javascript files, exactly as they are stored on the server. All visitors will be served the same files.\nDynamic websites use server-side processing to generate the underlying code behind each page. They support Create, Read, Update, Delete operations against a database. Web views can be custom rendered to each user.\n Part 1: Uplink CLI #   Download the Uplink Binary and upload your static site files to Storj DCS. You may also upload your files in any other manner, but you will need the Uplink CLI for the remaining steps. Share the bucket or object prefix (not individual objects) that will be the root of your website/subdomain. At the root, name your home pageindex.html. The website will serve the index.html file automatically e.g.http://www.example.test and http://www.example.test/index.htmlwill serve the same content. Anything shared with --dns will be readonly and available publicly (no secret key needed). You can optionally specify your preferred linkshare region with --base-url  Windows ./uplink.exe share --dns \u0026lt;hostname\u0026gt; sj://\u0026lt;bucket\u0026gt;/\u0026lt;prefix\u0026gt; --base-url \u0026lt;linkshare url\u0026gt; Linux uplink share --dns \u0026lt;hostname\u0026gt; sj://\u0026lt;bucket\u0026gt;/\u0026lt;prefix\u0026gt; --base-url \u0026lt;linkshare url\u0026gt; macOS uplink share --dns \u0026lt;hostname\u0026gt; sj://\u0026lt;bucket\u0026gt;/\u0026lt;prefix\u0026gt; --base-url \u0026lt;linkshare url\u0026gt;  Notably, this mechanism allows you to host multiple websites from the same bucket by using different prefixes. You may also create multiple subdomains by using different hostnames (however, the Uplink CLI only generates info for one at a time).\nThe command above prints a zone file with the information needed to create 3 DNS records. Your CNAME should match the linkshare service region you prefer.\n   Region CNAME     Asia link.ap1.storjshare.io   EU link.eu1.storjshare.io   US link.us1.storjshare.io    $ORIGIN example.com. $TTL 3600 \u0026lt;hostname\u0026gt; IN\tCNAME\tlink.\u0026lt;region\u0026gt;.storjshare.io. txt-\u0026lt;hostname\u0026gt; IN\tTXT storj-root:\u0026lt;bucket\u0026gt;/\u0026lt;prefix\u0026gt; txt-\u0026lt;hostname\u0026gt; IN\tTXT storj-access:\u0026lt;access key\u0026gt; Remember to update the $ORIGIN from example.com to your domain name (keep the trailing .). You may also change the DNS $TTL.\nFor example, for objects stored on the us1 satellite, running\nuplink share --dns www.example.com sj://bucket/prefix will output a zone file like the following:\n$ORIGIN example.com. $TTL 3600 www.example.com IN\tCNAME\tlink.us1.storjshare.io. txt-www.example.com\tIN\tTXT storj-root:bucket/prefix txt-www.example.com\tIN\tTXT storj-access:jqaz8xihdea93jfbaks8324jrhq1 Part 2: DNS Provider #  1. In your DNS provider, create a CNAME record on your hostname using the CNAME from your generated zone file as the target name.\nEnsure you include the trailing . at the end of your CNAME if your DNS providers allows.  2. Create 2 TXT records, prepending txt- to your hostname.\nRoot Path: the bucket or object prefix key that you want your root domain to resolve to (and that contains your index.html file).\nAccess Key: the readonly and public access key to your root path.\n3. You can check to make sure your DNS records are ready with dig @1.1.1.1 txt-\u0026lt;hostname\u0026gt;.\u0026lt;domain\u0026gt; TXT\n4. Without further action, your site will be served with HTTP. You can secure your site by using an HTTPS proxy server such as Fastly.\n5. Optionally, if you create a page titled 404.htmlin the root of your shared prefix, it will be served in 404 conditions.\n6. That\u0026rsquo;s it! You should be all set to access your website! e.g.http://www.example.test\n"},{"id":23,"href":"/dcs/billing-payment-and-accounts-1/pricing/billing-and-payment/","title":"How Billing is Calculated","first":"/dcs/","section":"Billing, Payment and Accounts","content":"How Billing is Calculated #  Billing is aggregated at the project level. A project is the Storj DCS service construct used for aggregating usage, calculating billing, invoicing fees, and collecting payment. Projects are created by a single user, then multiple users may be added to a project team, and one user may be on more than one project. Within a project, usage is tracked at the bucket level and aggregated for invoicing to the project. Project names are not client-side encrypted so that they may be rendered in the satellite user interface. For more information about Developer Accounts, Projects, Buckets, etc., please read the Key Architecture Constructs under the Concepts section of this Documentation.\nThe following table lists the types of metered services that appear in billing and usage user interfaces as well as invoices:\\\n   Metered Service Type Metered Units Increment Pricing Unit Price per Pricing Unit Description     Object Storage Bytes GB Hour GB Month $0.004 per GB Month Storage is calculated based on bytes uploaded, including any encryption-based overhead   Egress Bandwidth Bytes GB Total Volume $0.007 per GB Bandwidth related to object downloads calculated on bytes downloaded including long tail elimination-related bandwidth    The following section describes how the charges listed in the table above are calculated to provide users detailed insights into their cloud storage usage, broken down by storage, egress bandwidth, and number of objects. Note that for billing purposes, usage data is continuously rolled up and aggregated. Billing data is not displayed in real-time in the satellite interface and some time lag should be expected.\nObject Storage #  Object storage is priced per GB per month in increments of byte hours. The calculation of object storage fees is based on a standard 720-hour month. Actual storage is metered in bytes uploaded. Bytes uploaded include the bytes associated with actual objects plus any nominal overhead associated with encryption. Byte hours are calculated based on the number of hours bytes are stored on the Storj DCS Platform from when an object is uploaded to when it is deleted. The calculated number of bytes is then multiplied by the byte hour price. The byte hour price is derived from the GB month price divided by the standard 720-hour month and base 10 conversion of GB to bytes.\\\nExample\nA user uploads a 1TB file. Half way through the month, the user deletes the file. With encryption overhead, the file is stored as 1.001TB. The 1.001TB is accounted for as 1,001,000,000,000 bytes. The file is stored for 360 hours. The file is stored for 360,360,000,000,000 byte hours. The price per GB month is $0.004. The price per GB hour is $0.000005556. The price per byte hour is $0.000000000000005556. The total amount charged for the storage is $2.00.\nBandwidth Fee #  Download bandwidth, also referred to as egress bandwidth, is priced per GB in increments of bytes downloaded. The calculation of download bandwidth price per byte is derived from the GB download bandwidth divided by the base 10 conversion of GB to bytes. The calculated number of bytes is then multiplied by the byte download bandwidth per byte price.\nWhen an object is downloaded, there are a number of factors that can impact the actual amount of bandwidth used. The download process includes requests for pieces from more than the minimum number of storage nodes required. While only 29 pieces out of 80 are required to reconstitute an object, in order to avoid potential long-tail performance lag from a single storage node, an Uplink will try to retrieve an object from 39 storage nodes. The Uplink will terminate all incomplete downloads in process once 29 pieces are successfully downloaded and the object can be re-encoded. In addition, if a user terminates a download before completion, the amount of data that is transferred might exceed the amount of data that the customer’s application receives. This discrepancy can occur because a transfer termination request cannot be executed instantaneously, and some amount of data might be in transit pending execution of the termination request. This data that was transferred is billed as data download bandwidth.\nExample\nA user downloads one 1 TB file. Based on the long tail elimination, up to 1.3 TB of download bandwidth may be used. The 1.3 TB of download bandwidth is accounted for as 1,300,000,000 bytes. The price per GB is $0.007. The price per byte is $0.000000045. The total amount charged for the egress is $9.10.\nUnlike other cloud object storage vendors, we don\u0026rsquo;t use high egress fees to create vendor lock-in. If you discover that Storj DCS isn\u0026rsquo;t a fit for your project or application and you transfer your data to another service, use our support portal to submit a ticket and let us know. As long as you follow the process, we won\u0026rsquo;t charge you for that egress bandwidth.\nAll Projects have Project Limits on certain important constructs. Increases in Project Limits may impact the price of your use of Storj DCS. To learn more, check out the Project Limits and Usage Limit Increases sections of this Documentation\\\n"},{"id":24,"href":"/dcs/concepts/encryption-key/how-encryption-is-implemented/","title":"How Encryption is Implemented","first":"/dcs/","section":"Encryption","content":"How Encryption is Implemented #  Ensuring the Privacy and Security of Data on Storj DCS #  All data stored on the distributed and decentralized network of storage nodes and all metadata stored on Satellites is encrypted.\nBy encrypting file-paths, content, and metadata client-side, we avoid the danger of making this data available to attackers, and anyone else who is unable to derive the necessary encryption keys.\nThe network\u0026rsquo;s encryption method is purposely designed to avoid using the same keys for content encryption of different files and different segments of the same file. This is advantageous not only because it makes file sharing of encrypted files more secure, but because it does not put other segments or files at risk if one of them is compromised.\nThe encryption algorithm we used for content and metadata is easily configurable between AES-GCM and “ Secretbox,” which are both authenticated encryption algorithms. This means that if any encrypted data is tampered with, the client downloading the data will know about it once the data is decrypted.\nFirst, it’s critical to understand the definitions of a few key concepts used on the network for encryption.\n Segment: The largest subdivision of a file. All the segments of a file are usually the same size. In most cases, the last segment will be smaller than the rest. Path: The representation for a file’s “location.” Paths are essentially an arbitrary number of strings delimited by slashes (e.g. this/is/a/file.txt). On the Storj network, the Satellite uses paths to keep track of file metadata as well as pointers to storage nodes that possess encrypted file content. Root secret: A private string defined by the client that is used to derive keys for encrypting and decrypting data stored on the network. Object key: A key derived from the root secret and the file path. There is a different path key for every element in the path, and a path key is used to derive new path keys for lower level path items. Random key: A randomly generated key used to encrypt segment content and metadata. Derived key: A key derived from the path key for the lowest level path element. The derived key is used to encrypt the random key before it is stored in a segment’s metadata. HMAC: Hash-based message authentication code. We generate HMACs using path elements and encryption keys in order to derive new keys for lower levels of the path. Using hashes makes it easy to generate keys from higher levels without making it possible to generate higher level keys from lower level ones. AES-GCM: An authenticated encryption algorithm that makes use of the Advanced Encryption Standard and uses the Galois/Counter mode for encrypting blocks. Secretbox: An authenticated encryption algorithm from the NaCl library that combines the Salsa20 encryption cipher and Poly1305 message authentication code.  Path Encryption  #  Paths are encrypted in a hierarchical and deterministic way using the root encryption key. Each path component is encrypted separately based on information derived from previous path components.\nConsider an unencrypted path p that is made up of path elements p1/p2/…/pn. The end goal is to generate an encrypted path e, which is made up of elements e1/e2/…/en. We have a root secret, s0, and can derive a path key using this secret, k0 = K(s0). We then define the next secret as s1 = HMAC(s0, p1) and encrypt the first path element as e1 = encrypt(k0, p1). In more general terms, each derived secret si = HMAC(si-1, pi), and each encrypted path element ei = encrypt(ki-1, pi) where the path key ki-1 = K(si-1).\nThis method of path encryption allows us to do some interesting things. Consider a user, Brandon, with several files and subdirectories under the path p1/p2/p3/. Brandon wants to share everything under this path with Nat, another user, without revealing anything at a higher level (p1/…, p1/p2/…). Brandon can provide Nat with the encrypted path e1/e2/e3/ and the secret s3. Nat is now able to derive the encryption keys for any of Brandon’s files prefixed with the path e1/e2/e3/. However, she will be unable to decrypt any of the first three path elements or files that do not have the required prefix.\nWhile there are many benefits to path encryption, one challenge exists around efficiently listing unencrypted file names. Since the order of listed items is determined by the paths stored on the Satellite, listed items will always be returned in order based on their encrypted path names, but will not be alphabetical when the paths are decrypted.\nUsers of the network are able to opt out of path encryption on a per-bucket basis because of this limitation. If a user opts out of encrypted paths, the paths will still only be visible to the Satellite. Storage nodes do not have information about paths or metadata associated with pieces they are storing.\nContent and Metadata Encryption  #  When a user uploads a file, we read it one segment at a time on the client-side. Before each segment is split up, erasure encoded, and stored on remote storage nodes, we generate a random content encryption key. We also create a starting nonce equal to the segment number and use it along with the random key to encrypt the segment data.\nNext, we generate the derived key, dk, which we define with sn+1 = HMAC(sn, “content”), where dk = K(sn+1) and sn is the last secret generated from the file path using the technique detailed above. The reason we add one more derivation step instead of setting dk = K(sn) is because a file path can also be a prefix for other file paths. For instance, a/b/c is a valid file path, but so is a/b/c/d. If Brandon wants to share a/b/c with Nat, he should be able to provide Nat with a derived key to decrypt the file c, but it shouldn’t be possible for Nat to derive the key to access the file d even though it has the same prefix. By adding one more dimension of key derivation for content encryption, we avoid this issue.\nEach segment has metadata associated with it on the Satellite. Segment metadata includes the random key used to encrypt that segment’s content. We encrypt the random key with the derived key (dk) and a randomly generated nonce. The nonce is stored along with the encrypted content key in the segment metadata. This way, we use a different random encryption key for each segment, but anyone with the derived key can decrypt those keys.\nThe last segment’s metadata contains information in addition to the encrypted key and key nonce. The additional information is the metadata for the entire file. Some of this metadata is unencrypted, such as encryption type (AES-GCM or Secretbox) and encryption block size, since they are necessary to properly decrypt the file and metadata. The remainder of the metadata, which includes the number of segments, segment size, last segment size, and additional arbitrary metadata is encrypted with the last segment’s random content encryption key.\nIn summary, encryption and security on decentralized cloud storage networks has been carefully thought out to enable the sharing of files without compromising entire buckets of data. Cloud storage platforms must have the ability to easily share data for deployments like CDNs, websites and other use-cases. By deriving keys hierarchically from file paths, and encrypting data with different keys, Storj DCS maintains data privacy without removing important features.\nThis description pulls heavily from Moby Von Briesen\u0026rsquo;s blog post, located here: https://storj.io/blog/2018/11/security-and-encryption-on-the-v3-network/\nYou can read more about encryption on the V3 Storj network in sections 3.6 and 4.1 of our whitepaper.\n"},{"id":25,"href":"/node/resources/faq/how-the-online-score-is-calculated/","title":"How is the online score calculated?","first":"/node/","section":"FAQ's","content":"How is the online score calculated? #  The implementation matches the design doc here: Storage Node Downtime Tracking with Audits.\nIn production, we have 12-hour windows and a 30-day tracking period, which translates to 60 windows per tracking period and two windows a day. Every single audit the storage node gets will affect its online score to some extent. For example, if a node got audited during 30 seconds of downtime, that offline audit will have a negative effect on the online_score of the storage node. But other audits that happened inside the same 12 hour window will be equally weighted.\nSo in one 12-hour window, if a storage node gets 1 offline audit and 10 total audits, the online_score for that window will be 0.9. Then, the score for that window will be averaged with all the other windows in the 30-day tracking period to calculate the storage node\u0026rsquo;s overall online_score. So if this storage node had perfect uptime outside of the 12-hour window mentioned above, the online score would be approximately\n  \\[(59*1.0\u0026#43;1*0.9)/60 = 0.99833\\]  The online_score is reported back to nodes not immediately but with some delay (up to 12 hours), so it may not drop until long after the downtime happened.\nFor a more detailed description of the downtime tracking calculations, please refer to this blueprint.\n"},{"id":26,"href":"/dcs/concepts/access/encryption-and-keys/key-management/","title":"Key Management","first":"/dcs/","section":"Encryption Keys","content":"Key Management #  A Word of Caution on encryption keys #  At several points in the Documentation it’s important to point out three important things about your encryption keys. Please make sure you clearly understand how encryption keys are used on Storj DCS. You, your application and/or your users are responsible for managing your encryption keys.\nIf you lose your encryption keys, you have lost the ability to decrypt your data rendering it useless.  Thing 1: Your encryption keys are your data #  Storj DCS does not have access to your encryption keys. If you lose your encryption keys, they are gone. If you can’t decrypt your data, you’ve effectively lost it. All of it.\nThing 2: Make sure you backup your encryption keys #  It is very important you make sure to backup your encryption keys in a safe place. Storj DCS does not have any features or functions to back up encryption keys. We have a reference implementation of a user interface to ensure a user has backed up their encryption keys, but all of that happens client-side.\nThing 3: Secure your encryption keys #  This probably goes without saying, but be careful with how your app stores and transmits encryption keys.\nBy keeping encryption and access management separate, and by implementing client-side encryption, Storj DCS ensures that your data can’t be processed, mined, scanned by Storj or any unauthorized 3rd parties. If you don’t, it will end badly and Storj won’t be able to help.\nThing 4: Choose complex encryption passphrases. #  If your encryption key is easily guessable, or is leaked via some means, you will have to re-encrypt and re-upload all of your data to change your keys. This is a consequence of the encryption passphrase being controlled by you and being deterministic.\nTo try and help encourage users to have the right behavior, the access grant creation wizard on the Satellite dashboard will prompt first time users to create a 12 word passphrase.\\\n"},{"id":27,"href":"/node/dependencies/quic-requirements/linux-configuration-for-udp/","title":"Linux Configuration for UDP","first":"/node/","section":"QUIC requirements","content":"Linux Configuration for UDP #  If you are running your node on Linux, you might see warnings about the receive buffer size.\nUDP transfers on high-bandwidth connections can be limited by the size of the UDP receive buffer. This buffer holds packets that have been received by the kernel, but not yet read by the application. Once this buffer fills up, the kernel will drop any new incoming packet.\nOur software attempts to increase the UDP receive buffer size. However, on Linux, an application is only allowed to increase the buffer size up to a maximum value set in the kernel, and the default maximum value is too small for high-bandwidth UDP transfers.\nWe recommend increasing the maximum buffer size by running the following to increase it to ~2.5MB.\nsysctl -w net.core.rmem_max=2500000 To make this value persistent across reboots, run the following instead:\necho \u0026#34;net.core.rmem_max=2500000\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -w net.core.rmem_max=2500000 reference: udp receive buffer size in quic-go\n"},{"id":28,"href":"/dcs/getting-started/quickstart-uplink-cli/interacting-with-your-first-object/list-an-object/","title":"List an Object","first":"/dcs/","section":"Interacting With Your First Object CLI","content":"List an Object #  List the object in our bucket #  To view the cheesecake photo in our bucket, let\u0026rsquo;s use the list command:\nWindows ./uplink.exe ls sj://cakes Linux uplink ls sj://cakes macOS uplink ls sj://cakes  Result\n#  "},{"id":29,"href":"/dcs/api-reference/uplink-cli/meta-command/meta-get-command/","title":"meta get","first":"/dcs/","section":"meta","content":"meta get #  Usage #  Windows ./uplink.exe meta get PATH [KEY] [flags] Linux uplink meta get PATH [KEY] [flags] macOS uplink meta get PATH [KEY] [flags]  Flags #     Flag Description     --access string the serialized access, or name of the access to use   --help, -h help for get    Retrieve all metadata of an object #  Suppose you have uploaded your object with metadata using this command:\nWindows ./uplink.exe cp cheesecake.jpg sj://cakes --metadata \u0026#39;{\\\u0026#34;baker\\\u0026#34;:\\\u0026#34;cheeseman\\\u0026#34;, \u0026#34;\\pictur e-author\\\u0026#34;: \u0026#34;\\picman\\\u0026#34;}\u0026#39; Linux uplink cp cheesecake.jpg sj://cakes --metadata \u0026#39;{\u0026#34;baker\u0026#34;:\u0026#34;cheeseman\u0026#34;, \u0026#34;pictur e-author\u0026#34;: \u0026#34;picman\u0026#34;}\u0026#39; macOS uplink cp cheesecake.jpg sj://cakes --metadata \u0026#39;{\u0026#34;baker\u0026#34;:\u0026#34;cheeseman\u0026#34;, \u0026#34;pictur e-author\u0026#34;: \u0026#34;picman\u0026#34;}\u0026#39;  Retrieving all metadata defined for object sj://cakes/cheesecake.jpg is done with:\nWindows ./uplink.exe meta get sj://cakes/cheesecake.jpg Linux uplink meta get sj://cakes/cheesecake.jpg macOS uplink meta get sj://cakes/cheesecake.jpg  Query for a specific key in metadata #  You can retrieve the value of key baker for object sj://cakes/cheesecake.jpg using:\nWindows ./uplink.exe meta get sj://cakes/cheesecake.jpg baker Linux uplink meta get sj://cakes/cheesecake.jpg baker macOS uplink meta get sj://cakes/cheesecake.jpg baker  Querying for a non-existent key will raise an error.  "},{"id":30,"href":"/node/resources/faq/migrate-my-node/migrating-from-docker-cli-to-a-gui-install-on-windows/","title":"Migrating from Docker CLI to a GUI Install on Windows","first":"/node/","section":"How do I migrate my node to a new device?","content":"Migrating from Docker CLI to a GUI Install on Windows #  Migrating from Docker CLI to a GUI Install on Windows #  1. Make sure the Docker version is stopped and removed.\n2. Move orders from the data location to the installation folder location (\u0026quot;%ProgramFiles%\\Storj\\Storage Node\\orders\u0026quot; by default) (PowerShell):\nrobocopy /MIR /MOVE D:\\Storj\\orders \u0026#34;$env:ProgramFiles\\Storj\\Storage Node\\orders\u0026#34; 3. Point to the same exact storage folder where you were previously storing the data.\nDo NOT copy the path from the old config.yaml or source part of the --mount option of your Docker node where the storage subfolder was not explicitly included in the path.\nIt is better to specify the path to the storage subfolder with the Browse\u0026hellip; button.\n 4. Verify the complete path to the correct storage folder on your hard drive.\nIf you choose a different folder, your previously stored data will not be recognized, and your node will be disqualified.  Migrating from Docker CLI on Linux to a GUI install on Windows #  First you need to transfer both the identity and the data from the Linux installation to the new Windows device: How do I migrate my node to a new device?\nThen you can follow the Migrating from Docker CLI to a GUI Install on Windows guide.\n"},{"id":31,"href":"/dcs/api-reference/s3-compatible-gateway/multipart-upload/multipart-part-size/","title":"Multipart Part Size","first":"/dcs/","section":"Multipart Upload","content":"Multipart Part Size #  For best performance and cost with Storj DCS, you should plan to configure your AWS S3 client library to use a larger part size than standard. Not doing so could result in much higher fees.\nWe recommend 64MB.\n Background #  When an object is uploaded using Multipart Upload, a file is first broken into parts, each part of a Multipart Upload is also stored as one or more Segments. With Multipart Upload, a single object is uploaded as a set of parts.\nThe ideal part size for large files is 64MB, so that there is one Segment per part. Using a smaller Part size will result in a significant increase in the number of segments stored on Storj DCS. At large scale, this could impact both the performance and cost of your storage.\nLearn more about how small Part sizes can impact your storage costs.\nEach part is an integral portion of the data comprising the object. The object parts may be uploaded independently, in parallel, and in any order. Uploads may be paused and resumed by uploading an initial set of parts, then resuming and uploading the remaining parts. If the upload of any part fails, that part may be re-uploaded without impacting the upload of other parts.\nAll of these parts are broken into one or more Segments by the Storj DCS Gateway based on whether the Part Size is smaller or larger than the default Segment size. While Multipart Upload is most appropriate for files larger than the 64MB default Segment size, the Part Size is configurable in applications that use Multipart Upload.\nConfiguration for the AWS CLI #  If you are using the Amazon AWS CLI, you can configure it to use a larger part threshold as follows:\naws configure set default.s3.multipart_threshold 64MB "},{"id":32,"href":"/dcs/api-reference/s3-compatible-gateway/multipart-upload/","title":"Multipart Upload","first":"/dcs/","section":"Storj-hosted S3 Compatible Gateway","content":"Multipart Upload #  Multipart Upload is a function that allows large files to be broken up into smaller pieces for more efficient uploads. When an object is uploaded using Multipart Upload, a file is first broken into parts, each part of a Multipart Upload is also stored as one or more Segments. With Multipart Upload, a single object is uploaded as a set of parts.\nEach part is an integral portion of the data comprising the object. The object parts may be uploaded independently, in parallel, and in any order. Uploads may be paused and resumed by uploading an initial set of parts, then resuming and uploading the remaining parts. If the upload of any part fails, that part may be re-uploaded without impacting the upload of other parts.\nAll of these parts are broken into one or more Segments by the Storj DCS Gateway based on whether the Part Size is smaller or larger than the default Segment size. While Multipart Upload is most appropriate for files larger than the 64MB default Segment size, the Part Size is configurable in applications that use Multipart Upload.\nUsing Multipart Upload #  Multipart upload takes a single object and divides it into encapsulated pieces to be uploaded, with all the pieces representing the complete object. Once all the pieces are completely uploaded the platform will assemble the pieces into a single logical object. The purpose of providing multipart capability is to more easily resume and manage transfers of larger files so developers will want to take advantage of this capability within their applications.\nSpecific benefits for usage of multipart upload: #   Speed - Concurrently uploaded multiple pieces of a single object Streaming - When the size of the object is unknown you can upload parts of the object until specifically completing the operation. Resuming Operations - If connectivity is disrupted you can resume uploading pieces anytime after the multipart process is initiated.  Workflow for Multipart upload #  As described below, mutlipart upload is a process consisting of: starting the upload, transferring each piece, and finally completing the multipart upload. Upon successful upload of the final piece, Storj DCS will logically reassemble the object, apply metadata and make the object accessible. During the multipart upload operation, you can get status of active upload operations and get lists of parts you have uploaded. More detail on multipart operations is provided in the sections below.\nInitiate Multipart upload #  At the start of a multipart upload, Storj DCS will return an ID that you use to reference your multipart upload; you need to include this ID when working with the object. Operations such as uploading parts, listing parts and canceling the multipart operation.\nUpload Part #  While uploading an object part you need to specify the ID received when you created the multipart upload along with a unique part number that your specific call will be sending to the platform. Because of the capability of multipart upload to work non sequentially, you can upload any part at any time before the multipart upload is completed. Additionally, you can over right existing parts numbers you\u0026rsquo;ve previously transferred as long as the multipart upload has not been completed. Part numbers are chosen by the client and are between 1 and 2^31. When a part is uploaded, the Storj DCS platform will return several items, one of which is an ETag. To complete the multipart upload process you will need to provide a list of part IDs and their corresponding ETags.\nBe advised that billing occurs when data is stored on the Storj DCS platform - as such - when you initiate a multipart upload and begin uploading parts, charges will be applied based on the amount of space the parts occupy. Billing will occur regardless of a multipart upload being completed.\nCompleting a multipart upload #  When you upload the final part of a multipart upload you need to call the complete operation to tell Storj DCS to reconstitute the object from the individual parts you have uploaded. When the complete operation concludes all metadata and individual parts will be consolidated into a single object.\nTo call the complete operation you need to provide the list of ETags and their corresponding part IDs. You should maintain this list in your application.\nIf you decide to cancel the multipart operation, you must provide the object key and multipart ID you received when you initiated the multipart upload. Space that was used during the multipart process will be freed when all the active multipart transactions have been completed and the abort operation has been called.\nListing for multipart uploads #  During a multipart upload you can list active upload transactions or the parts that have been successfully uploaded. For a single list parts request, storage DCS will return information up to a maximum of 1000 parts. For objects with more than 1000 parts, multiple requests are required. Because of the distributed nature of Storj DCS, you should not use the response from listing multipart uploads as input to complete in multipart upload operation. Be advised that part listing requests will only return completed part uploads - any active part uploads will not be returned.\nMultipart upload for distributed systems #  Depending on the behavior of your application it may be possible for multiple concurrent operations to be performed on the same object key. It may occur that an application that sends multiple requests for the same object during a multipart operation would cause that object to ultimately not be accessible. One example would be permission changes or object removal while a multipart upload is in process for a specific object. The multipart operation may successfully complete , however, the operations received on that multipart object would change the availability of that object when the multipart upload is completed.\nCost for Multipart upload #  When a multipart upload is initiated, storage DCS processes and makes available all constituent parts of the multipart object. Multipart object is either made available by completing the multipart upload process or cancelled by explicitly stopping the multipart upload process through API call. Storj DCS will measure resource usage on multipart upload operations unless they are explicitly cancelled. only when a multipart upload is explicitly cancelled are the associated resources freed.\nMultipart upload limits #     Item Limit     Maximum object size No practical limit.   Maximum number of parts per upload 2^31   Part numbers 0 to 2^31   Part size No practical limit.   Maximum number of parts returned for a list parts request 1000   Maximum number of multipart uploads returned in a list multipart uploads request 1000    "},{"id":33,"href":"/node/","title":"Node Operator","first":"/node/","section":"Storj Docs","content":"Introduction #  👋 New to Storj?  #  Have unused hard drive capacity and bandwidth? #  Storj pays you for your unused hard drive capacity and bandwidth in STORJ tokens! Get started with Hosting a Node.\nWant to store data on the decentralized object storage network? #   Learn about Storj DCS, the enterprise, production-ready version of the Storj network, complete with guaranteed SLAs.\n📨 Received an Authorization Token? #  Authorization Token ❓ Need Help?  #  For questions, please join our forum .\n💻 Want to Contribute?  #  All of our code for Storj is completely open source. Have a code change you think would make Storj better? Please send a pull request along! Make sure to sign our Contributor License Agreement (CLA) first. See our license section for more details.\n"},{"id":34,"href":"/node/dependencies/port-forwarding/","title":"Port Forwarding","first":"/node/","section":"Dependencies","content":"Port Forwarding #  Most, if not all ISPs give a dynamic IP address, which means your IP can change at any time. As a work around, you need a dynamic DNS service to ensure your storage node is connected.\nAdvanced: Already have a Static IP? Configure Port Forwarding ↓\nSetup Dynamic DNS Service: Hostname Configuration #  If you don\u0026rsquo;t have a static IP, the first step is setting up a hostname.\nThere are various services available that allow you to set up a DDNS hostname. The following steps will guide you through doing this with one of the services, NoIP.\nCreate a Free Hostname using NoIP #  Add a free hostname using NoIP which needs to be renewed every 30 days when using a free account. On the NoIP website, scroll down to “Create Your Free Hostname now”, then do the following:\n In the hostname input field, select a hostname of your liking (e.g. mystoragenode), it can contain letters and numbers. Next, select a hostname provider of your liking (e.g. “. ddns.net”) in the box to the right. Click “Sign Up\u0026quot;.  On the sign-up page, enter your email, username, and password. Make sure to write these details down, as you will need them later.\nOnce you have created an account and clicked the confirmation link in the e-mail, scroll down to where it says “How to remote access your device” and click “Get started with Dynamic DNS.”\nThis will take you to the NoIP dashboard.\nSetup Dynamic DNS Service: Dynamic Update Client Tool #  You\u0026rsquo;ll need to setup a dynamic update tool, which automatically tracks and assigns the IP address to your hostname. So, if your public IP changes, your node won’t get disconnected from the network.\nPlease keep your hostname at hand as we will need it later.\nIf your router supports NoIP\u0026rsquo;s Dynamic DNS, we highly recommend configuring your router. Here\u0026rsquo;s how to do that.  Windows Dynamic Update Client for Windows #  1. Download and install with link above.\n2. Enter your NoIP credentials.\n3. Select the hostname you created earlier and click \u0026ldquo;save\u0026rdquo;.Linux Dynamic Update Client for Linux #  First, open a new terminal window so you can type commands into Linux. Once you’ve opened your terminal window, log in as “root” user. You can become the root user from the command line by entering sudo -s followed by the root password on your machine.\nNavigate to src folder:\ncd /usr/local/src/ Download the necessary files:\nwget http://www.no-ip.com/client/linux/noip-duc-linux.tar.gz Extract the files using the following command:\ntar xf noip-duc-linux.tar.gz Navigate to the folder:\ncd noip-2.1.9-1/ Once in the directory, type:\nmake install You’ll then be prompted to log in with your NoIP account username and password.\nIf you get “make not found” or “missing gcc” then you don’t have the gcc compiler tools installed. Install the gcc compiler tools to proceed.  As root again (or with sudo) issue this command:\n/usr/local/bin/noip2 -C You’ll then be prompted for your NoIP username and password, as well as the hostnames you wish to update.\nOne of the questions is, “Do you wish to update ALL hosts?” If answered incorrectly, this could affect hostnames in your account that are pointing at other locations.  Now that the client is installed and configured, you just need to launch it. Simply issue this final command to launch the client in the background:\n/usr/local/bin/noip2 By default, the DUC software will not start when you reboot your system.  Check the README file in the no-ip-2.1.9 folder for instructions on how to make the client run at startup. This varies depending on what Linux distribution you are running.\nmacOS Dynamic Update Client for Mac #  1.Download the most recent version above.\n2. Open the downloaded file and drag the NoIP icon to the applications folder.\n3.Open the applications folder and double-click on the NoIP DUC icon.\n4. Enter your NoIP username and password and Log In.\n5. Select the hostnames you would like updated.\nOnce complete, you can close the Select a Hostname window.\n Setup Port Forwarding: Router Port Forwarding Configuration #  To set up port forwarding (TCP/UDP) on a router, we must first get the gateway IP address so we can access the router:\nWindows Open command prompt and run the following command\nipconfig | findstr /i \u0026#34;Gateway\u0026#34; Linux Open the terminal app and run the following command\nip route | grep default macOS Open the terminal app and run the following command\n netstat -nr | grep default  Once you have your gateway IP, open a browser and type it in.\nYou\u0026rsquo;ll be forwarded to a log in page of your router.\nIf you don\u0026rsquo;t know the log in, try googling the default username and password for your router\u0026rsquo;s make and model.\nOnce you are logged in to your router, find the port forwarding tab (for some routers it\u0026rsquo;s in \u0026ldquo;advanced settings\u0026rdquo;).\nNow, you will need get the local IP of your machine the node is running on:\nWindows Open a command prompt and run the following command:\nipconfig | findstr /i \u0026#34;IPv4\u0026#34; Linux Open the terminal app and run the following command:\nhostname -I The local IP will be the first set of numbers.\nmacOS Open the terminal app and run the following command:\nIf you are connected to a wireless network:\nipconfig getifaddr en0 If you are connected via Ethernet:\nipconfig getifaddr en1  Next, go back to your routers port forward page and add a new rule for port 28967 with the IPv4 address you just retrieved.\nWindows Make Sure to Add a Firewall Rule. #  Your Node will most likely read offline if there is not a firewall rule set in place. To do that, run a Powershell as Administrator and execute:\nNew-NetFirewallRule -DisplayName \u0026#34;Storj v3 TCP\u0026#34; -Direction Inbound –Protocol TCP –LocalPort 28967 -Action allow New-NetFirewallRule -DisplayName \u0026#34;Storj v3 UDP\u0026#34; -Direction Inbound –Protocol UDP –LocalPort 28967 -Action allow As an alternative, you can manually access it through Windows Firewall:\nSearch for \u0026ldquo;firewall\u0026rdquo; in your start bar on the lower left of your screen and click on \u0026ldquo;Windows Firewall with Advanced Security\u0026rdquo;\nA new window will appear. On the left of the screen, select \u0026ldquo;Inbound Rules\u0026rdquo;\nOnce selected, on the right side of the screen click on \u0026ldquo;New Rule\u0026rdquo;\nA new window will open. Under Rule Type select \u0026ldquo;Port\u0026rdquo; and click \u0026ldquo;Next\u0026rdquo;\nSelect TCP and specify the port you wish to use (default is 28967), then click \u0026ldquo;Next\u0026rdquo;\nSelect \u0026ldquo;Allow the connection\u0026rdquo; then click \u0026ldquo;Next\u0026rdquo;\nLeave all checkmarks checked, then click \u0026ldquo;Next\u0026rdquo;\nEnter a name for the new rule, and description if you\u0026rsquo;d like, then click \u0026ldquo;Finish\u0026rdquo;\nBe sure to repeat the above steps to also create a new firewall rule for UDP. See QUIC requirements\nCongratulations, you\u0026rsquo;ve set up port forwarding! #  Linux Congratulations, you\u0026rsquo;ve setup port forwarding! #  macOS Congratulations, you\u0026rsquo;ve set up port forwarding! #   Have any difficulties? #  Search for port forwarding instructions for your exact router model.\n"},{"id":35,"href":"/dcs/getting-started/quickstart-uplink-cli/prerequisites/","title":"Prerequisites","first":"/dcs/","section":"Quickstart - Uplink CLI","content":"Prerequisites #  Create an Account #   Click on Start for free\nSelect a Satellite, and sign up for an account.\nAfter sign up, you will receive a verification email. Click the button prompting you to verify your email.\nTo get started, the following steps will take you through the process of uploading your first object via the CLI Tool so that you can see how objects uploaded to the network are distributed across the nodes.\nDownload and install Uplink CLI #  "},{"id":36,"href":"/dcs/getting-started/quickstart-uplink-cli/sharing-your-first-object/prerequisites/","title":"Prerequisites","first":"/dcs/","section":"Sharing Your First Object","content":"Prerequisites #   You need to have the satellite account and installed Uplink CLI.\nIf you have followed the previous tutorial, you already have a cakes bucket. If you don\u0026rsquo;t, simply create it and re-upload using the following command:\nWindows ./uplink.exe mb sj://cakes Linux uplink mb sj://cakes macOS uplink mb sj://cakes  Let\u0026rsquo;s (re)upload our cheesecake image:\nWindows ./uplink.exe cp ~/Desktop/cheesecake.jpg sj://cakes Linux uplink cp ~/Desktop/cheesecake.jpg sj://cakes macOS uplink cp ~/Desktop/cheesecake.jpg sj://cakes  "},{"id":37,"href":"/dcs/storage/considerations/","title":"Product Overview","first":"/dcs/","section":"Decentralized Cloud Storage","content":"Product Overview #  Storj DCS (Decentralized Cloud Storage) is an encrypted, secure, and affordable object storage service that enables you to store, back up, and archive large amounts of data to the decentralized cloud.\nTo learn how files are uploaded to Storj DCS see how it works.\nHow Decentralized Cloud Storage Works #  When you upload your data to Storj DCS, your data is encrypted, erasure-coded, and spread across a network of statistically uncorrelated peers (Storage Nodes).\nBasic Tools for Storj Decentralized Cloud Storage #  The ways you can interact with Storj DCS include:\n Satellite Dashboard - The Satellite Dashboard allows you to manage projects and access for your data. Uplink CLI: The command-line interface allows you to upload and download files from the network, manage permissions and sharing. Gateway: A service that provides a compatibility layer between other object storage services such as Amazon S3. Client Libraries: The Storj DCS client libraries allow you to manage your data using your preferred language, including Go, Android, Xamarin (.NET/C#), and C. More language bindings coming soon!  Use cases for Storj DCS #  Common Use Cases "},{"id":38,"href":"/dcs/billing-payment-and-accounts-1/storj-token/promotional-credits/","title":"Promotional Credits","first":"/dcs/","section":"Payment Methods","content":"Promotional Credits #  The Storj DCS Service includes incentive programs to encourage adoption and use of the platform. These incentives are operationalized as Promotional Credits and include Credits and Coupons.\nCredits #  Credits are a bonus amount that is deposited to a user’s account that the user receives when paying in STORJ token. Credits may be offered from time to time, at varying percentages, at our sole discretion. You will be notified at the time of your deposit if a Credit is offered. Credits are non-refundable and unused Credits expire when you close the account.\nCoupons #  “Coupons” are a bonus amount that is deposited to a user’s account that the user may receive as part of a marketing promotion undertaken by us. Like typical coupons, these may be used only for the established period of time to pay for usage fees. Coupons may be offered from time to time, in varying amounts, at the sole discretion of Storj Labs. You will be notified if you receive a Coupon. Coupons are non-refundable, and unused Coupons expire when a user closes an account or at the expiration of the Coupon.\n"},{"id":39,"href":"/node/sno-applications/qnap-storage-node-app/","title":"QNAP Storage Node App","first":"/node/","section":"SNO Applications","content":"QNAP Storage Node App #  In this guide, we are going to walk through the installation process for the Storage Node QNAP application for your QNAP NAS Device – enabling you to:\n Monetize your excess capacity on the Storj Network Back up your NAS Device to the Network using Gateway-MT and any S3-compatible backup solution  For a video walkthrough of this process, please see below:\n  Prerequisites: #   Storage Node Identity Download the QNAP Binary  Sharing Capacity Prerequisites #  First, navigate to QNAP App Center application, by double clicking on its icon on the QNAP Desktop Homepage.\nNext, click \u0026ldquo;Install Manually\u0026rdquo; and click on the QNAP Package (downloaded above) to install the QNAP Storj application onto the device.\nBefore installing, you will be prompted to accept that the application has no official digital signature: hit \u0026ldquo;accept\u0026rdquo;\nOnce the installation is completed, the Storj Storage Node app will be available through the App Center, as well as be visible on the desktop.\nDon\u0026rsquo;t see the Storj app?\nGo to “Control Panel -\u0026gt; Privileges”, double click the storage node app and put a checkmark in the “Allow” box of the user you\u0026rsquo;re logging in with. After that it should appear in the start menu.\n Open the application, and click \u0026ldquo;Wizard\u0026rdquo; in the sidebar. You will be prompted with a walkthrough of the application, like so:\nFirst, enter your email address that you would like associated with notifications for your node:\nThen, add your ERC-20 Token Compatible Wallet Address for payouts.\nAfter, configure Storage Allocation, and set the amount of excess storage capacity you would like to share with the network.\nAfter, input a selected Storage Directory (which specifies the path where the data will be stored).\nNext, configure the external Port Forwarding (this allows the connection for the Storj Network to come in, and communicate with the software running on the NAS).\nAfter, set the location for your node\u0026rsquo;s identity certificate, like so:\nHit finish, and you have successfully completed that setup wizard for your QNAP device.\nRun the Application #  After following the configuration steps above, click \u0026lsquo;Start My Storage Node\u0026rsquo; and look for the log output, which indicates that the image is running.\nYou have successfully shared your excess capacity on your QNAP Device with the Storj Network.\nClick the Dashboard Option to view your current stats - and keep an eye on that payout address!\n"},{"id":40,"href":"/dcs/getting-started/quickstart-guide/","title":"Quickstart Guide","first":"/dcs/","section":"Getting Started","content":"Quickstart Guide #  Suggested Options #  Rclone via Gateway MT - Interact with our S3 Hosted Gateway via Rclone. This is ideal for large files and industry compatibility.\nRclone with Hosted Gateway AWS CLI via Gateway MT - Interact with our S3 hosted gateway using the official AWS CLI. Ideal for high-performance uploads and large files.\nQuickstart - AWS CLI and Hosted Gateway MT Alternative Options #  Web Object Browser - Allows for uploading and managing files directly through the browser with no command-line tool required.\nQuickstart - Object Browser Uplink CLI - Our client-side application that allows you to access Object Storage from the command line. Use this tool to upload and manage objects and buckets.\nQuickstart - Uplink CLI "},{"id":41,"href":"/dcs/how-tos/sync-files-with-rclone/rclone-with-native-integration/","title":"Rclone with Native Integration","first":"/dcs/","section":"Sync Files With Rclone","content":"Rclone with Native Integration #  Selecting an Integration Pattern #  Use our native integration pattern to take advantage of client-side encryption as well as to achieve the best possible download performance. Uploads will be erasure-coded locally, thus a 1gb upload will result in 2.68gb of data being uploaded to storage nodes across the network.\nUse this pattern for #   The strongest security The best download speeds  Setup #  First, Download and extract the rclone binary onto your system.\nExecute the config command:\nrclone config A text-based menu will prompt. Type n and hit Enter to create a new remote configuration.\ne) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q\u0026gt; n Enter a name for the new remote configuration, e.g. waterbear.\nname\u0026gt; waterbear Option Storage. Type of storage to configure. Choose a number from below, or type in your own value. A long list of supported storage backends will prompt. Enter storj (Option 36) and hit Enter.\nOption Storage. Type of storage to configure. Choose a number from below, or type in your own value. ... 36 / Storj Decentralized Cloud Storage \\ (storj) ... Storage\u0026gt; storj Choose your authentication method: existing access grant or new access grant from API Key ( Access token).\nStorage\u0026gt; storj Option provider. Choose an authentication method. Choose a number from below, or type in your own string value. Press Enter for the default (existing). 1 / Use an existing access grant. \\ (existing) 2 / Create a new access grant from satellite address, API key, and passphrase. \\ (new) provider\u0026gt; If you selected to authenticate with an existing access grant, enter the serialized access grant you have received by someone else.\nprovider\u0026gt; 1 Option access_grant. Access grant. Enter a value. Press Enter to leave empty. access_grant\u0026gt; 1cC... -------------------- [waterbear] type = storj access_grant = 1cC... -------------------- y) Yes this is OK (default) e) Edit this remote d) Delete this remote y/e/d\u0026gt; If you selected to authenticate with a new access grant, first enter the satellite address by selecting one from the list or enter the address of a 3rd-party satellite.\nprovider\u0026gt; 2 Option satellite_address. Satellite address. Custom satellite address should match the format: `\u0026lt;nodeid\u0026gt;@\u0026lt;address\u0026gt;:\u0026lt;port\u0026gt;`. Choose a number from below, or type in your own string value. Press Enter for the default (us-central-1.storj.io). 1 / US Central 1 \\ (us-central-1.storj.io) 2 / Europe West 1 \\ (europe-west-1.storj.io) 3 / Asia East 1 \\ (asia-east-1.storj.io) satellite_address\u0026gt; If you enter the a 3rd-party satellite, the address must include also the node ID of the satellite. This is required to establish a secure connection with the satellite.  The second step of creating a new access grant is to enter your generated API key.\nOption api_key. API key. Enter a value. Press Enter to leave empty. api_key\u0026gt; 1Cjfjf... The final step of creating a new access grant is to enter your encryption passphrase.\nOption passphrase. Encryption passphrase. To access existing objects enter passphrase used for uploading. Enter a value. Press Enter to leave empty. passphrase\u0026gt; your-secret-encryption-phrase The passphrase is used for encrypting and decrypting the data stored on Storj DCS (formerly known as Tardigrade). If you have any data previously uploaded to this project, you must enter the same passphrase in order to download it successfully.  A summary of the remote configuration will prompt. Type yand hit Enter to confirm it.\n[waterbear] type = storj satellite_address = 121RTSDpyNZVcEU84Ticf2L1ntiuUimbWgfATz21tuvgk3vzoA6@asia-east-1.tardigrade.io:7777 api_key = 1Cjfjf... passphrase = your-secret-encryption-phrase access_grant = 1E1F... -------------------- y) Yes this is OK (default) e) Edit this remote d) Delete this remote y/e/d\u0026gt; y Now you should see one remote configuration available. Enter q and hit Enter to quit the configuration wizard.\nCurrent remotes: Name Type ==== ==== waterbear storj e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q\u0026gt; q For additional security, you should consider using the (s) option\nSet configuration password option. It will encrypt the rclone.conf configuration file. This way secrets like the API Key ( access token), the encryption passphrase, and the access grant won\u0026rsquo;t be stolen if an attacker get access to your configuration file.\n Create a Bucket #  Use the mkdir command to create new bucket, e.g. mybucket.\nrclone mkdir waterbear:mybucket List All Buckets #  Use the lsf command to list all buckets.\nrclone lsf waterbear: Note the colon (:) character at the end of the command line.  Delete a Bucket #  Use the rmdir command to delete an empty bucket.\nrclone rmdir waterbear:mybucket Use the purge command to delete a non-empty bucket with all its content.\nrclone purge waterbear:mybucket Upload Objects #  Use the copy command to upload an object.\nrclone copy --progress ~/Videos/myvideo.mp4 waterbear:mybucket/videos/ The --progress flag is for displaying progress information. Remove it if you don\u0026rsquo;t need this information.  Use a folder in the local path to upload all its objects.\nrclone copy --progress ~/Videos/ waterbear:mybucket/videos/ Only modified files will be copied.  List Objects #  Use the ls command to list recursively all objects in a bucket.\nrclone ls waterbear:mybucket Add the folder to the remote path to list recursively all objects in this folder.\nrclone ls waterbear:mybucket/videos/ Use the lsf command to list non-recursively all objects in a bucket or a folder.\nrclone lsf waterbear:mybucket/videos/ Download Objects #  Use the copy command to download an object.\nrclone copy --progress waterbear:mybucket/videos/myvideo.mp4 ~/Downloads/ The --progress flag is for displaying progress information. Remove it if you don\u0026rsquo;t need this information.  Use a folder in the remote path to download all its objects.\nrclone copy --progress waterbear:mybucket/videos/ ~/Downloads/ Delete Objects #  Use the deletefile command to delete a single object.\nrclone deletefile waterbear:mybucket/videos/myvideo.mp4 Use the delete command to delete all object in a folder.\nrclone delete waterbear:mybucket/videos/ Print the Total Size of Objects #  Use the size command to print the total size of objects in a bucket or a folder.\nrclone size waterbear:mybucket/videos/ Sync Two Locations #  Use the sync command to sync the source to the destination, changing the destination only. Doesn’t transfer unchanged files, testing by size and modification time or MD5SUM. Destination is updated to match source, including deleting files if necessary.\nrclone sync --progress ~/Videos/ waterbear:mybucket/videos/ The --progress flag is for displaying progress information. Remove it if you don\u0026rsquo;t need this information.  Since this can cause data loss, test first with the --dry-run flag to see exactly what would be copied and deleted.  The sync can be done also from Storj DCS to the local file system.\nrclone sync --progress waterbear:mybucket/videos/ ~/Videos/ Or between two Storj DCS buckets.\nrclone sync --progress waterbear-us:mybucket/videos/ waterbear-europe:mybucket/videos/ Or even between another cloud storage and Storj DCS.\nrclone sync --progress s3:mybucket/videos/ waterbear:mybucket/videos/ "},{"id":42,"href":"/node/setup/gui-windows/storage-node/","title":"Storage Node","first":"/node/","section":"GUI Install - Windows","content":"Storage Node #  1) Download the Windows MSI installer\n2) Extract the MSI from the zip and double click it. The installation window will open.\n3) Accept the terms of our end-user license agreement.\n4) Select the folder you would like the Storage Node software to be installed in.\n5) Select the folder your Identity is stored in. When you generate your Storage Node Identity, the identity tool will show you what folder it was saved in. Please see the Identity section if you have not created one yet.\n6) Enter your ERC-20 compatible wallet address where you want to receive your STORJ token payouts.\n7) Enter your email address to receive alerts about software updates and new features.\n8) Enter your external IP address and port (\u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt;). You can find more information about how to configure this in our Port Forwarding section.\n9) Select where you would like the network data to be stored on your machine.\nThe network-attached storage location could work, but it is neither supported nor recommended!  Moving an existing node on a CLI Setup to a Windows GUI Setup? Follow these instructions.  10) Select how much storage space you would allocate to the network. We require a minimum of 500 GB plus 10% overhead to be allocated.\n11) Click the install button to install your Storage Node with what you configured in the previous steps.\n12) Wait for a few moments while the Storage Node is being installed on your machine.\n13) Click the Finish button to exit the installer. Congrats, your Storage Node is now live!\nNote: The Storage node and auto updater are installed as a Windows service. If you want to stop or restart the storage node you have to open the windows service page to do so.  "},{"id":43,"href":"/dcs/api-reference/s3-compatible-gateway/","title":"Storj-hosted S3 Compatible Gateway","first":"/dcs/","section":"SDK \u0026 Reference","content":"Storj-hosted S3 Compatible Gateway #  S3 compatibility #  When Amazon launched its S3 service 15 years ago and created the cloud storage industry, it also unknowingly made object storage the standard for storing data in the cloud. Object storage organizes data into objects, which contain the data itself, its metadata, and a unique identifier. These objects are stored in buckets rather than a hierarchical system. Since then, the grand majority of cloud storage services have reinforced this interface, and the majority of people storing data in the cloud use similar architectures.\nAmazon S3 is accessed via APIs, most of which rely on the HTTP protocol and XML serialization. By making a storage system compatible with these APIs, it makes it much easier for users to migrate to new services without much effort. All you have to do is point files to the new buckets and migrate any static data you’d like to keep. For example, core features such as basic upload and download, of course, should map quite easily to the new ecosystem, including systems like Storj. We support organizing objects by bucket and key, all HTTP verbs including HEAD, byte-range fetches, as well as uploading files in multiple parts.\nSee the compatibility table for GatewayST and S3-compatibility list of GatewayMT.\nSecurity and encryption #  Where the Storj network really excels compared to centralized providers is in its privacy and security, so we’d be remiss in not addressing these topics specifically as they pertain to S3.\nThe distributed security tokens (access grants) that Storj typically uses (via libuplink, etc.) contain too much detail to fit into an S3 access key or secret key field. Storj offers an S3-specific authorization service, which maps S3-compatible credentials to a Storj access grant. This service saves access grants encrypted into a database. The access grants are individually encrypted using information from the much shorter returned access key, which is not stored in our auth service. Access grants never remain decrypted longer than they are needed, and only a hash of the access key is ever persisted. In short, the system is designed to protect your data at rest.\nBenefits #  Overall, our S3 compatibility project has been a huge effort to address the needs of certain customers, making it easier than ever to migrate to the decentralized cloud. It should provide bandwidth-limited customers with more than three times faster access. It provides a drop-in replacement for S3 with the great majority of use-cases. Finally, it offers users the flexibility to dial in the balance of security versus accessibility, allowing access to files directly from web browsers in ways they never could before.\nUsage #  To save on costs and improve performance, please see this important note on multipart upload part sizes.  There are two primary ways to get started using our hosted S3 gateway and get issued an S3 compatible access key, secret key, and endpoint.\nUsing via the Web interface #  The first main way is to use the web wizard in the Satellite web interface. The web wizard is simple and easy to use, but lacks some configurability, like the ability to restrict to specific prefixes within a bucket or use a different auth service. If you need those features, consider the CLI, farther below.\nAfter logging in, create a new access grant and select \u0026ldquo;continue in browser.\u0026rdquo;\nOnce you have created an access grant, make sure to select the \u0026ldquo;Generate S3 credentials\u0026rdquo; option, at the bottom.\nYou can now use the generated access key, secret key, and endpoint in your AWS S3-supporting application.\nCLI #  The CLI is not as easy, but is more flexible in allowing you to control what specific encrypted paths the Gateway has access to. Via the Uplink CLI, you\u0026rsquo;ll want to run the uplink share command with the --register option.\nuplink share is a flexible command that allows you to restrict and generate new access grants with a variety of restrictions. By adding --register, the uplink will use the default --auth-service flag to determine where to exchange the restricted access grant for an access key, secret key, and endpoint.\nHere is an example command that exposes read-only access to the gateway for one specific prefix in a bucket:\nuplink share --readonly=true --register sj://bucket/prefix/ This will output an access key, a secret key, and an S3 compatible endpoint for you to use.\nRegions and Points of Presence #  We currently have hosted Gateways in 3 regional locations and expect to expand as needed. Gateway endpoints that currently exist are:\n  https://gateway.us1.storjshare.io  https://gateway.ap1.storjshare.io  https://gateway.eu1.storjshare.io  These are all interchangeable, and use the same encrypted database of access keys. Use whichever one is closest to you!\nSource code #  All of the code for this feature (and the auth service access key database) lives here: https://github.com/storj/gateway-mt. If you want to run your own multitenant gateway and encrypted database of access keys, please do, and let us know if you have any problems.\nIf you are looking to self-host, you should also consider the self-hosted single-tenant S3 gateway, which is much easier to set up and run.\n"},{"id":44,"href":"/dcs/support/support-process-overview/","title":"Support Overview","first":"/dcs/","section":"Support","content":"Support Overview #  We want all of the users of Storj DCS to be successful and take advantage of the benefits decentralized cloud storage offers. We understand that we\u0026rsquo;ll have a users with a range of different levels of experience, different use cases and different technologies being integrated. If you\u0026rsquo;re having an issue or wondering where to get started, we\u0026rsquo;ve got a couple of ways we can help.\nStart with the Documentation #  There is a lot of information in the Documentation. The foundational information in under Concepts can be really helpful if you\u0026rsquo;re new to Storj DCS. Most people start with the Getting Started Section. If you\u0026rsquo;re already an experienced developer, maybe you just want to head straight on over to the SDK and Reference section or to the Guides. If you are a solution architect trying to figure out how Storj DCS fits into your application stack, try the Solution Architecture section.\nIf you still need help, we\u0026rsquo;ve got a few more options.\nAsk for Help in the Storj Forum #  The Storj Community is a great place to meet other people who are developing using Storj DCS and ask questions. You\u0026rsquo;ll find people with tremendous knowledge about the Storj DCS service, including Storj engineers. If you\u0026rsquo;re stuck on an issue, chances are someone has seen it before or can help you troubleshoot it. Join, browse, ask and answer! There are people on from all over the world, and members of the community, our community mods and Storj engineers are generally pretty quick to respond.\nOk, and if that\u0026rsquo;s not what you\u0026rsquo;re looking for, try the Support KB.\nCheck out the Support Knowledge Base #  We\u0026rsquo;ve gotten a lot of questions over the last few years and we\u0026rsquo;ve come up with some pretty good answers. A lot of those answers have made it into the documentation or are part of a longer discussion in the Forum. Search the Support KB for your issue and see if there is self-service information to get you unblocked.\nAlright, and if the Support KB doesn\u0026rsquo;t have what you need, then maybe it\u0026rsquo;s time to reach out to our support team.\nSubmit a Support Ticket #  Sometimes you just need help, and it\u0026rsquo;s ok to ask. Our support process, responsibilities and hours are published in our Terms of Service. Just for convenience, our support hours are:\n Between 9:00 A.M. to 8:00 P.M. Eastern Time on Business Days for Severity Level 1 Errors Between 9:00 A.M. to 5:00 P.M. Eastern Time on Business Days for other Support Requests  You can submit a ticket via our Support Portal or via our Support Twitter handle - @StorjSupport.\n"},{"id":45,"href":"/dcs/concepts/overview/","title":"Understanding Storj DCS","first":"/dcs/","section":"Concepts","content":"Understanding Storj DCS #  Overview #  Storj makes open-source software that anyone can run - individuals with a Network Attached Storage Device or NAS, those with desktop computers that are always on, businesses, or data centers - that allows these users to share unused disk drive space and bandwidth with the network. Our software aggregates all of that storage capacity and bandwidth to create an extremely secure, private, and durable cloud storage service for developers.\nStorj makes that storage and bandwidth available as a distributed cloud object storage service for developers, with an enterprise-grade 99.95% availability, eleven 9s of durability, and S3 compatibility under the Storj DCS brand.\nPeer Classes #  There are 3 main components, or peer classes, on the network - Storage nodes, the application that enables people to share excess hard drive capacity and bandwidth with the network, Uplink Clients, developer tools (sometimes hosted) to upload and download data, and finally, the Satellite, a hosted set of services that handles access management, metadata management, storage node reputation, and data repair, as well as billing and payment.\nThe Storage Node stores data for others, and gets paid for storage and bandwidth. All data stored on storage nodes is client-side encrypted and erasure-coded.\nUplink Clients enables developers to store data on Storj DCS, handling end-to-end encryption from the client-side (by default), and erasure coding, where files are split into 80 pieces then distributed across our network of storage nodes. Each of the 80 pieces is stored on different, diverse storage nodes, with different operators, power supplies, networks, and geographies, etc. - yielding tremendous security, performance, and durability advantages. The client-side managed, end-to-end encryption combined with our edge-based access management capabilities provide easy-to-use tools for building applications that are more private, more secure, and less susceptible to a range of common attack vectors. Uplink clients include both our self-hosted and Storj-hosted S3 compatible gateways, the CLI, and the libuplink Go library and bindings.\nThe Satellite is a set of hosted services that handles developer account access, API access management, metadata management, storage node reputation, data audit, and data repair, as well as billing developers and payment for Storage Nodes. Storj Labs satellites are operated under the Storj DCS brand.\nHow it\u0026rsquo;s Different #  To give you a sense of how the Storj DCS service is different, it’s helpful to describe what happens with a round-trip upload and download of a file. Files are uploaded using an Uplink client, whether directly using the CLI or S3 compatible gateway, or indirectly using a tool that has integrated the libuplink library, such as FileZilla, Rclone or Restic.\nWhat Happens When You Upload #  When a file is uploaded, it’s first encrypted by the Uplink client using an encryption key held by that client. Next, it’s erasure-coded, meaning it’s broken up into at least 80 pieces, of which only 29 (any 29) are required to reconstitute a file. (The redundancy from erasure coding is far more efficient than replicating files and this technology used by most data storage systems, including DVDs, which is why you can still watch a movie even if there are scratches and fingerprints on the disk.)\nThe Uplink Client then contacts the satellite to get a list of Storage Nodes on which to store the pieces. The satellite returns more than 80 Storage Node addresses. The Uplink Client uploads pieces peer-to-peer, in parallel, directly to the Storage Nodes. The Uplink client stops attempting to upload pieces once 80 pieces have been successfully uploaded to at least 80 Storage Nodes.\nThe Uplink Client attempts a few more than 80 during the upload process to eliminate any long-tail effect and the related latency from the slowest Storage Nodes.\nWhat Happens When You Download #  When the Uplink Client downloads a file, it’s essentially the same process as an upload but in reverse. The Uplink Client requests a file from the Satellite and the Satellite returns a list of 35 Storage Nodes from which the Uplink Client can retrieve the pieces of the file. The Uplink Client starts attempting to download pieces from all 35 Storage Nodes, again, stopping once it has retrieved the 29 pieces needed to reconstitute the file after eliminating latency from the long-tail effect. The pieces are re-encoded and then decrypted by the Uplink client as only it has the encryption key.\nWhy Developers Love Storj DCS #  Even with the sophisticated privacy and security features and the default multi-region availability, developers love Storj DCS because it\u0026rsquo;s extremely easy for them to get started and build more secure and private applications.\nWhen uploading and downloading files, a developer or app issues a simple cp command - the Uplink Client does the rest, abstracting all of the complexity of encryption, erasure coding, and distributing pieces on storage nodes to the Storj software in the background.\nWhat\u0026rsquo;s more, developers can use the Satellite Admin Console web interface or the share command with one either our CLI or developer library/SDK to abstract all the complexity of granular access management via client-side delegated authorization.\n"},{"id":46,"href":"/dcs/api-reference/uplink-cli/access-command/access-list-command/","title":"access list","first":"/dcs/","section":"access","content":"access list #  Usage #  Windows ./uplink.exe access list [flags] Linux uplink access list [flags] macOS uplink access list [flags]  Flags #     Flag Description     --access string the serialized access, or name of the access to use   --help, -h help for list    Examples #  Windows ./uplink.exe access list Linux uplink access list macOS uplink access list  =========== ACCESSES LIST: name / satellite ================================ cheesecake / 12EayRS2V1kEsWESU9QMRseFhdxYxKicsiFmxrsLZHeLUtdps3S@us1.storj.io:7777 pumpkin-pie / 12L9ZFwhzVpuEKMUNUqkaTLGzwY9G24tbiigLiXpmZWKwmcNDDs@eu1.storj.io:7777 tarte / 121RTSDpyNZVcEU84Ticf2L1ntiuUimbWgfATz21tuvgk3vzoA6@ap1.storj.io:7777 "},{"id":47,"href":"/dcs/solution-architectures/common-architectural-patterns/","title":"Common Architectural Patterns","first":"/dcs/","section":"Solution Architectures","content":"Common Architectural Patterns #  There are a standard set of integration patterns in which the Storj Uplink is implemented. This section provides a solution architecture overview of the following integration patterns.\n   Platform/Service Description Decentralized Advantage     Cloud-hosted Gateway S3-compatible cloud hosted gateway providing elastic object storage capacity Easy implementation and broad compatibility. Note: uses server-side encryption   Hybrid Cloud On Premise Gateway On-premis to cloud elastic storage capacity Enhanced privacy via end-to-end encryption   Cloud Native Applications Web-based applications interact with S3-compatible cloud hosted gateway Server-side encryption and industry-leading access management controls with highly distributed network of storage nodes make it easy to build more secure and private applications   Mobile Apps Choose libuplink library for end-to-end encryption or S3-compatible cloud hosted gateway for ease of integration Take advantage edge-based delegated authorization for secure and private file sharing   Command Line File Transfer Command line tool for end-to-end encrypted large file transfer between people or environments Fast, easy, secure, private and economical way to move large files   Client App Integration Integrate libuplink into applications with native cloud storage use Easily integrate secure, private and economical cloud object storage inn your app (Examples FileZilla, Rclone and Restic)   Dapp Integration Add decentralized object storage to your decentralized app S3 compatibility, default multi-region high availability via a network of decentralized storage nodes, and enhanced security and privacy through delegated authorization.   Multi-cloud Storage Neutral, provider-agnostic cloud storage Low egress costs and distributed storage provide consistent performance for inter-cloud transit    "},{"id":48,"href":"/dcs/api-reference/uplink-cli/cp-command/","title":"cp","first":"/dcs/","section":"Uplink CLI","content":"cp #  Usage #  Windows ./uplink.exe cp [flags] SOURCE DESTINATION Linux uplink cp SOURCE DESTINATION [flags] macOS uplink cp SOURCE DESTINATION [flags]  The cp command is used to upload and download objects. The cp command abstracts the complexity of encryption, erasure coding, and distributing pieces of a file to storage nodes.\nFlags #     Flag Description     --access string the serialized access, or name of the access to use   --expires string optional expiration date of an object. Please use format (yyyy-mm-ddThh:mm:ssZhh:mm)\n   --help, -h help for cp   --metadata string optional metadata for the object. Please use a single level JSON object of string to string only   --parallelism int controls how many parallel uploads/downloads of a single object will be performed (default 1)   --progress if true, show progress (default true)    Examples #  Copy a local file into an existing bucket #  When the cp command is used to copy a file to Storj DCS (upload), the CLI first encrypts the file client-side, then splits it into a minimum of x erasure-coded pieces, and finally, the x pieces are uploaded in parallel to x different storage nodes. x currently equals 80 but is subject to change depending on continuous optimization.\nTo copy cheesecake.jpg into the existing bucket cakes, use the following command:\nWindows ./uplink.exe cp cheesecake.jpg sj://cakes Linux uplink cp cheesecake.jpg sj://cakes macOS uplink cp cheesecake.jpg sj://cakes  You cannot use regular expressions to specify which files to copy (e.g. uplink cp cheese* sj://cakes will not work). Also, you can only specify one source at a time (no uplink cp cheesecake.jpg cheesecake.png sj://cakes)  Output:\nCopy a file from a bucket to a local drive #  When the cp command is used to copy a file from Storj DCS (download), the CLI first downloads the minimum number of pieces to reconstitute a file (typically 29 pieces), then re-encodes the pieces into a single file, and finally decrypts the file client-side.\nTo copy a file from a project to a local drive, use:\nWindows ./uplink.exe cp sj://cakes/cheesecake.jpg ~/Downloads/ Linux uplink cp sj://cakes/cheesecake.jpg ~/Downloads/ macOS uplink cp sj://cakes/cheesecake.jpg ~/Downloads/  Copy a local file into a bucket with an expiration date #  The uploaded object can be set to expire at a certain time. After the expiration date, the file is no longer available and no longer will generate usage charges. To set an expiration date for a file when uploading it, you should use the cp command with the --expires flag:\nWindows ./uplink.exe cp cheesecake.jpg --expires 2021-12-31T13:00:00+02:00 sj://cakes Linux uplink cp cheesecake.jpg --expires 2021-12-31T13:00:00+02:00 sj://cakes macOS uplink cp cheesecake.jpg --expires 2021-12-31T13:00:00+02:00 sj://cakes  The date is given in the yyyy-mm-ddThh:mm:ssZhh:mm format defined in ISO 8601. 2021-12-31T13:00:00+02:00 reads \u0026ldquo;December, 31st at 1pm UTC+2\u0026rdquo;. A date ending with \u0026ldquo;Z\u0026rdquo;, such as 2021-12-31T13:00:00Z, is in UTC.\nThe command above gives the following output:\nCopy an object with parallelism #  If you have enough upstream bandwidth, you can use the multipart functionality to upload objects faster.\nTo increase upload speed, you can use the cp command with the --parallelism 10 flag (the number you can set according to your preferences and available upstream bandwidth):\nWindows ./uplink.exe cp cheesecake.jpg sj://cakes --parallelism 10 Linux uplink cp cheesecake.jpg sj://cakes --parallelism 10 macOS uplink cp cheesecake.jpg sj://cakes --parallelism 10  Since our sample object is small, you likely will not notice a difference.\nIt would be significantly different with big objects like videos or OS images etc. and for upstream bandwidth much greater than 100Mbps.\nCopy an object from one location to another in Storj DCS #  It is possible to copy a file from one Storj DCS location to another Storj DCS location within the same project.\nWhen the cp command is used to copy a file from one Storj DCS location to another Storj DCS location, the CLI will download the object from the previous location and upload it to a new location.\nThe download bandwidth will count against your egress limits. You can be charged for egress traffic according to your tariff plan.  To create a new bucket, we will use the mb command, as copying is possible only to an existing bucket.\nWindows ./uplink.exe mb sj://new-recipes Linux uplink mb sj://new-recipes macOS uplink mb sj://new-recipes  Bucket new-recipes created Nested buckets are not supported, but you can use prefixes, as they would act almost like subfolders.  To copy a file from a project to another bucket in the same project and with prefix cakes, use:\nWindows ./uplink.exe cp sj://cakes/cheesecake.jpg sj://new-recipes/cakes/cheesecake.jpg Linux uplink cp sj://cakes/cheesecake.jpg sj://new-recipes/cakes/cheesecake.jpg macOS uplink cp sj://cakes/cheesecake.jpg sj://new-recipes/cakes/cheesecake.jpg  Sample Output:\nCopy a file to a bucket with metadata #  You can include metadata when uploading your file using the \u0026ndash;metadata flag. These metadata are provided in JSON format.\nYou must use a single level JSON object of string to string only (e.g. \u0026lsquo;{\u0026ldquo;key1\u0026rdquo;:\u0026ldquo;value1\u0026rdquo;, \u0026ldquo;key2\u0026rdquo;: \u0026ldquo;value2\u0026rdquo;\u0026rsquo;}  For example, to include information about the baker of a cheesecake and the author of the photo:\nWindows ./uplink.exe cp cheesecake.jpg sj://cakes --metadata \u0026#39;{\\\u0026#34;baker\\\u0026#34;:\\\u0026#34;cheeseman\\\u0026#34;, \\\u0026#34;picture_author\\\u0026#34;: \\\u0026#34;picman\\\u0026#34;}\u0026#39; Linux uplink cp cheesecake.jpg sj://cakes --metadata \u0026#39;{\u0026#34;baker\u0026#34;:\u0026#34;cheeseman\u0026#34;, \u0026#34;picture_author\u0026#34;: \u0026#34;picman\u0026#34;}\u0026#39; macOS uplink cp cheesecake.jpg sj://cakes --metadata \u0026#39;{\u0026#34;baker\u0026#34;:\u0026#34;cheeseman\u0026#34;, \u0026#34;picture_author\u0026#34;: \u0026#34;picman\u0026#34;}\u0026#39;  You can retrieve these metadata using the meta get command.\n"},{"id":49,"href":"/dcs/getting-started/quickstart-uplink-cli/sharing-your-first-object/generate-access/","title":"Create an Access to an Object","first":"/dcs/","section":"Sharing Your First Object","content":"Create an Access to an Object #  There are two ways to share access to an object:\n by link sharing by importing the access using uplink CLI  In both cases, you can create an access using the uplink share command. For example:\nWindows ./uplink.exe share sj://cakes/cheesecake.jpg --export-to cheesecake.access Linux uplink share sj://cakes/cheesecake.jpg --export-to cheesecake.access macOS uplink share sj://cakes/cheesecake.jpg --export-to cheesecake.access  An access generated using uplink share with no arguments creates an access to your entire project with read permissions.  The --export-to flag is used to export the access to a file. This gives the following output:\n=========== ACCESS RESTRICTIONS ========================================================== Download : Allowed Upload : Disallowed Lists : Allowed Deletes : Disallowed NotBefore : No restriction NotAfter : No restriction Paths : sj://cakes/cheesecake.jpg =========== SERIALIZED ACCESS WITH THE ABOVE RESTRICTIONS TO SHARE WITH OTHERS =========== Access : 12yUGNqdsKX1Xky2qVoGwdpL... Exported to: cheesecake.access Restrictions #  The --readonlyflag prevents all write operations (delete and write). Similarly, the --writeonly flag prevents all read operations (read and list).\nBy default, the access is a read-only. To give full permissions, use--readonly=false\nYou may also indicate the duration of access with a start and end time.\nThe list of all restrictions can be found here.\nExample:\nWindows ./uplink.exe share --readonly=false --not-before=+2h --not-after=+10h sj://cakes/ Linux uplink share --readonly=false --not-before=+2h --not-after=+10h sj://cakes/ macOS uplink share --readonly=false --not-before=+2h --not-after=+10h sj://cakes/  =========== ACCESS RESTRICTIONS ========================================================== Download : Allowed Upload : Allowed Lists : Allowed Deletes : Allowed NotBefore : 2021-04-17 17:22:39 NotAfter : 2021-04-18 01:22:39 Paths : sj://cakes/ (entire bucket) =========== SERIALIZED ACCESS WITH THE ABOVE RESTRICTIONS TO SHARE WITH OTHERS =========== Access : 123qSBBgSUSqwUdbJ6n4bxLM... See the Uplink CLI share command reference for more actions  "},{"id":50,"href":"/node/setup/gui-windows/dashboard/","title":"Dashboard","first":"/node/","section":"GUI Install - Windows","content":"Dashboard #  Storage Node Operator Web Dashboard #  When you finish the Windows installation for your Storage Node, a Windows shortcut will automatically be created that will open the Storage Node Operator Dashboard in your web browser.\nYou can also access the dashboard by opening the following URL in your web browser:\nDirectly on your node: #  http://127.0.0.1:14002/ Device on local network: #  http://\u0026lt;your-nodes-local-ip\u0026gt;:14002/ Storage Node Dashboard Concepts #     Concept Description     Satellite Satellites act as the mediator between clients (people up- and downloading data) and Storage Node Operators (people storing data). Satellites facilitate the storage interaction and decide which storage nodes will store which pieces.   Bandwidth Used This Month The amount of total bandwidth you\u0026rsquo;ve provided to the network since the beginning of the current period.   Usage / Repair / Audit Usage bandwidth is the bandwidth a Storage Node uses so customers can download their data, for which a Storage Node Operator is paid $20/TB.\n\nRepair bandwidth is the bandwidth usage resulting from regenerating a bad Storage Node\u0026rsquo;s deleted data that is part of the repair process, for which a Storage Node Operator sending the data to new nodes is paid $10/TB.\n\nAudit bandwidth is the data downloaded from the Storage Node, which the Satellite uses to measure file durability and Node reputation.\n   Egress Ingress\n Egress is the data the customer downloads from the network.\nIngress is the data the network uploads to a Storage Node.\n   Disk Space Used This Month The amount of total disk space used on a storage node in the current monthly period, for which a storage node is paid $1.50/TB.\n   Disk Space Remaining The amount of disk space available to use by the network for the remaining month.   Uptime Checks Uptime checks occur to make sure a Storage Node is still online. This is the percentage of uptime checks a Storage Node has passed.   Audit Checks Audit checks occur to make sure the data sent to a Storage Node is still held on the node and intact. This is the audit score, represented as percentage. With score less than 60% node will be disqualified.    How to remote access the web dashboard "},{"id":51,"href":"/dcs/billing-payment-and-accounts-1/storj-token/debits-against-payment-methods/","title":"Debits Against Payment Methods","first":"/dcs/","section":"Payment Methods","content":"Debits Against Payment Methods #  If you have an amount due, your payment methods will be debited in the following order (the order of operations is entirely programmatic):\n Coupon - Any valid Coupon on an account will first be used to pay an invoice amount. If the entire invoice is paid with Coupons, no further debits are made in that billing cycle. If, after all Coupons are exhausted in a billing cycle, a balance on an invoice remains, the balance will be debited using the next available payment method; Credits - After applying any Coupons, any valid Credit on an account will then be used to pay an invoice amount. If the entire invoice is paid from the amount of valid Credits on an account, no further payment is needed. If, after all Credits are exhausted in a billing cycle, a balance on an invoice remains, the balance will be debited to the next available payment method; STORJ Token Balance - After applying any Coupons and Credits, any valid STORJ token balance on an account will then be used to pay an invoice amount. If the entire invoice is paid from the amount of STORJ token balance on an account, no further payment is needed. If, after all STORJ token balance is exhausted in a billing cycle, a balance on an invoice remains, the balance will be debited to the next available payment method; Credit Card - After applying any Coupons, Credits, or STORJ token available, any valid credit card on an account will then be used to pay an invoice amount. If the entire invoice is paid from a credit card on an account, no further payment is needed. If a credit card payment is partially or fully rejected, and a balance on an invoice remains, the user will be notified by email registered to the registered email address on the account of the unpaid balance that must be paid.  All unpaid balances must be paid via a valid payment method in a billing cycle. Note that, if there is no valid payment method on an account, and a new payment method is not added within a reasonable amount of time, we reserve the right to reduce account usage limits to zero and/or reclaim the available resources (the storage space and bandwidth made available by Storage Node Operators to the Storj network) and delete your data stored on the network pursuant to our data retention policy.\\\n"},{"id":52,"href":"/dcs/concepts/definitions/","title":"Definitions","first":"/dcs/","section":"Concepts","content":"Definitions #  The Storj DCS service uses an array of different technologies such as strong encryption and erasure codes to ensure a differentiated level of privacy and security across a network of peer classes. Some of the terms we use may be familiar and some may be new, but if you see a word you don\u0026rsquo;t recognize or see a familiar word in a different context, you\u0026rsquo;ll probably find a useful definition here:\n Peer Class A cohesive collection of network services and responsibilities. There are three different peer classes that represent services in our network: storage nodes, Uplinks, and Satellites. Storage Node This peer class stores data for others, and gets paid for storage and bandwidth. All data stored on storage nodes is client-side encrypted and erasure coded. Uplink This peer class represents any application or service that implements libuplink and stores and/or retrieves data. This peer class is not necessarily expected to remain online like the other two classes and is relatively lightweight. This peer class performs encryption, erasure encoding, and coordinates with the other peer classes on behalf of the customer/client. Object data stored by the Uplink client is encrypted by the Uplink client, including metadata.  Libuplink A library which provides all necessary functions to interact with storage nodes and Satellites directly. This library will be available in a number of different programming languages. Gateway A service which provides a compatibility layer between other object storage services such as Amazon S3 and libuplink exposing an Amazon S3-compatible API.  Gateway ST - a single-tenant, self-hosted S3-compatible gateway service provided as part of the Storj DCS developer tools/SDK Gateway MT - a multi-tenant, multi-region, cloud-hosted S3-compatible gateway service provided as part of the Storj DCS service. We consider this service server-side encrypted instead of end-to-end encrypted because this service temporarily manages encryption for you. While more complex, this can be run on your own infrastructure. Linksharing/webhosting - a gateway for standard HTTP requests, so you can share objects with users via a web browser or even host full websites. This can also be run on your own infrastructure if preferred.   Uplink CLI A command line interface for uploading and downloading files from the network, managing permissions and sharing, and managing accounts.   Satellite - A peer class and one of the primary components of the Storj network. The satellite participates in the node discovery system, caches node address information, stores per-object metadata, maintains storage node reputation, aggregates billing data, pays storage nodes, performs audits and repair, and manages authorization and user accounts. Users have accounts on and trust specific Satellites. Any user can run their own Satellite, but we expect many users to elect to avoid the operational complexity and create an account on another Satellite hosted by a trusted third party such as Storj Labs, a friend, group, or workplace. Storj Labs satellites are operated under the Storj DCS brand. This component has a couple of main responsibilities:  developer account registration \u0026amp; management, API credential \u0026amp; access management, billing \u0026amp; payment, audit \u0026amp; repair, garbage collection \u0026amp; other chores.   Satellite Developer Account - Basic information about users is stored and used to allow users to access Storj DCS. User account data includes user name, email, password, and payment methods. User account data is not client-side encrypted so that it may be rendered in the satellite user interface. Strong Encryption - a strong encryption algorithm is one that can guarantee the confidentiality of sensitive data. The time and cost of the compute resources required to decrypt data encrypted via a strong encryption method is either not possible, feasible, or economically justifiable, within the usable lifespan of the data. Weak Encryption - a weak encryption algorithm is one that cannot guarantee the confidentiality of sensitive data. Antiquated encryption algorithms such as DES (or even 3DES) no longer provide sufficient protection for use with sensitive data. Encryption algorithms rely on key size as one of the primary mechanisms to ensure cryptographic strength. Cryptographic strength is often measured by the time and computational power needed to generate a valid key. Advances in computing power and cryptanalytic techniques have made it possible to obtain small encryption keys in a reasonable amount of time. For example, the 56-bit key used in DES posed a significant computational hurdle in the 1970s when the algorithm was first developed, but today DES can be cracked in less than a day using commonly available equipment. Backdoor - A backdoor refers to any method by which authorized and unauthorized users are able to get around normal security measures and gain high level user access (aka root access) on a computer system, network or software application. Open-source - Open source software is software with source code that anyone can inspect, modify, and enhance. Privacy Policy - A privacy policy is a statement or a legal document that discloses some or all of the ways a party gathers, uses, discloses, and manages a customer or client\u0026rsquo;s data. Personal information can be anything that can be used to identify an individual, not limited to the person\u0026rsquo;s name, address, date of birth, marital status, contact information, ID issue, and expiry date, financial records, credit information, medical history, where one travels, and intentions to acquire goods and services. It is often a statement that declares a party\u0026rsquo;s policy on how it collects, stores, and releases personal information it collects. It informs the user what specific information is collected, and whether it is kept confidential, shared with partners, or sold to other firms or enterprises. Privacy policies typically represent a broader, more generalized treatment, as opposed to data use statements, which tend to be more detailed and specific. Data Element Definitions - Object data stored on the network is structured in a way that maps to constructs consistent with traditional object storage generally, with aspects that also include attributes adapted to the distributed storage model.  Project - A project is the basic unit for aggregating usage, calculating billing, invoicing fees, and collecting payment. Currently, user accounts are limited to a handful of Projects by default, but users can request and pay for more. Multiple users may be added to a project team. Within a Project, usage is tracked at the bucket level and aggregated for invoicing to the project. Project names are not client-side encrypted so that they may be rendered in the satellite user interface. Bucket - A bucket is an unbounded but named collection of files identified by paths. Every object has a unique object key (or path) within a bucket. Bucket names are not client-side encrypted so that they may be rendered in the satellite user interface. Object Key (or Path) - An object key is a unique identifier for an object (or file) within a bucket. An object key is an arbitrary string of bytes. Object keys contain forward slashes at access control boundaries. Forward slashes (referred to as the path separator) separate path components. An example path might be videos/carlsagan/gloriousdawn.mp4, where the path components are videos, carlsagan, and gloriousdawn.mp4. Object keys are client-side encrypted. Object or file - An object (or file) is the main data type in our system. An object is referred to by an object key, contains an arbitrary amount of bytes, and has no minimum or maximum size. An object is represented by an ordered collection of one or more segments. Segments have a fixed maximum size. An object also supports a limited amount of key/value user- defined fields in which to store user metadata. Like object keys, the object data is client-side encrypted. Segment - A segment represents a single array of bytes, between 0 and a system-configurable maximum segment size. The max Segment size on Storj DCS Satellites is 64MB. An object smaller than 64MB is stored as one segment. Objects larger than 64MB are stored in multiple 64MB Segments. Each Segment is stored as 80 pieces on the network. Only 29 Pieces of the 80 are required to reconstitute a Segment. All Segments are required to reconstitute an Object. Segment data is client-side encrypted.  Remote Segment - A remote segment is a segment that will be erasure encoded and distributed across the network. A remote segment is larger than the metadata required to keep track of its bookkeeping, which includes information such as the IDs of the nodes that the data is stored on. Remote segment data is client-side encrypted. Inline Segment - An inline segment is a segment that is small enough where the data it represents takes less space than the corresponding data a remote segment will need to keep track of which nodes had the data. In these cases, the data is stored “inline” instead of being stored on nodes. Inline segment data is client-side encrypted.   Stripe - A stripe is a further subdivision of a segment. A stripe is a fixed amount of bytes that is used as an encryption and erasure encoding boundary size. Erasure encoding happens on stripes individually, whereas encryption may happen on a small multiple of stripes at a time. All segments are encrypted, but only remote segments erasure encode stripes. A stripe is the unit on which audits are performed. See whitepaper section 4.8.3 for more details. Stripe data is client-side encrypted. Erasure Share - When a stripe is erasure encoded, it generates multiple pieces called erasure shares. Only a subset of the erasure shares is needed to recover the original stripe. Each erasure share has an index identifying which erasure share it is (e.g., the first, the second, etc.). Data is client-side encrypted before it is erasure coded into erasure shares. Piece - When a remote segment’s stripes are erasure encoded into erasure shares, the erasure shares for that remote segment with the same index are concatenated together, and that concatenated group of erasure shares is called a piece. If there are n erasure shares after erasure encoding a stripe, then there are n pieces after processing a remote segment. The _i_th piece is the concatenation of all of the _i_th erasure shares from that segment’s stripes. See whitepaper section 4.8.5 for more details. Pieces are client-side encrypted before they are erasure coded into erasure shares. Metadata - Metadata is data that is stored about the objects stored on the service. Metadata includes object key data, pointer data, the encrypted per-object randomized salted encryption key-related data, and user-defined metadata. Metadata is client-side encrypted. Pointer - A pointer is a subset of metadata that is data structure that either contains the inline segment data, or keeps track of which storage nodes the pieces of a remote segment were stored on, along with other per-file metadata. Pointer data is not client side encrypted, so that a Satellite can repair a segment and replace the location of pieces.   Erasure Code Definitions - Data redundancy on the network is achieved using erasure codes. Erasure coding is a means of data protection in which data is broken into pieces, where each piece is expanded and encoded with redundant data. The pieces are then stored across a set of different storage locations to reduce the risk of data loss due to the loss of any one data location.  Erasure Code Ratio - An erasure code is often described as a ratio of two numbers, k and n. If a block of data is encoded with a k, n erasure code, there are n total generated erasure shares, where only any k of them are required to recover the original block of data! It doesn’t matter if you recover all of the even numbered shares, all of the odd numbered shares, the first k shares, the last k shares, whatever. Any k shares can recover the original block. Expansion Factor - The amount of extra data stored to achieve redundancy based on achievement of a target durability. Expansion factor for a durability measurement achieved via erasure codes is calculated by dividing n by k. Expansion factor for a durability measurement achieved via replication is calculated by multiplying the number of replicas by 100%. At any given expansion factor, erasure codes have a significantly higher durability than replication. Erasure Code Variables  k** = 29** – This is the number of pieces required to recreate a Segment. Any 29 of the pieces of a Segment can be used to reconstitute a Segment l** = 39** – This is the number of pieces an Uplink will attempt to download when downloading a Segment. The Uplink will cut off any piece downloads after 29 pieces have been downloaded. The Uplink attempts to download more pieces than needed to eliminate the long tail effect of dependency on the slowest Storage Node. m** = 52** – This is the repair threshold for a Segment. Satellites track when Storage Nodes fail or leave the network making pieces unavailable. If too many storage nodes become unavailable, putting the potential durability of a Segment at risk, the Satellite will recreate the missing pieces via file repair and store the repaired pieces on diverse, health storage nodes. n** = 80** – The maximum number of pieces stored for a Segment o** = 130** – The number of pieces an Uplink attempts to upload to diverse Storage Nodes when uploading a Segment.     Multipart Upload Multipart Upload is a function that allows large files to be broken up into smaller pieces for more efficient uploads. When an object is uploaded using Multipart Upload, a file is first broken into parts, each part of a Multipart Upload is also stored as one or more Segments. With Multipart Upload, a single object is uploaded as a set of parts. Each part is an integral portion of the data comprising the object. The object parts may be uploaded independently, in parallel, and in any order. Uploads may be paused and resumed by uploading an initial set of parts, then resuming and uploading the remaining parts. If the upload of any part fails, that part may be re-uploaded without impacting the upload of other parts. All of these parts are broken into one or more Segments by the Storj DCS Gateway based on whether the Part Size is smaller or larger than the default Segment size. While Multipart Upload is most appropriate for files larger than the 64MB default Segment size, the Part Size is configurable in applications that use Multipart Upload.  Part - a single piece of an object that has been separated into multiple piece during a MultiPart Upload.   Graceful Exit A function by which a storage node can transition the data it stores to other nodes on the network, without triggering the repair process for the purpose of exiting the network. Value Attribution Through our partner program, we offer a variety of programs for partners who refer business to us. We track referrals using Value Attribution.  Value Attribution Code Partners have a code that is passed by and to a User Agent to track Storj DCS usage associated with a partner\u0026rsquo;s application.   Encryption-related Terms  Segment: The largest subdivision of an object or part. All the segments of an object or part are usually the same size. In most cases, the last segment will be smaller than the rest. Object key or path: The representation for a object\u0026rsquo;s \u0026ldquo;location.\u0026rdquo; Paths are essentially an arbitrary number of strings delimited by slashes (e.g. this/is/an/object.txt). On the Storj network, the Satellite uses object keys to keep track of object metadata as well as pointers to storage nodes that possess encrypted object content. Root Secret: The private client-side encryption key defined in the client configuration that used to derive keys for encrypting and decrypting data stored on the service. Object encryption key: A key derived from the root secret and the object key. There is a different path key for every path component in a forward-slash separated object key, and each path component is used to derive new path keys for lower level path items. Random Key: A randomly generated key used to encrypt segment content and metadata. Derived key: A key derived from the path key for the lowest level path element. The derived key is used to encrypt the random key before it is stored in a segment’s metadata. HMAC: Hash-based message authentication code. We generate HMACs using path elements and encryption keys in order to derive new keys for lower levels of the path. Using hashes makes it easy to generate lower level keys from higher levels without making it possible to generate higher level keys from lower level ones. AES-GCM: An authenticated encryption algorithm that makes use of the Advanced Encryption Standard and uses the Galois/Counter mode for encrypting blocks. Secretbox: An authenticated encryption algorithm from the NaCl library that combines the Salsa20 encryption cipher and Poly1305 message authentication code.   Access Management-related Terms  Access Grant: An encoded string that contains an API Key, an Encryption store, and the address of the Satellite storing the object metadata for the purpose of sharing access to objects stored on Storj DCS. API Key: A string generated for a project to authorize access management to data on the service. The API key is an authorization token based on an implementation called macaroons and is sent to the Satellite in order to authorize requests. Encryption store: a collection of encryption key information used to allow a user to access one or more objects, object key prefixes, or buckets. Restricted Access Grant: An access grant derived from another access grant with one or more restrictions applied to the internal API key, encryption store, or most commonly both. Caveat: an access restriction that is encoded into an API key that is generated client side and is interpreted by a satellite. Current supported restrictions are:  Specific Operations​: Caveats can restrict whether an API Key can perform any of the following operations: Read, Write, Delete, List. Bucket:​Caveats can restrict whether an API Key can perform operations on one or more Buckets. Path and Path Prefix:​ Caveats can restrict whether an API Key can perform operations on objects within a specific path in the object hierarchy. These caveats are also applied to the encryption store, when added, thus removing unnecessary decryption information from restricted access grants. Time Window​: Caveats can restrict when an API Key can perform operations on objects stored on the service, either before or after a specified date/time or between to dates/times.      "},{"id":53,"href":"/dcs/concepts/encryption-key/design-decision-end-to-end-encryption/","title":"Design Decision: End-to-end Encryption","first":"/dcs/","section":"Encryption","content":"Design Decision: End-to-end Encryption #  Strong encryption is critical to decentralized projects especially where a significant part of the infrastructure is run by independent third-party providers. Encryption is especially important with data storage to deliver security + privacy and ensure developers are in control of their data.\nEnd-to-end encryption means that only you have access to your data and the associated metadata. If end-to-end encryption is essential to your use case, Storj DCS provides the Uplink CLI, libuplink library, a variety of developer tools including FileZilla and Rclone, and the self-hosted GatewayST that allow you to ensure data and metadata are encrypted before they ever reach any service operated by Storj or any other third party.\nWhether you create an Access Grant in the Satellite Admin Console, or you use one of the uplink clients, you, and only you have access to your encryption key. Within the Storj DCS encryption ecosystem, all of the tools are interoperable and encryption is easily managed between tools.\n"},{"id":54,"href":"/dcs/getting-started/quickstart-uplink-cli/interacting-with-your-first-object/download-an-object/","title":"Download an Object","first":"/dcs/","section":"Interacting With Your First Object CLI","content":"Download an Object #  Download our object #  To download our cheesecake photo, let\u0026rsquo;s use the copy command:\nWindows ./uplink.exe cp sj://cakes/cheesecake.jpg ~/Downloads/cheesecake.jpg Linux uplink cp sj://cakes/cheesecake.jpg ~/Downloads/cheesecake.jpg macOS uplink cp sj://cakes/cheesecake.jpg ~/Downloads/cheesecake.jpg  Result\n"},{"id":55,"href":"/dcs/downloads/download-self-hosted-s3-compatible-gateway/","title":"Download Self-hosted S3 Compatible Gateway","first":"/dcs/","section":"Downloads","content":"Download Self-hosted S3 Compatible Gateway #  Creating Your Account Self-hosted S3 Compatible Gateway "},{"id":56,"href":"/dcs/concepts/access/encryption-and-keys/","title":"Encryption Keys","first":"/dcs/","section":"Access Management","content":"Encryption Keys #  Managing Encryption Keys #  One very important design consideration is that data stored on Storj DCS is encrypted. That means only you have the encryption keys for your data. The service doesn\u0026rsquo;t ever have access to or store your encryption keys. If you lose your keys (and lose the Access Grant containing your encryption passphrase), you will be unable to recover your data.\nEncryption \u0026amp; Access Grants #  Where an API Key within an Access Grant controls what resources and operations a Satellite will allow a user to access and perform, an HD Encryption Key controls what buckets, path prefixes, and objects a user has the ability to decrypt or encrypt.\nAt each level of the hierarchy of buckets, path prefixes, and objects, a child HD Encryption Key is derived in essentially the same manner as the serialized API Key. Both are hierarchically deterministic, but where the hierarchy of an API Key is based on the hierarchy of access restrictions, the hierarchy of encryption keys is based on the hierarchy of paths and objects.\nThe primary difference is that while a Satellite could generate restricted Access Grants that would be essentially the same artifact as a restricted Access Grant generated by an Uplink Client, a Satellite never has access to Encryption Keys.\nSupported Protocols #  Out-of-the-box, the Uplink Client supports the AES-256-GCM encryption standard.\nBeing open source, developers can remove the default encryption standards and replace them with a custom or preferred encryption scheme.\nIt doesn’t matter what encryption solution you use with your application, but it does matter that you encrypt your data. Remember that your data is erasure-coded and distributed across diverse storage nodes that are assumed to be untrusted as they are operated by complete strangers distributed all over the globe.\nIn addition to not wanting to expose your data to the risk of compromise by byzantine storage nodes, unencrypted data creates a potential insider threat from rogue satellite operators.\nDuring ordinary Satellite file repair operation, file segments are downloaded by Satellites, re-encoded, and redistributed across storage nodes. As long as all data is encrypted client-side, the repair function does not expose the privacy or security of the data.\nAllowing Decryption for Shared Access to Objects #  The first part of this documentation explains how Access Grants work when access to objects stored on the Storj DCS Platform is to be shared between applications. Encryption keys work in a very similar way.\nEncryption Management Encoded into the Access Grant #  Each Access Grant has an Encryption Passphrase that is configured when the Access Grant is initially created. This Encryption Passphrase is used to encrypt all objects and metadata stored on Storj DCS. When a child Restricted Access Grant is created, two things happen:\n A restricted API Key is created inside of the new child Restricted Access Grant with a caveat that includes the restrictions in the API Key A child encryption key (or set of appropriate descendant encryption keys) is derived from the appropriate encryption keys in the parent Access Grant.  If the child Restricted Access Grant includes an object key path component prefix based restriction, not only will the API Key be restricted to just objects with that path prefix, but the hierarchically deterministic derived encryption key store contained in that child Restricted Access Grant can only be used to decrypt or encrypt objects with that same path prefix. Just as the child API Key cannot access objects beyond the restrictions contained in it\u0026rsquo;s caveat, that child Encryption Key cannot be used to decrypt objects above the path restriction to which it is limited.\nBy creating a restricted Access Grant, whether through the Satellite Admin Console or using an Uplink client, creating the Access Grant automatically creates an API Key and Encryption Key with the appropriate scope of restriction.  At each level of the hierarchy of buckets, object key prefixes, and objects, a child HD Encryption Key is derived, and the hierarchy is encoded in the object and object metadata. Based on where an object falls in the hierarchy, if the parent encryption key is known, an encryption key can be derived for any level of the hierarchy that is valid from that point in the hierarchy and below to any child objects below it in the hierarchy.\nSimilar to the hierarchically derived structure of Access Grants, developers or applications don’t need to worry about the complexity of maintaining a set of keys. Since the hierarchy is encoded into the encryption mechanism, a shareable key can be derived on demand. The shareable HD Encryption Key set is described as an EncryptionAccess in the Uplink Client.  Important Design Point #  While the Access Grant is intended to be passed from an Uplink Client to a Satellite, the EncryptionAccess is designed to be passed peer to peer, but never to the Satellite. For efficiency and ease of use, the file sharing functions of the Uplink Client constructs a ‘security envelope’ that contains both the API Key and the EncryptionAccess, that is passed peer-to-peer. The application behavior is then for the receiving peer Client Uplink to separate the API Key and the EncryptionAccess, pass the API Key to the Satellite to obtain access to the object, and then, as the Object is downloaded, it is decrypted client-side using the HD Key. This behavior is automatic and all of that complexity is abstracted behind the cp and share commands.\n"},{"id":57,"href":"/dcs/support/faqs/","title":"FAQ","first":"/dcs/","section":"Support","content":"FAQ #  How do I navigate to the binary location? #  How to navigate to the Desktop folder? Execute the command:\ncd ~/Desktop Or use your own location where you extracted the binary. It could be Downloads folder:\ncd ~/Downloads If you followed Download Uplink CLI or Download Self-hosted S3 Compatible Gateway - you doesn\u0026rsquo;t need to change the current location. The shell will open the user\u0026rsquo;s home folder by default. Or you can navigate to there:\ncd ~   How do I upload a file? #  You can upload your first object by following our documentation here.\nHow do I create a URL to share an object? #  All you need to create a shareable URL is the linksharing base URL for your region, a public, read-only access key from the Gateway MT of the same region, and the path to your object or bucket. For a shortcut, Uplink CLI will generate the shareable URL for you.\nPlease, do not use the possibility to manually build the URL with your access grant described below!\nThe access grant contains your derived encryption key, it\u0026rsquo;s especially dangerous if you would use your root access grant with full access - it will give a full access to your project to everyone!\nUse the safe --url option instead!\n You can build a not safe link like so: \u0026lt;base url\u0026gt;/s/\u0026lt;access key\u0026gt;/\u0026lt;path sans sj://\u0026gt;\n   Satellite Region Base URL     Asia  https://link.ap1.storjshare.io   EU  https://link.eu1.storjshare.io   US  https://link.us1.storjshare.io    e.g. https://link.ap1.storjshare.io/s/\u0026lt;access key\u0026gt;/my/path\nPlease, think twice before using the described method above - it exposes your access grant. If you would like to use this method anyway, then make sure to limit the access as much as possible. Ideally - read-only with time duration and only to one or two objects/paths, not to the whole bucket!  Can I use Storj DCS for web hosting? #  The Storj DCS service allows you to host static websites along with multimedia streaming, large file distribution, and other web-delivered assets.\nSince your webpages and assets are simply objects stored on the network and there is no server/database, Storj DCS does not support the hosting of dynamic websites. However, you can store all of your unchanging assets on Storj DCS and reference them from your dynamic site that is hosted on an external compute service of your choice.\nThere are a few ways you can host your static site on Storj DCS. We recommend using the Uplink CLI but you may also use the single-tenant S3 Gateway to host your site.\nStatic websites serve files, including HTML, CSS, and Javascript files, exactly as they are stored on the server. All visitors will be served the same files.\nDynamic websites use server-side processing to generate the underlying code behind each page. They support Create, Read, Update, Delete operations against a database. Web views can be custom rendered to each user.\n What happens if nodes go offline? #  When your data is uploaded, each object is encrypted, then broken into 64 MB Segments, then each Segment is erasure coded, meaning it\u0026rsquo;s broken into 80 pieces, of which only 29 are required to reconstitute an object or segment. Each of those 80 pieces is then uploaded directly, peer-to-peer, to statistically uncorrelated storage nodes. The erasure coded redundancy means that 51 of those nodes (operated by different people, in different locations, with different power supplies and internet connections. If too many nodes fail or leave the network, the network can repair the missing pieces.\nYou can learn more under Concepts for File Redundancy and File Repair.\nHow are encryption keys managed? #  Storj DCS is a secure and private object storage service. While there are several different ways to interact with the service, including an S3 compatible gateway, CLI, developer library and tools like FileZilla, Rclone, Restic, Duplicati and more, you are responsible for keeping your encryption keys safe.\nYou can learn more under Concepts for Encryption and Access Management.\nWhen do you create an Access Grant in Satellite UI and when do you use the CLI? #  You can generate an Access Grant in the Satellite Admin Console, or you can use either our Go Library or the CLI. In general, you use the Satellite Admin Console web interface to create an Access Grant that you can then use to set up whatever client tool you are using. The CLI, client library or other client tool can then use that Access Grant to interact with the Storj DCS service, or create restricted Access Grants - child Access Grants of the parent created in the Satellite Admin Console.\nIf you want to learn more, check out the Key Architecture Constructs section or read all about Access Management\nHow can I delete an Access Grant? #  Access Grants created using the Satellite user interface my be deleted using the Remove button on the Access page. Check the box next to the Access Grant(s) you want to delete, then click the Remove Selected button and follow the prompts.\nImportant: If you delete an Access Grant from the Satellite user interface, that Access Grant will immediately cease to function, and all hierarchically derived child Access Grants will also cease to function. Any data uploaded with that Access Grant will persist on Storj DCS. If you didn\u0026rsquo;t back up the encryption passphrase used with the Access Grant you are deleting, you will not be able to decrypt that data without that encryption passphrase, and it will be effectively unrecoverable.  If you created a child Access Grant client-side, using the CLI, the client Go library, or any other client-side tool or implementation, you can\u0026rsquo;t \u0026ldquo;delete\u0026rdquo; the access because, by design and for enhanced privacy and security, the Satellite is not aware of Access Grants created in a client. When presented with any Access Grant, the Satellite can only verify whether the Access Grant is valid for the resource being requested. For this reason, Access Grants that have been created client-side cannot be deleted, but must be revoked instead.\nYou can learn more under Concepts for Access Grants.\nHow do I recover from having lost my encryption key associated with an access grant? #  Your encryption keys effectively are your data. If you\u0026rsquo;ve lost the encryption key associated with an Access Grant, but you still have the Access Grant, DO NOT DELETE OR REVOKE that Access Grant. An Access Grant will continue to work until revoked or deleted. An Access Grant contains a serialized API key, encryption key, and the Satellite that holds the metadata for an object, but what is serialized in the access grant is derived from the passphrase - the passphrase is not stored in the access grant directly. Of course, that encryption passphrase is not stored by any Storj DCS service.\nThe safest approach would be to download your data with the working Access Grant, then create a new Access Grant with a new encryption passphrase and re-upload the data. Be sure to save that encryption passphrase in a secure location! As long as you have the encryption passphrase, you can generate new Access Grants that will work with pre-existing data.\nIf you\u0026rsquo;ve lost the Access Grant and you don\u0026rsquo;t have a backup of the encryption passphrase, you will not be able to decrypt your data and it is effectively lost.\nHow can I revoke an Access Grant I shared with someone? #  Access Grants can be created either in a browser or with the CLI or library, they can be further restricted, client-side creating additional hierarchically derived Access grants. Since these restricted Access Grants are managed client-side through delegated authorization, no server has any registry that these Access Grants even exist. While this gives developers a powerful tool kit to create more private and secure applications, shared access also needs to be revoked. The Storj DCS service has an API for revoking Access Grants via a revocation list.\nYou can learn more under Concepts for Access Revocation.\nWhat kind of restrictions can I put on an Access Grant? #  You can generate a restricted Access Grant from the Satellite user interface, using the CLI, or using the client Go Library. While the possibilities for access controls that can be encoded in a caveat are virtually unlimited, the specific caveats supported on Storj DCS are as follows:\n Specific operations: Caveats can restrict whether an API Key can perform any of the following operations: Read, Write, Delete, List Bucket: Caveats can restrict whether an API Key can perform operations on one or more Buckets Path and path prefix: Caveats can restrict whether an API Key can perform operations on Objects within a specific path in the object hierarchy Time window: Caveats can restrict when an API Key can perform operations on objects stored on the service  For some sample Go code around access-restriction, check out: https://godoc.org/storj.io/storj/lib/uplink#example-package\u0026ndash;RestrictAccess\nHow do I pay with Storj Token? #  When you decide to become a paid customer of Storj DCS, you can choose to pay with a credit card or using STORJ token. The process for adding a payment method is covered in our Billing Documentation.\nWhat are the current rate and usage limits? #  The default usage limits for a new account are published on the Usage Limits section under Concepts.\nHow do I increase my usage limits? #  The default usage limits may not be suitable for all projects. Usage limits may be increased for paid tier accounts. A valid credit card or a sufficient balance of STORJ token relative to the usage limit increase requested as the payment method must be added before a usage limit request form may be submitted. Please note that you will be required to verify email address on account by making a help desk account before requesting a limit increase.\nFor more information on rate limits view the Limits section under Concepts.\nHow do I get support? #  Our support process is described under the Support Process Overview section of this documentation. Our Support SLA is covered under our Terms of Service.\nHow am I billed for usage? #  For detailed information on how billing and payment work on Storj DCS, please see the Billing \u0026amp; Payment section of this documentation.\nHow can I remove my credit card from my account? #  For detailed information on how to remove a credit card from the Storj DCS service, please see Deleting a Payment Method under the Billing \u0026amp; Payment section of this documentation. Please note that a valid payment method must be maintained on a paid tier account. You may be required to submit a support request as part of the payment method removal process.\nHow can I delete a bucket? #  Buckets can be created and deleted using the S3-compatible gateway, CLI, or Go library. For detailed information on how deleting a bucket works on Storj DCS, please see the appropriate section of this documentation:\n Delete a bucket from the Satellite user interface  Delete a bucket from the command line Delete a bucket using the Go library Delete a bucket using the S3-compatible gateway  How do I delete all my data? #  The easiest way to delete your data is to use the CLI. For detailed information on how to use the command for removing buckets on Storj DCS, please see the section of this documentation on how to delete a bucket from the command line.\nHow do I delete my account? #  We want all of our users to receive value when they choose the Storj DCS service for their storage needs, but it’s possible that a user may no longer need Storj DCS services. If a user wants to stop using their account and permanently delete it, the user may do so only after following the steps outlined in the Billing Documentation to eliminate service usage.\nThe process to eliminate service usage starts with deleting all data from the service, including all objects and buckets. Next, all Access Grants should be deleted. Once this is done, the user should submit a support ticket to remove all payment methods and delete the account.\nFor detailed information on how to close your account on Storj DCS, please see the Closing an Account section of this documentation.\nDoes Storj DCS provide tools for end-user identity management for applications that store data on the service? #  The Storj DCS service is not designed to handle identity management for end users of applications that store data on the service. User authentication is expected to be handled by applications. Application developers may then make further design decisions related to use the authorization management functions of the service to enable secure and private sharing of data between users of an application or sharing data with a publicly available URL.\n"},{"id":58,"href":"/dcs/billing-payment-and-accounts-1/pricing/free-tier/","title":"Free Plan","first":"/dcs/","section":"Billing, Payment and Accounts","content":"Free Plan #  Storj DCS currently offers a free level of service to new accounts. When a customer creates a new account, the user automatically is able to use the Storj DCS service at no cost up to specified limits of service. The free level of service is limited to the level of service for a single project as described below:\n 150GB per month of static object storage for a single project. Static object storage is calculated in “terabyte months,” which is a prorated calculation of how much storage is used throughout the month, broken down by hour. Storing 150GB of data for 30 days, 300GB of data for 15 days, or 450GB of data for 10 days, would each be the equivalent to 150GB per month of free storage. 150GB of download bandwidth per month for a single project.  Usage for Static Object Storage and Download Bandwidth is calculated in the same way for the Free Plan as for the Pro Plan. A credit against billing will be applied each month for the Free Plan usage for as long as the Free Plan is offered. Accounts using the Free Plan of service are subject to Project Usage Limits.\\\n"},{"id":59,"href":"/node/setup/gui-windows/","title":"GUI Install - Windows","first":"/node/","section":"Setup","content":"GUI Install - Windows #  Before a GUI Install, you must: #  Receive an Authorization Token #  Authorization Token Generate an Identity #  Identity Set Up Port Forwarding #  Port Forwarding Failure to complete these steps will prevent your storage node from working.  "},{"id":60,"href":"/dcs/how-tos/host-a-static-website/","title":"Host a Static Website","first":"/dcs/","section":"How To's","content":"Host a Static Website #  The Storj DCS service allows you to host static websites along with other web-delivered assets such as streaming multimedia and large file distribution.\nSince your webpages and assets are simply objects stored on the network and there is no server/database, Storj DCS does not support the hosting of dynamic websites. However, you can store all of your unchanging assets on Storj DCS and reference them from your dynamic site that is hosted on an external compute service of your choice.\nThere are a few ways you can host your static site on Storj DCS. We recommend using the Uplink CLI and Linksharing Service but you may also use the single-tenant S3 gateway to host your site.\nStatic websites serve files, including HTML, CSS, and Javascript files, exactly as they are stored on the server. All visitors will be served the same files.\nDynamic websites use server-side processing to generate the underlying code behind each page. They support Create, Read, Update, Delete operations against a database. Web views can be custom rendered to each user.\n "},{"id":61,"href":"/dcs/getting-started/satellite-developer-account/manage-projects/","title":"Manage Projects","first":"/dcs/","section":"Quickstart - Satellite Admin Console","content":"Manage Projects #  When you log into the Satellite Admin Console, you start on the Project Dashboard for your default Project. A Project is the basic unit for aggregating usage, calculating billing, invoicing fees, collecting payment, and handling access management. Users can create multiple Projects and projects are invoiced separately. Within a Project, usage is tracked at the Bucket level and aggregated for invoicing to the Project. Project names are not client-side encrypted so that they may be rendered in the Satellite user interface. There are two main drivers for creating multiple Projects: access management and billing.\nLearn more about Projects in Key Architecture Constructs under Concepts.  To select, create or Manage Projects you can click the name of your project on the left side toolbar above Dashboard.\nIn the Manage Project screen you can create or rename or modify your limits (if you use a Pro account).\nCreate a new Project #  On Projects screen to create a new Project select the Create Project. On Project Dashboard you can click the name of the current project and select Create Project.\nThe availability of this function depends on your account tier. Please check Usage Limits for details.  Specify the Project Name, optional Description and confirm the creating with the Create Project button.\nModify the existing Project #  To modify the existing Project on the Projects screen you can select a needed project and modify its name or description.\nChanging Project Limits #  If your account tier allows you to change your Limits, you will have more options than a Free tier.\nSelect Edit to the right of the limit to change it. However, it will not allow to increase limits greater than your available maximum. To change the maximum you need to file a support request to change your limits.\nDelete the existing Project #  At the moment the Satellite Admin Console will not allow you to delete a Project.\nBut you can delete all buckets and Access Grants from it and rename it to something like \u0026ldquo;not used\u0026rdquo;. The empty project costs nothing.  If you believe that you need to remove it anyway, then please remove all data and Access Grants from it before file a support request.\nWe do not have an access to your data and Access Grants, because they are encrypted, and cannot remove your data on your behalf. So, please, remove them yourself before file a support request. We will ask you to do so anyway.  "},{"id":62,"href":"/node/resources/faq/migrate-my-node/migrating-from-windows-gui-installation-to-a-docker-cli/","title":"Migrating from Windows GUI installation to Docker CLI","first":"/node/","section":"How do I migrate my node to a new device?","content":"Migrating from Windows GUI installation to Docker CLI #  Preparing the destination system #  1. Check system requirements\n2. Install Docker\nOn Windows we recommend to install version 2.1.0.5. We do not recommend using later versions as a lot of bugs have been reported.  3. The difference between the Windows GUI and the Docker CLI is where each system stores the data. The Windows GUI version stores data in the path specified in the storage.path parameter of the configuration file \u0026quot;%ProgramFiles%\\Storj\\Storage Node\\config.yaml\u0026quot;, while the Docker version stores data in the subfolder called storage which is automatically (silently) added to the specified path through the --mount option.\nThe same applies for the orders folder. In the Windows GUI version it\u0026rsquo;s stored in the installation location (\u0026quot;%ProgramFiles%\\Storj\\Storage Node\\orders\u0026quot; by default), the docker version stores orders alongside with data in the data location.\nFor example, if the storage folder specified in the Windows GUI is D:\\STORJ, then for the Docker version you should move the content of the D:\\STORJ folder to the subfolder storage, i.e. D:\\STORJ\\storage, the orders should be moved from the \u0026quot;%ProgramFiles%\\Storj\\Storage Node\\orders\u0026quot; to the D:\\STORJ\\orders.\nThe same applies for Linux/MacOS systems. How do we accomplish that?\nWe will use the D:\\STORJ path of the source Windows system as an example. The destination path depends on OS:  Windows In Windows, we will use the same folderD:\\STORJ and PowerShell as a terminal.\nWe will assume that your Windows user is called user and it has full access to the D:\\STORJ folder.\nIf you are moving the identity and data to the new Windows CLI host, you need to share the destination folder on that host and follow the guide How to migrate the Windows GUI node from one physical location to another?\n Linux In Linux, we will use the /mnt/storj/storagenodefolder, where/mnt/storj is the path to the statically mounted disk.\nWe will assume that your Linux host has IP address 192.168.1.68, the Linux user is called user and it has full access to the /mnt/storj/storagenode folder.\nPlease replace /mnt/storj with your actual path. Open a terminal on your Linux system and execute:\nmkdir -p /mnt/storj/storagenode/storage   To be able to copy the identity and data from Windows to Linux, we need to have a SSH server enabled on your Linux system. You can google how to do this for your specific Linux distro. For example, for a Debian-based OS it can be done as follows:\nsudo apt update \u0026amp;\u0026amp; sudo apt install ssh -y macOS In MacOS, we will use the/Volumes/Storj/storagenodefolder, where /Volumes/Storj is the path where the disk is mounted.\nWe will assume that your MacOS host has IP address 192.168.1.69, the MacOS user is called macuser and it has full access to the /Volumes/Storj/storagenode folder.\nPlease replace /Volumes/Storj with your actual path. Open a terminal window (you can find it with the global search function in your MacOS) and execute:\nmkdir -p /Volumes/Storj/storagenode/storage   To be able to copy files from the Windows OS, we will use a SSH session to your MacOS. To accomplish this, you need to configure the remote access to your MacOS:\nHow to Enable SSH on a Mac from the Command Line\n 4. Setup port forwarding to your new system.\nPreparing the source Windows system #  The configuration steps are different depending on the destination OS.\nWindows If your source and destination OSes are both Windows, you can use the integrated robocopy command-line utility to copy your files across the network or local system: How to migrate the Windows GUI node from one physical location to another?\nIf your source and destination is the same Windows, stop and disable storagenode service to avoid disqualification. Execute in the elevated PowerShell:\nStop-Service storagenode Set-Service storagenode -StartupType Disabled And rename folders to use with docker. Please replace D:\\STORJ with your actual path (PowerShell):\nmv D:\\STORJ D:\\storage mkdir D:\\STORJ mv D:\\storage D:\\STORJ\\storage   Linux Since your source OS is Windows, and the destination OS is Linux, you need to have some Linux-compatible utilities to migrate the identity and data.\nWe will use WSL and Ubuntu package:\nInstall WSL\nmacOS Since your source OS is Windows, and the destination OS is MacOS, you need to have some MacOS-compatible utilities to migrate the identity and data.\nWe will use WSL and Ubuntu package:\nInstall WSL\n Copy identity, orders and data from the Windows GUI storagenode #  You can use this guide to migrate the identity, orders and data to a different device: How do I migrate my node to a new device?\nWindows We will assume that your identity is placed in the default location, i.e. %APPDATA%\\Storj\\Identity\\storagenodeand you used the default setup location, i.e. orders are located there: \u0026quot;C:\\Program Files\\Storj\\Storage Node\\orders\u0026quot;  robocopy /MIR /MOVE $env:AppData\\Storj\\Identity\\storagenode D:\\STORJ\\identity The storage data has already been migrated in the previous Preparing the source Windows system step.\nNow move orders to the data location:\nrobocopy /MIR /MOVE \u0026#34;$env:ProgramFiles\\Storj\\Storage Node\\orders\u0026#34; D:\\STORJ\\orders Linux We will assume that your identity is placed in the default location, i.e. %APPDATA%\\Storj\\Identity\\storagenodeand you used the default setup location, i.e. orders are located there: \u0026quot;C:\\Program Files\\Storj\\Storage Node\\orders\u0026quot;  Run an Ubuntu shell, which you have installed in the Preparing the source Windows system step above, and run the command:\nrsync -aP /mnt/d/Users/user/AppData/Roaming/Storj/Identity/storagenode/ user@192.168.1.68:/mnt/storj/storagenode/identity/ Since you are migrating to Linux, you can still continue running the source Windows GUI node, and while the migration is happening, run these commands in the Ubuntu shell to migrate the orders and data:\nrsync -aP \u0026#34;/mnt/c/Program Files/Storj/Storage Node/orders\u0026#34; user@192.168.1.68:/mnt/storj/storagenode/orders/ rsync -aP /mnt/d/STORJ/ user@192.168.1.68:/mnt/storj/storagenode/storage/ When each command completes, run them a few more times until the difference is negligible, then stop the Windows GUI storagenode service from the elevated PowerShell:\nStop-Service storagenode Set-Service storagenode -StartupType Disabled Now return to the Ubuntu shell and execute rsync one last time:\nrsync -aP --delete /mnt/d/STORJ/ user@192.168.1.68:/mnt/storj/storagenode/storage/ and for orders:\nrsync -aP --delete \u0026#34;/mnt/c/Program Files/Storj/Storage Node/orders\u0026#34; user@192.168.1.68:/mnt/storj/storagenode/orders/ macOS We will assume that your identity is placed in the default location, i.e. %APPDATA%\\Storj\\Identity\\storagenodeand you used the default setup location, i.e. orders are located there: \u0026quot;C:\\Program Files\\Storj\\Storage Node\\orders\u0026quot;  Run an Ubuntu shell, which you have installed in the Preparing the source Windows system step above, and run the command:\nrsync -aP /mnt/d/Users/user/AppData/Roaming/Storj/Identity/storagenode/ macuser@192.168.1.69:/Volumes/Storj/storagenode/identity/ Since you are migrating to MacOS, you can still continue running the source Windows GUI node, and while the migration is happening, run these commands in the Ubuntu shell to migrate the orders and data:\nrsync -aP \u0026#34;/mnt/c/Program Files/Storj/Storage Node/orders\u0026#34; macuser@192.168.1.69:/Volumes/Storj/storagenode/orders/ rsync -aP /mnt/d/STORJ/ macuser@192.168.1.69:/Volumes/Storj/storagenode/storage/ When each command completes, run them a few more times until the difference is negligible, then stop the Windows GUI storagenode service from the elevated PowerShell:\nStop-Service storagenode Set-Service storagenode -StartupType Disabled Now return to the Ubuntu shell and execute rsync one last time:\nrsync -aP --delete /mnt/d/STORJ/ macuser@192.168.1.69:/Volumes/Storj/storagenode/storage/ and for orders:\nrsync -aP --delete \u0026#34;/mnt/c/Program Files/Storj/Storage Node/orders\u0026#34; macuser@192.168.1.69:/Volumes/Storj/storagenode/orders/  When the data migration is completed, you should remove the storagenode Windows GUI version from the source Windows.\nIf you did not remove the Windows GUI storagenode instance, it could be automatically started by storagenode-updater service resulting in two copies of the same node in the network. It will be disqualified within a hour because it will not have all pieces since after the migration.  Running storagenode in Docker #  Now you can run the storagenode container following this guide: Storage Node.\n"},{"id":63,"href":"/dcs/billing-payment-and-accounts-1/storj-token/","title":"Payment Methods","first":"/dcs/","section":"Billing, Payment \u0026 Accounts","content":"Payment Methods #  The Storj DCS supports two payment methods—credit card and STORJ token, our ERC20 compatible utility token that leverages the Ethereum blockchain.\nIt is important to note that “Smart Contract\u0026quot; wallets such as Argent Wallet, Authereum, and Gnosis are not compatible with depositing STORJ tokens via CoinPayments. Please only send your STORJ tokens from a wallet that does not use a smart contract by default to send the tokens.  How To Add A Payment Method #  Once you have created a user account and a project, registered users are required to add a payment method before storing data on Storj DCS. Payment methods are added from the Billing section of the user interface.\nBegin by selecting \u0026ldquo;Billing\u0026rdquo; from the Settings dropdown menu at the top of your dashboard.\nUsing a Credit Card #  To use a credit or debit card, under Payment Method, select Add Card.\nYou’ll be prompted to add your card information. Using a credit card is somewhat self-explanatory, but there are some key points users should understand:\n When paying for their cloud storage bill with a credit card, users will be charged every month at the end of the billing period. The billing period is a period of time starting at 00:00:00 UTC on the first day of the month and finishing at 23:59:59 UTC on the last day of the month. This means that, for example, users in the United States, whose application incorporating Uplink interacts with a Satellite to upload files at 10:00 PM EST on the last day of the month, will be charged for their usage in the following billing period. Users can add multiple credit cards, but only one can be the default method of payment. A default credit card can only be deleted once all usage has stopped on the account, all data is removed, and any outstanding balance is paid. To remove a credit card from an account, any outstanding balance must be paid prior to contacting our support team to request that the card be removed. Up to one full billing cycle may be required prior to the removal of all payment methods. Once all payment methods are removed from an account, usage limits will be reset to zero until a new payment method is added. If a credit card expires, a user will receive a notification to the registered email on the account, however, if a new payment method is not added within a reasonable amount of time, we reserve the right to reclaim the available storage and bandwidth, reset usage limits to zero and delete any data stored on the account pursuant to our data retention policy.  Using STORJ Token #  In addition to credit cards, users may also pay for usage fees with STORJ token. Storj created the STORJ utility token as a medium of exchange on its decentralized cloud storage network. The STORJ utility token facilitates payments from people around the world for their use of the Storj DCS network to store their data, and Storj uses it to pay our community of Storage Node Operators that rent their unused hard drive capacity and bandwidth to the network.\nMaking payments with STORJ token is a different process than using a credit card. When using STORJ as your payment method, you commit to using a pre-payment model, which means you must first deposit X amount of STORJ tokens, which will be automatically converted to Y amount of $USD based on the spot price of the token at the time of the payment transaction.\nPlease note that it is not necessary to open an account with CoinPayments in order to make a STORJ token deposit to your Storj DCS account. You can use the workflow below to receive the STORJ deposit address through CoinPayments, and then directly send the requested amount of STORJ token from your own wallet to that deposit address.\nAlso note that if a user wants to pay solely using STORJ token, they must add a minimum of $25.00 USD worth of STORJ tokens to their Storj DCS account in a single transaction to activate the account and allow usage of the Storj DCS Platform services.\nImportant: If a user does not want to add a credit card but wants to pay only via STORJ token, they should exclusively deposit STORJ token and should not add a credit card to their account because, once a credit card is added as a payment method, it can only be removed by closing the account or requesting removal of the credit card after adding STORJ token, using the support process defined in this documentation.\nDepending on the amount of platform usage and the usage limits configured on your account, you may be required to keep a minimum deposit of STORJ token on your account to ensure monthly usage fees are paid in full each billing cycle.\nWe realize the STORJ token price can fluctuate, so our billing system is designed to ensure any such fluctuation does not affect the amount deposited in terms of USD. The value attributed to your account will be based on the STORJ to USD exchange rate at the time of deposit, not at the time of usage or bill payment. In other words, the account balance will be incremented by the USD equivalent of any STORJ deposit as soon as the deposit gets registered from CoinPayments. This is intended to minimize the potential impact of fluctuations in the STORJ token price because the price is fixed in USD on an account until the deposit balance is exhausted.\nTo deposit STORJ tokens in an account, navigate to the Payment Methods section and click on the Add STORJ button. Next, select a prefilled USD amount to be deposited or enter any custom amount that is at least $25 and then select Continue to Coin Payments.\nMake sure that your browser is not blocking popups, as CoinPayments will open a popup window with the payment instructions, including the STORJ token deposit address and QR code.  Users are then redirected to the CoinPayments checkout page where they can find the STORJ deposit address they should send the tokens to, as well as a QR code with the address to easily scan it using a mobile wallet. On this checkout page, users will see the amount in USD that they previously selected. They will also see the corresponding number of STORJ token to deposit into the CoinPayments deposit address, which equals the deposit amount divided by the current exchange rate. Users can then deposit the indicated amount of STORJ to the address provided.\nIf a user adds less STORJ than the amount indicated by CoinPayments, the details on the checkout page remain valid, and users still have an opportunity to send the remaining balance for up to two hours. Users can deposit STORJ to the checkout page in multiple transactions from different wallets. If they accidentally deposit more STORJ than is required on the checkout page, then they should apply for a refund directly through CoinPayments support ( https://coinpay.freshdesk.com/support/home). Storj only deposits the amount originally specified to a customer\u0026rsquo;s Storj DCS accounts.\nThe commitment made on the checkout page expires two hours after the transaction is initiated. If the deposit is not completed within the two-hour time frame, the transaction will be canceled. If you do not deposit the full amount within the two-hour time frame, and there’s no update to your Storj DCS account balance, then you should contact CoinPayments support ( https://coinpay.freshdesk.com/support/home).\nIf the full amount is deposited and the transaction is confirmed, the account should update within a few hours. Within 2 hours of CoinPayments having confirmed the transaction (checkout status having changed from \u0026ldquo;pending\u0026rdquo; to \u0026ldquo;paid\u0026rdquo;, the funds will be reflected in the Storj DCS account\u0026rsquo;s Balance History in the Billing Section. The checkout status should change from “pending” to “paid.\u0026quot;\nIf a user closes the checkout page inadvertently following the checkout process, the information can be recovered. Users have access to the transaction details via a link provided in the billing history, which stores a list of all transactions including status, details, and the link to the checkout page.\nImportant: The deposit address displayed in the CoinPayments deposit wizard interface is a deposit-only address to prepay for usage on the Storj DCS Platform. Users are not able to remove or otherwise withdraw tokens from this address. Any request for a refund or return of an unused prepaid deposit must follow the support process described in this documentation.\nIf the STORJ token balance runs out, a user will receive a notification to the registered email on the account. However, if a new payment is not added within a reasonable amount of time, Storj reserves the right to reduce account usage limits to zero and/or reclaim the available storage and bandwidth resources and delete your data stored on the network pursuant to our data retention policy.\n\\\n"},{"id":64,"href":"/node/before-you-begin/prerequisites/","title":"Prerequisites","first":"/node/","section":"Before You Begin","content":"Prerequisites #  Hardware Requirements (Recommended) #  ✅ One (1) processor core dedicated to each node service\n✅ 8 TB and a maximum of 24 TB of available space per node\nMinimum of 550 GB with no maximum of available space per node\n✅ 16+ TB of unmetered bandwidth available per month; unlimited preferred\nMinimum of 2 TB of bandwidth available per month\n✅ 100 Mbps bandwidth upstream\nMinimum of 5 Mbps bandwidth upstream\n✅ 100 Mbps bandwidth downstream\nMinimum of 25 Mbps bandwidth downstream\n✅ Uptime (online and operational) of 99.5% per month\nMinimum uptime (online and operational) of 99.3% per month, max total downtime of 5 hours monthly\nSystem Requirements #  Windows Windows 8, Windows Server 2012 or later.\nIf you are currently running a storage node on Windows using the Docker desktop, it will require good monitoring. If you are still running a node with Docker, your node may go offline randomly and require restarting your node, so it is recommended you switch to the Windows GUI. Learn more about Docker issues.\nLinux (Preferred) CentOS - A maintained version of CentOS 7\nDebian - 64-bit version of one of these Debian or Raspbian versions:\n Buster 10 Stretch 9 (stable) / Raspbian Stretch  Fedora - 64-bit version of one of these Fedora versions:\n 28 29  Ubuntu - 64-bit version of one of these Ubuntu versions:\n Cosmic 18.10 Bionic 18.04 (LTS) Xenial 16.04 (LTS)  Make sure you use static mount for your hard drive via /etc/fstab  macOS macOS Sierra 10.12 and newer macOS releases are supported\nMac hardware must be a 2010 or newer model\nVirtualBox prior to version 4.3.30 cannot be installed. If you have a newer version of VirtualBox installed, it’s fine.\nRunning a node on macOS will require good monitoring. Due to issues with Docker, your node may go offline randomly and require restarting your node. Learn more.\n Internet Connection #  It is highly recommended to have your Storage Node connected via LAN instead of WiFi to ensure a consistent and stable connection.\nPower Supply #  If you live in a location where power outages or brownouts are a frequent occurrence, please consider protecting your hardware, including the equipment you run your node on, as well as your router/modem, with an Uninterrupted Power Supply (UPS). This would help protect against damage to your hardware and against the corruption of your database resulting from abrupt shutdowns, which could lead to the unrecoverable loss of your node.\nKeep your node updated #  Where can I check for a new version? Using a Remote Connection #  What if I\u0026#39;m using a remote connection? "},{"id":65,"href":"/node/dependencies/quic-requirements/","title":"QUIC requirements","first":"/node/","section":"Dependencies","content":"QUIC requirements #  What is QUIC? #  See https://en.wikipedia.org/wiki/QUIC\nThis is a protocol based on UDP which promises more efficient usage of the internet connection with parallel downloads and uploads. This is exactly how our software operates.\nHow to configure Quic? #  You need to port forward not only TCP, but also UDP. You also need to allow the node\u0026rsquo;s port for the UDP protocol in your firewall, see Port Forwarding.\n\nIn case of Docker installation, you should use tcp and udp notations in the port mapping of your docker run command, see Storage Node.\nWhat else should be configured? #  Linux users, please take a look at Linux Configuration for UDP\nWindows users, please take a look at the Firewall configuration instructions in the Port Forwarding section for Windows.\n"},{"id":66,"href":"/dcs/getting-started/quickstart-objectbrowser/","title":"Quickstart - Object Browser","first":"/dcs/","section":"Getting Started","content":"Quickstart - Object Browser #  Introduction #  The Storj DCS Satellite Admin Console supports uploading and managing objects directly through the browser with no command-line tool required. This component uses our hosted S3-compatible Gateway service.\nBy using hosted Gateway MT you are opting in to server-side encryption.  Configure Object Browser Access #  Navigate to the Buckets page within your project. If you do not have any buckets yet - we will create a demo-bucket for you.\nWhen you click on the bucket, you will be prompted to read carefully - The object browser uses Server-side encryption.\nDon\u0026rsquo;t forget to save your Encryption Passphrase generated below, you will need it for future access.  If this is your first time using the object browser, you must create an encryption passphrase. We strongly encourage you to use a mnemonic phrase. The GUI automatically generates one on the client side for you in the Generate a new passphrase tab. You can also download it as a text file.\nAlternatively, you can enter your own passphrase in the Enter your own passphrase tab.\nTo continue, you need to mark the checkbox [v] I understand, and I have saved the passphrase, this will enable the button Next \u0026gt;.\nWhen you click the Next \u0026gt; button you will be placed into the Objects view if you already have buckets, otherwise a new bucket demo-bucket will be created and you will be placed into that bucket view.\nUpload files and folders #  If you have not yet created a bucket, the bucket demo-bucket will be created automatically to allow you to upload objects right away.\nTo upload your first object, drag it into the browser or select Upload File and browse to the file you wish to upload.\nYou can upload not only files but folders too, just drag them into the browser or select Upload Folder and browse to the folder you wish to upload.\nIf you want to create a folder, you can do that with the New Folder button.\nWhen you drag and drop your file into the Satellite Admin Console Object Browser, the Storj DCS S3-compatible Gateway will encrypt the data using server-side encryption, break large files into 64MB Segments (or for smaller files a single segment), then erasure code the segments, breaking each segment into 80 pieces, then distributing those pieces over our network of thousands of independently operated storage nodes.  Deleting files #  1. If you select the three vertical dots on the right side of the file, a popup menu will appear:\n2. Select the Delete command.\n3. Confirm deletion with Yes.\nCreating buckets #  Buckets are your containers that store objects.\nYou can create your buckets in the Objects view or if you click on the \u0026lt;-Back to Buckets button in the bucket view.\nTo create a new bucket, click the New bucket button in the Objects view. A new module window will pop up called Create Bucket. Please provide a name using only lower case alphanumeric characters and dashes (this is a limitation for compatibility with existing object storages).\nAfter creating your new bucket, you will be placed into the bucket where you can make folders and/or upload files.\nDeleting buckets #  1. Clicking the three vertical dots on the right side of the bucket, a popup menu will appear:\n2. Click the Delete command\n3. Type the Bucket Name and Confirm Delete Bucket.\nBe careful when deleting buckets - If you have objects in the bucket being deleted, they will be deleted too!  Share a file #  After an upload completes, you will have the option of creating a share link. If you wish, click the file name - it will open a preview with a map. Here you can click the Share button.\nOr you can click on the three vertical dots to the right of the file you want to share, and select Share to share your object.\nThe Share pop-up window allows you to share the link via social media or copy it with Copy Link.\nThe share link includes a rendering of where the pieces of your file are located on the globally distributed network of storage nodes, as well as a preview of that file.\nThis concludes the Object Browser Quickstart.\n"},{"id":67,"href":"/dcs/how-tos/sync-files-with-rclone/rclone-with-hosted-gateway/","title":"Rclone with Hosted Gateway","first":"/dcs/","section":"Sync Files With Rclone","content":"Rclone with Hosted Gateway #  Selecting an Integration Pattern #  Use our S3 compatible Hosted Gateway integration pattern to increase upload performance and reduce the load on your systems and network. Uploads will be encrypted and erasure-coded server-side, thus a 1GB upload will result in only in 1GB of data being uploaded to storage nodes across the network.\nUse this pattern for #   Reduced upload time Reduction in network load  By selecting this integration pattern you are opting in to server-side encryption  Prerequisites #  Generate Credentials to the Gateway MT #  Navigate to the Access page within your project and then click on Create Access Grant +. A modal window will pop up and you can enter a name for this access grant.\nAssign the permissions you want this access grant to have, then click on Continue in Browser:\nEnter the Encryption Passphrase you used for your other access grants. If this is your first access grant, we strongly encourage you to use a mnemonic phrase as your encryption passphrase. (The GUI automatically generates one on the client-side for you)\nClick on the Generate S3 Gateway Credentials link and then click on the \u0026lsquo;Generate Credentials\u0026rsquo; button.\nCopy your Access Key, Secret Key, and Endpoint to a safe location.\nNow you are ready to configure Rclone\nSetup #  First, Download and extract the rclone binary onto your system.\nExecute the config command:\nrclone config A text-based menu will prompt. Type n and hit Enter to create a new remote configuration, select n (New Remote).\ne) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q\u0026gt; Enter a name for the new remote configuration, e.g. waterbear.\nwaterbear A long list of supported storage backends will prompt. Select 5 (5 / Amazon S3 Compliant Storage Provider) and hit Enter.\n 5 / Amazon S3 Compliant Storage Providers including AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Lyve Cloud, Minio, RackCorp, SeaweedFS, and Tencent COS  \\ (s3) A further list of S3 storage providers will prompt. Select 14 (14 / S3 Compatible Gateway) \\ (Storj) and hit Enter.\n14 / Storj (S3 Compatible Gateway)  \\ (Storj) A choice will be given on how you will enter credentials. Strike Enter for the default choice of 1 (Enter AWS credentials in the next step).\n1 / Enter AWS credentials in the next step You will be asked for your Access Key ID followed by the Secret Access Key that you previously generated, follow the pattern in the code block below.\n# AWS Access Key ID # Enter your \u0026lt;AccessKeyId\u0026gt; \u0026lt;AccessKeyId\u0026gt; Strike Enter  # AWS Secret Access Key # Enter your \u0026lt;SecretAccessKeyId\u0026gt; \u0026lt;SecretAccessKeyId\u0026gt; Strike Enter You will be asked for what Region to connect to, Endpoint, and Location Constraint.\n# Region to connect to Strike enter for default  # (1 / Use this if unsure. Will use v4 signatures and an empty region)  # Endpoint for S3 API # Enter the Storj DCS Gateway URL https://gateway.us1.storjshare.io Strike Enter  # Location Constraint Strike enter for default  # (\u0026#34;\u0026#34;) A list of Canned Access Control Lists used when creating buckets will be presented.\n# Canned ACL used when creating buckets and storing or copying objects # Select your prefured option, otherwise strike enter for the most secure default Strike enter for default or enter your prefured number followd by enter You will be asked if you want to edit the advanced config.\n# Edit Advanced Config? (y/n) Strike enter for default # y) Yes # n) No (Default) y/n\u0026gt; y # Value \u0026#34;bucket_acl\u0026#34; = \u0026#34;\u0026#34; # Edit? (y/n)\u0026gt; # y) Yes # n) No (Default) Strike enter for default until reach the \u0026#34;chunk_size\u0026#34; # Value \u0026#34;chunk_size\u0026#34; = \u0026#34;5M\u0026#34; # Edit? (y/n)\u0026gt; # y) Yes # n) No (default) y/n\u0026gt; y # Chunk size to use for uploading. # # When uploading files larger than upload_cutoff or files with unknown # size (e.g. from \u0026#34;rclone rcat\u0026#34; or uploaded with \u0026#34;rclone mount\u0026#34; or google # photos or google docs) they will be uploaded as multipart uploads # using this chunk size. # # Note that \u0026#34;--s3-upload-concurrency\u0026#34; chunks of this size are buffered # in memory per transfer. # # If you are transferring large files over high-speed links and you have # enough memory, then increasing this will speed up the transfers. #  # Rclone will automatically increase the chunk size when uploading a # large file of known size to stay below the 10,000 chunks limit. # # Files of unknown size are uploaded with the configured # chunk_size. Since the default chunk size is 5MB and there can be at # most 10,000 chunks, this means that by default the maximum size of # a file you can stream upload is 48GB. If you wish to stream upload # larger files then you will need to increase chunk_size. # Enter a size with suffix k,M,G,T. Press Enter for the default (\u0026#34;5M\u0026#34;). chunk_size\u0026gt; 64M Hit enter for default until end of advanced configuration A summary of the remote configuration will prompt. Type yand hit Enter to confirm.\n[waterbear] type = s3 provider = Other env_auth = false access_key_id = \u0026lt;AccessKey\u0026gt; secret_access_key = \u0026lt;SecretAccessKey\u0026gt; endpoint = https://gateway.us1.storjshare.io chunk_size = 64M -------------------- y) Yes this is OK (default) e) Edit this remote d) Delete this remote y/e/d\u0026gt; Now you should see one remote configuration available. Enter q and hit Enter to quit the configuration wizard.\nCurrent remotes:  Name Type ==== ==== waterbear s3 For additional security, you should consider using the s) Set configuration password option. It will encrypt the rclone.conf configuration file. This way secrets like the access token, the encryption passphrase, and the access grant can\u0026rsquo;t be stolen if an attacker gains access to your configuration file.  Create a Bucket #  Use the mkdir command to create new bucket, e.g. mybucket.\nrclone mkdir waterbear:mybucket List All Buckets #  Use the lsf command to list all buckets.\nrclone lsf waterbear: Note the colon (:) character at the end of the command line.  Delete a Bucket #  Use the rmdir command to delete an empty bucket.\nrclone rmdir waterbear:mybucket Use the purge command to delete a non-empty bucket with all its content.\nrclone purge waterbear:mybucket Upload Objects #  Use the copy command to upload an object.\nrclone copy --progress ~/Videos/myvideo.mp4 waterbear:mybucket/videos/ The --progress flag is for displaying progress information. Remove it if you don\u0026rsquo;t need this information.  Use a folder in the local path to upload all its objects.\nrclone copy --progress ~/Videos/ waterbear:mybucket/videos/ Only modified files will be copied.  List Objects #  Use the ls command to list recursively all objects in a bucket.\nrclone ls waterbear:mybucket Add the folder to the remote path to list recursively all objects in this folder.\nrclone ls waterbear:mybucket/videos/ Use the lsf command to list non-recursively all objects in a bucket or a folder.\nrclone lsf waterbear:mybucket/videos/ Download Objects #  Use the copy command to download an object.\nrclone copy --progress waterbear:mybucket/videos/myvideo.mp4 ~/Downloads/ The --progress flag is for displaying progress information. Remove it if you don\u0026rsquo;t need this information.  Use a folder in the remote path to download all its objects.\nrclone copy --progress waterbear:mybucket/videos/ ~/Downloads/ Delete Objects #  Use the deletefile command to delete a single object.\nrclone deletefile waterbear:mybucket/videos/myvideo.mp4 Use the delete command to delete all object in a folder.\nrclone delete waterbear:mybucket/videos/ Print the Total Size of Objects #  Use the size command to print the total size of objects in a bucket or a folder.\nrclone size waterbear:mybucket/videos/ Sync Two Locations #  Use the sync command to sync the source to the destination, changing the destination only. Doesn’t transfer unchanged files, testing by size and modification time or MD5SUM. Destination is updated to match source, including deleting files if necessary.\nrclone sync --progress ~/Videos/ waterbear:mybucket/videos/ The --progress flag is for displaying progress information. Remove it if you don\u0026rsquo;t need this information.  Since this can cause data loss, test first with the --dry-run flag to see exactly what would be copied and deleted.  The sync can be done also from Storj DCS to the local file system.\nrclone sync --progress waterbear:mybucket/videos/ ~/Videos/ Or between two Storj DCS buckets.\nrclone sync --progress waterbear-us:mybucket/videos/ waterbear-europe:mybucket/videos/ Or even between another cloud storage and Storj DCS.\nrclone sync --progress s3:mybucket/videos/ waterbear:mybucket/videos/ "},{"id":68,"href":"/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/set-up-uplink-cli/","title":"Set Up Uplink CLI with Access Grant","first":"/dcs/","section":"Uploading Your First Object CLI","content":"Set Up Uplink CLI with Access Grant #  1. Check Prerequisites.\n2. Save Access Grant to a file. The Access Grant that you created in the web interface (or in uplink CLI) needs to be saved to disk in a plain text file for simplicity (for example - Mac terminal would not allow you to paste the whole access grant directly due terminal limitations). Specify the path to the saved access grant in the following command (~/Downloads/accessgrant.txt for example).\n3. Import Access Grant.\nWindows PowerShell #  For security reasons it\u0026rsquo;s better to use a casual user to work from the CLI, thus please run PowerShell as a casual user, not as an Administrator.   Navigate to the directory your uplink.exe file is located:\n./uplink.exe access save main accessgrant.txt Linux uplink access save main accessgrant.txt macOS uplink access save main accessgrant.txt  Please note that Storj Labs does not know or store your encryption passphrase, so if you lose it, you will not be able to recover your files.  7. Your Uplink is configured and ready to use!\n"},{"id":69,"href":"/node/setup/cli/storage-node/","title":"Storage Node","first":"/node/","section":"CLI Install","content":"Storage Node #  Download the Storage Node Docker Container #  docker pull storjlabs/storagenode:latest Storage Node Concepts #  Before running your Storage Node for the first time, please note the Storage Node concepts to be used.\n   Concepts Description     -p How to specify ports - which are points through which information flows between your Node and the Storj network. -p \u0026lt;host-port\u0026gt;:\u0026lt;container-port\u0026gt;/tcp -p \u0026lt;host-port\u0026gt;:\u0026lt;container-port\u0026gt;/udp /tcpand/udpwill allow your node to connect with the network via both TCP and UDP\n\n-p 28967:28967 is the core port used to connect with the network.\n-p 14002:14002 is the port used for the storage node GUI dashboard.\n   WALLET A wallet address to receive STORJ token payouts for running the node. Learn how to obtain a valid payout address.   EMAIL Email address so that we can notify you when a new version has been released. (recommended)   ADDRESS External IP address or the DDNS you configured and the port you opened on your router \u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt;\n\nIf you are using a custom port other than 28967, you have to change the -p 28967:28967 to -p \u0026lt;port\u0026gt;:28967\n   STORAGE How much disk space you want to allocate to the Storj network \nBe sure not to over-allocate space! Allow at least 10% extra for overhead. If you over-allocate space, you may corrupt your database when the system attempts to store pieces when no more physical space is actually available on your drive. The minimum storage shared requirement is 500 GB, which means you need a disk of at least 550 GB total size to allow for the 10% overhead.\n\nIf you set up a brand new node, make sure the storage destination folder doesn\u0026rsquo;t already have aconfig.yaml and/or storage folder inside, otherwise the storagenode container could fail to start.\n   \u0026lt;identity-dir\u0026gt; Replace it to the location of your identity files. You can copy the absolute path from the output of the identity commands you ran earlier.   \u0026lt;storage-dir\u0026gt; Replace it with the local directory where you want files to be stored on your hard drive for the network. Please consider using a subfolder instead of the root of the disk, this could prevent starting from scratch if the disk were to disappear/accidentally disconnect.\nThe network-attached location could work, but it is neither supported nor recommended!\n\nNote: the current database backend is BoltDB which requires mmap, hence you have to use a file system which supports mmap.\n    Setting up the Storage Node #  The setup step must be performed only once. If a node has already been set up, running with the SETUP flag will result in failure.  Previous versions of thedocker runcommand that used the -v rather than the --mount option will not work properly. Copy the updated command below.  A network-attached storage location may work, but this is neither supported nor recommended!  Windows  Copy the command into a plain text editor (the Notepad++ is recommended), do not use any word processor:  docker run --rm -e SETUP=\u0026#34;true\u0026#34; --mount type=bind,source=\u0026#34;\u0026lt;identity-dir\u0026gt;\u0026#34;,destination=/app/identity --mount type=bind,source=\u0026#34;\u0026lt;storage-dir\u0026gt;\u0026#34;,destination=/app/config --name storagenode storjlabs/storagenode:latest Linux Linux Users: You must static mount via /etc/fstab. Failure to do so will put you in high risk of failing audits and getting disqualified. Here\u0026rsquo;s how to do that.   Copy the command into a plain text editor like a nano:  docker run --rm -e SETUP=\u0026#34;true\u0026#34; \\ --user $(id -u):$(id -g) \\ --mount type=bind,source=\u0026#34;\u0026lt;identity-dir\u0026gt;\u0026#34;,destination=/app/identity \\ --mount type=bind,source=\u0026#34;\u0026lt;storage-dir\u0026gt;\u0026#34;,destination=/app/config \\ --name storagenode storjlabs/storagenode:latest macOS  Copy the command into a plain text editor (do not use any word processors include Notes):  docker run --rm -e SETUP=\u0026#34;true\u0026#34; \\ --user $(id -u):$(id -g) \\ --mount type=bind,source=\u0026#34;\u0026lt;identity-dir\u0026gt;\u0026#34;,destination=/app/identity \\ --mount type=bind,source=\u0026#34;\u0026lt;storage-dir\u0026gt;\u0026#34;,destination=/app/config \\ --name storagenode storjlabs/storagenode:latest  2. Replace the \u0026lt;identity-dir\u0026gt; and \u0026lt;storage-dir\u0026gt; with your parameters.\n3. Copy the updated command.\n4. Run it in a terminal window.\n5. Your node has been set up!\nRunning the Storage Node #  Windows 1. Copy the command into a text editor (the Notepad++ is recommended), do not use any word processor:\ndocker run -d --restart unless-stopped --stop-timeout 300 -p 28967:28967/tcp -p 28967:28967/udp -p 127.0.0.1:14002:14002 -e WALLET=\u0026#34;0xXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34; -e EMAIL=\u0026#34;user@example.com\u0026#34; -e ADDRESS=\u0026#34;domain.ddns.net:28967\u0026#34; -e STORAGE=\u0026#34;2TB\u0026#34; --mount type=bind,source=\u0026#34;\u0026lt;identity-dir\u0026gt;\u0026#34;,destination=/app/identity --mount type=bind,source=\u0026#34;\u0026lt;storage-dir\u0026gt;\u0026#34;,destination=/app/config --name storagenode storjlabs/storagenode:latest Linux 1. Copy the command into a plain text editor, like a nano:\ndocker run -d --restart unless-stopped --stop-timeout 300 \\  -p 28967:28967/tcp \\  -p 28967:28967/udp \\  -p 127.0.0.1:14002:14002 \\  -e WALLET=\u0026#34;0xXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34; \\  -e EMAIL=\u0026#34;user@example.com\u0026#34; \\  -e ADDRESS=\u0026#34;domain.ddns.net:28967\u0026#34; \\  -e STORAGE=\u0026#34;2TB\u0026#34; \\  --user $(id -u):$(id -g) \\  --mount type=bind,source=\u0026#34;\u0026lt;identity-dir\u0026gt;\u0026#34;,destination=/app/identity \\  --mount type=bind,source=\u0026#34;\u0026lt;storage-dir\u0026gt;\u0026#34;,destination=/app/config \\  --name storagenode storjlabs/storagenode:latest macOS 1. Copy the command into a plain text editor (do not use any word processors include Notes):\ndocker run -d --restart unless-stopped --stop-timeout 300 \\  -p 28967:28967/tcp \\  -p 28967:28967/udp \\  -p 127.0.0.1:14002:14002 \\  -e WALLET=\u0026#34;0xXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34; \\  -e EMAIL=\u0026#34;user@example.com\u0026#34; \\  -e ADDRESS=\u0026#34;domain.ddns.net:28967\u0026#34; \\  -e STORAGE=\u0026#34;2TB\u0026#34; \\  --user $(id -u):$(id -g) \\  --mount type=bind,source=\u0026#34;\u0026lt;identity-dir\u0026gt;\u0026#34;,destination=/app/identity \\  --mount type=bind,source=\u0026#34;\u0026lt;storage-dir\u0026gt;\u0026#34;,destination=/app/config \\  --name storagenode storjlabs/storagenode:latest  2. Edit the WALLET, EMAIL, ADDRESS, STORAGE and replace the \u0026lt;identity-dir\u0026gt;, and \u0026lt;storage-dir\u0026gt; with your parameters.\n3. Copy the updated command.\n4. Run it in a terminal window.\n5. You\u0026rsquo;re officially a Storage Node operator! 🎉\nYou can also check to see if the node was started properly by by running the following command in the terminal\ndocker ps -a Check the status of your node #  You can check the status of your node, along with many other statistics by running the web dashboard. It will look like this:\n"},{"id":70,"href":"/dcs/api-reference/uplink-cli/","title":"Uplink CLI","first":"/dcs/","section":"SDK \u0026 Reference","content":"Uplink CLI #  To setup uplink see Prerequisites.  The uplink command can take the following child commands:\n   Command Description      access set of commands to manage accesses    cp copy a file from outside of Storj bucket to inside or vice versa    ls List objects and prefixes or all buckets    mb make a new bucket    meta metadata related commands    mv moves a Storj object to another location in Storj DCS    rb remove a bucket    rm remove a file from a Storj bucket    setup create an uplink config file    share shares restricted access to objects    Flags #     Flag Description     --advanced if used in with -h, print advanced flags help   --config-dir string main directory for uplink configuration    "},{"id":71,"href":"/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/","title":"Uploading Your First Object CLI","first":"/dcs/","section":"Quickstart - Uplink CLI","content":"Uploading Your First Object CLI #  Install and configure the CLI and follow the steps below to upload your first object to Storj DCS.\nEvery time you upload a file, the Storj DCS CLI will do all the heavy lifting - encrypt the data using end-to-end encryption (including path and metadata), break large files into 64MB Segments (or for smaller files into a single segment), then erasure code the segments, breaking each segment into 80 pieces, then distributing those pieces over our network of thousands of independently operated storage nodes. All of that happens in the background with a simple cp command.  Prerequisites Create an Access Grant Set Up Uplink CLI with Access Grant Create a Bucket Upload an Object View Distribution of an Object "},{"id":72,"href":"/dcs/api-reference/s3-compatible-gateway/using-presigned-urls/","title":"Using presigned URLs","first":"/dcs/","section":"Storj-hosted S3 Compatible Gateway","content":"Using presigned URLs #  Introduction #  All objects and paths are private and encrypted by default. However, it is possible to use a pre-signed URL via our S3-compatible gateway to enable unauthenticated customers/users to upload objects to buckets or access objects in buckets without providing an Access Grant or S3-compatible gateway access credentials.\nHTTP GET vs Storj Linkshare Service #  While we support this behavior via the S3-compatible pre-signed URL function, as an alternative to sharing with a customer/user via a GET, consider utilizing our Linkshare service. One advantage of this approach is the ability to easily create perpetual share links, valid until you remove them or until a configurable end date of any duration. You can even host a static webpage on Storj DCS via Linkshare.\nTutorial #  The goal of the following tutorial is to guide you in the creation of pre-signed URLs for storage DCS using a Python script and our multi-tenant hosted gateway.\nOur lab example took place on MacOSX and used BREW as a package manager. Depending on your host operating system, you will need to use the appropriate package manager to fetch the prerequisites listed below.\nOur implementation of the S3 standard allows additional configuration options. Please reference the official AWS S3 User Guide for additional details.\nPrerequisites #  # Install python brew install python3 # install boto3 pip3 install boto3 pip3 install requests Script #  Create your script my_put_script.py #  This script will create a “put” pre-signed URL to be used for uploading\nBelow you can see we need to set the following parameters:\n ACCESS_KEY - S3 Credential created with Access SECRET_KEY - S3 Credential created with Access URL - You can use us1, eu1, or ap1 depending on location BUCKET NAME - Name of the bucket related to this URL url - Use ‘put_object to upload and ‘get_object’ to download/share Key - Path of the object you wish to upload ExpiresIn - How long the URL will be valid from its creation (in seconds)  import boto3 ACCESS_KEY = \u0026#34;Your_Access_Key\u0026#34; SECRET_KEY = \u0026#34;Your_Secret_Key\u0026#34; URL = \u0026#34;https://gateway.us1.storjshare.io\u0026#34; BUCKET_NAME = \u0026#34;yourbucketname\u0026#34; session = boto3.session.Session() s3 = session.client(service_name=\u0026#34;s3\u0026#34;, aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, endpoint_url=URL) url = s3.generate_presigned_url(\u0026#39;put_object\u0026#39;, Params={\u0026#34;Bucket\u0026#34;:BUCKET_NAME, \u0026#34;Key\u0026#34;:\u0026#34;path/within/bucket/file.name\u0026#34;}, ExpiresIn=3600) print(url) Execute script myscript.py #  The output of this script will be your pre-signed URL\npython3 my_put_script.py Upload with URL and Curl #  Set for file name and extension and paste in your newly generated pre-signed URL. Note that the pre-signed URL below is invalid and included as an example only.\ncurl -v --upload-file file.name \u0026#34;https://gateway.us1.storjshare.io/yourbucketname/path/within/bucket?AWSAccessKeyId=jvruleqdpbwqx7vxmwgqbtlbmapa\u0026amp;Signature=fUNxawPyFd%2F9apR%2FZnKmR%2BPXGCA%3D\u0026amp;Expires=1628019103\u0026#34; "},{"id":73,"href":"/dcs/concepts/access/encryption-and-keys/when-to-use-different-encryption-keys/","title":"When to use different encryption keys","first":"/dcs/","section":"Encryption Keys","content":"When to use different encryption keys #  With encryption embedded into Access Grants, it is possible to create multiple Access Grants with the same access authorization but different encryption keys. The default behavior of the Uplink Client is not to display data that the Uplink Client cannot decrypt. It is possible to view the data in it\u0026rsquo;s encrypted state, but it is not possible to derive an encryption key or otherwise reverse engineer an encryption key from an Access Grant with authorization to read data, if that Access Grant does not have an encryption key scoped to the data in question.\nIn general, the best practice is to use one encryption passphrase per bucket. If an object with the same path and object name uploaded by two uplinks with encryption keys derived from the same encryption passphrase, the most recent upload will over-write the older object.\nIf an object with the same path and object name uploaded by two uplinks with encryption keys derived from different encryption passphrases, the objects will be uploaded as two separate objects.\nBecause encryption keys are hierarchically deterministic, when an Access Grant is used to create a restricted Access Grant that is restricted to a particular path prefix, the encryption key of the child Restricted Access Grant is derived from the parent Access Grant. In this case, the Parent Access Grant could decrypt the data within the path restriction associated with the parent Access Grant and the child Restricted Access Grant, but the encryption key in the child Restricted Access Grant could not be used to decrypt data to which the parent Access Grant might have access but which is outside the scope of the path restriction of the the child Restricted Access Grant.\nThe only use case for which using different encryption passphrases is common is in the case of a multiuser or multi-tenant application, storing data on behalf of different users or entities. In this case, the default behavior could be used in which is user or tenant within the application would be logically represented as a top-level path within the bucket that corresponds to the application. Each user or tenant within the application would be issued a child Restricted Access Grant scoped to the top level path associated to that user or tenant.\nThe child Restricted Access Grant would allow a user or tenant to interact with object data restricted to their top level path, but not allow access outside of that path. Users or tenants would be unable to view or interact with data associated with peer tenants or users.\nIt is also possible to overwrite the encryption key within a child Restricted Access Grant, allowing users to \u0026ldquo;choose\u0026rdquo; their own encryption passphrase. If those users lost access to that passphrase and the associated Access Grant, the data would be unrecoverable.\n"},{"id":74,"href":"/dcs/concepts/access/access-grants/when-to-use-the-satellite-web-interface-and-when-to-use-the-cli/","title":"When to use the Satellite Web Interface and When to use the CLI","first":"/dcs/","section":"Access Grants","content":"When to use the Satellite Web Interface and When to use the CLI #  You can generate an Access Grant in the Satellite Admin Console, or you can use either our Go Library or the CLI.\nWhen to use the Satellite Admin Console #  In general, you use the Satellite Admin Console web interface to create an Access Grant that is then used to set up whatever client tool you are using. In order to configure and use the CLI or an application like FileZilla or Rclone, you must first create an Access Grant to configure the client. You may create an unrestricted Access Grant or Restricted Access Grant with limited access. The Satellite Admin Console web interface is also used to generate credentials if you want to use an application with the Storj hosted S3-compatible gateway.\nRemember: When you use an Access Grant to generate credentials for the Storj hosted S3-compatible gateway, the hosted gateway uses server-side encryption. If end-to-end encryption is essential for your use case, you should encrypt your data before sending it to the hosted gateway, or use a self-hosted S3-compatible Gateway.  When to use an Uplink client #  Once you have created an Access Grant from the Satellite Admin Console web interface, the CLI, client library or other client tool can then use that Access Grant to interact with the Storj DCS service, or create additional restricted Access Grants - child Access Grants of the parent created in the Satellite Admin Console. The Uplink Client can be used to create additional child Restricted Access Grants.\nRemember: When you create child Restricted Access Grants from a parent Restricted Access Grant, the child Restricted Access Grants can have the same level of access as the parent or less access, but never more.\nFor example: A parent Restricted Access Grant that only has Read and Write access to a particular may be used to create child Restricted Access Grants that have Read-only access to the bucket or just one path within that bucket. But, a parent Restricted Access Grant that only has Read and Write access to a particular may NOT be used to create child Restricted Access Grants that have Read-write-delete access to the bucket or another path within another bucket to which the parent Restricted Access Grant does not have any access.\n If you want to learn more, check out the Key Architecture Constructs section or read all about Access Management.\nLearn how to create an Access Grant using the Satellite Admin Console in the Satellite Admin Console Quickstart.\nLearn how to create an Access Grant using the Uplink CLI in the Uplink CLI Quickstart.\n"},{"id":75,"href":"/node/resources/faq/where-can-i-check-for-a-new-version/","title":"Where can I check for a new version?","first":"/node/","section":"FAQ's","content":"Where can I check for a new version? #  We have a proposal for the Storage Nodes Operators - let\u0026rsquo;s work together and keep our network up to date to offer a great service to our Customers!\nPlease, read this thread on the forum: Keeping your node up to date, changes to storage node software! PLEASE READ this thread!\nAutomatic updater #  The best way is to run the recommended Automatic updates (for Docker) - this is not necessary with the Windows GUI version as it already has an automatic updater included, and the Linux version with similar capabilities is coming soon.\nCommunity Forum #  You can check for the latest version of storagenode on our forum: https://forum.storj.io/tag/official-changelog\nPlease always read the changelog whenever a new version comes out, as it may include important changes which could affect your node or your potential income.\nYou can subscribe to the changelog thead on the forum to be updated!\nOther ways to check the version #  You can also track our GitHub repo: https://github.com/storj/storj/releases/latest and https://hub.docker.com/r/storjlabs/storagenode\n"},{"id":76,"href":"/dcs/api-reference/uplink-cli/access-command/access-register/","title":"access register","first":"/dcs/","section":"access","content":"access register #  Usage #  Windows ./uplink.exe access register \u0026lt;flags\u0026gt; \u0026lt;ACCESS-GRANT\u0026gt; Linux uplink access register \u0026lt;flags\u0026gt; \u0026lt;ACCESS-GRANT\u0026gt; macOS uplink access register \u0026lt;flags\u0026gt; \u0026lt;ACCESS-GRANT\u0026gt;  Flags #     Flag Description     --access string the serialized access, or name of the access to use   --auth-service string the address to the service you wish to register your access with   --public if the access should be public. Default false    Example #  Once you have an access grant from the Satellite Admin Console or uplink share you can register it with a GatewayMT auth service and designate the access to be public (no secret ket necessary to access) or private. If you want to use it to host a static site or share a URL, you must create a public access.\nWindows ./uplink.exe access register --public=true --auth-service=\u0026lt;gateway auth service url\u0026gt; \u0026lt;ACCESS-GRANT\u0026gt; Linux uplink access register --public=true --auth-service=\u0026lt;gateway auth service url\u0026gt; \u0026lt;ACCESS-GRANT\u0026gt; macOS uplink access register --public=true --auth-service=\u0026#34;https://auth.us1.storjshare.io\u0026#34; 1CoNaDGVYBVZcurLF99kmPrGLfcMw5qw9Vxv9hAKp9NK...  ========== CREDENTIALS =================================================================== Access Key ID: jw7w7n2... Secret Key : jycbodr... Endpoint : https://gateway.us1.storjshare.io "},{"id":77,"href":"/dcs/concepts/access/access-revocation/","title":"Access Revocation","first":"/dcs/","section":"Access Management","content":"Access Revocation #  While delegated authorization and the ability to generate Access Grants at the edge provides the opportunity to create more private and secure applications, there are design considerations to take into account when building applications with data sharing capabilities based on long-lived bearer tokens.\nWhile it is possible to create Access Grants with time-based restrictions and to required Access Grants be refreshed as they expire, applications must be able to revoke access to data. Access grant revocation is supported on Storj DCS in two ways:\n Deleting a primary Access Grant - from the Satellite Admin Console, it is possible to delete a primary Access Grant. Deleting a primary Access Grant also immediately invalidates all child Restricted Access Grants derived from that primary Access Grant. Adding an Access Grant to the Revocation service - by adding an Access Grant to the authorization revocation service, only the API Key associated with that Access Grant is revoked (along with any child Restricted Access Grants further derived from that Access Grant). This can be done via the CLI.  Imagine the case where you have used a primary Access Grant to create dozens of child Restricted Access Grants. Deleting the primary Access Grant immediately invalidates all of the Access Grants derived from that primary Access Grant. Conversely, adding one of the child Restricted Access Grants to the Access Revocation service invalidates only that Access Grant and any child Restricted Access Grants derived from it. Deleting a Primary Access Key has the same effect as adding it to the Access Revocation service.\nThe main differences is that primary Access Grants are created from the Satellite Admin Console and therefore the Satellite is aware that the associated API Key was created. The Satellite has no way to know if further child Restricted Access Grants were created client side. While this makes for much more private application sharing, it does require that the associated Access Grants be managed appropriately.\n"},{"id":78,"href":"/dcs/billing-payment-and-accounts-1/storj-token/changing-payment-methods/","title":"Changing Payment Methods","first":"/dcs/","section":"Payment Methods","content":"Changing Payment Methods #  Users may change payment methods on accounts, but at least one valid payment method must be configured on an account at all times.\nChanging Credit Cards #  If you want to change the credit card on an account, follow the process outlined above to add a second card under Payment Methods, Add Card.\nOnce a second card has been added, you may select the new card as the Default card. It will now be possible to remove the other card.\nChanging from Credit Card to STORJ Token #  If you want to pay only in STORJ token, you must not add a credit card and should only have a STORJ token balance on the account. If you previously added a credit card and want to pay using STORJ token instead, follow the instructions above to deposit funds to the account using STORJ token.\nIf a user wants to remove a credit card that has been added to an account and only use STORJ, the user should add a deposit to the account using the instructions above, then submit a support request to remove the credit card.\nAs described above any invoices will be first debited against the STORJ token payment balance and only if an unpaid balance remains will the credit card be debited.\nChanging from STORJ token to Credit Card #  If a user wants to change from paying only in STORJ token to paying with a credit card, the user should add a credit card to the account. Any invoices will be first debited against the remaining STORJ token payment balance and once the balance is exhausted, any unpaid balance remaining will be debited against the credit card. All future payments will then be debited against the credit card.\\\n"},{"id":79,"href":"/dcs/how-tos/configure-tools-for-the-partner-program/","title":"Configure Tools for the Partner Program","first":"/dcs/","section":"How To's","content":"Configure Tools for the Partner Program #  Partner Program #  The Storj Partner Ecosystem enables developers to build Storj DCS Connectors, which their customers can use to store data on Storj DSC.\nThe data itself is client-side encrypted, however we are able to measure the aggregate volume of storage and bandwidth usage. When a user of a Storj DCS Connector stores data in a bucket, we are able to give the partner attribution for the stored data and the used bandwidth for the Connector Integration, and provide programmatic revenue share.\nYou can learn more about our partner program here.\nValue Attribution #  Value attribution is done on a per bucket basis. To recognize which partner the shared revenue should go to, we use a user agent that identifies it. A bucket can only have one user agent value and it can be set only once, and only on an empty bucket. This has the following consequences:\n Uploading an object to a bucket can only be done after setting the user agent. Otherwise, the bucket won\u0026rsquo;t be empty and you will be unable to set the user agent. If you upload an object to a bucket with a defined user agent, the shared revenue will go to the corresponding partner. If it is not your user agent, it won\u0026rsquo;t be in your shared revenue.  Setting the User Agent #  Before continuing, beware that partner value attribution is only possible if you are registered as such by Storj. You can access an up to date list of recognized user agents here.  Uplink CLI #  UserAgent can only be configured during setup:\nuplink setup --client.user-agent \u0026#34;MyCompany\u0026#34; or by adding or updating the following lines of the uplink configuration yaml:\n# User-Agent used for connecting to the satellite client.user-agent: \u0026#34;MyCompany\u0026#34; S3 Gateway #  UserAgent can only be configured during setup:\ngateway setup --client.user-agent \u0026#34;MyCompany\u0026#34; or by adding or updating the following lines of the uplink configuration yaml:\n# User-Agent used for connecting to the satellite client.user-agent: \u0026#34;MyCompany\u0026#34; In code #  UserAgent can be configured from Go code:\nuplink.Config{UserAgent: \u0026#34;MyCompany\u0026#34;}.OpenProject(...) "},{"id":80,"href":"/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/create-a-bucket/","title":"Create a Bucket","first":"/dcs/","section":"Uploading Your First Object CLI","content":"Create a Bucket #  Check Prerequisites.\nCreate a bucket in our Project #  Let\u0026rsquo;s create a bucket to store photos of cake for our \u0026ldquo;food app\u0026rdquo; project:\nWindows ./uplink.exe mb sj://cakes Linux uplink mb sj://cakes macOS uplink mb sj://cakes  Result\n"},{"id":81,"href":"/dcs/getting-started/satellite-developer-account/dashboard/","title":"Dashboard","first":"/dcs/","section":"Quickstart - Satellite Admin Console","content":"Dashboard #  When you log into the Satellite Admin Console, you start on the Project Dashboard for your default Project. A Project is the basic unit for aggregating usage, calculating billing, invoicing fees, collecting payment, and handling access management. Users can create multiple Projects and projects are invoiced separately. Within a Project, usage is tracked at the Bucket level and aggregated for invoicing to the Project. Project names are not client-side encrypted so that they may be rendered in the Satellite user interface. There are two main drivers for creating multiple Projects: access management and billing.\nLearn more about Projects in Key Architecture Constructs under Concepts.  On the Project Dashboard, there are a number of navigational elements and information displays:\n  Projects management - This element allows you to add Projects and switch between different Projects. There you also have a Manage Projects setting.\n\\\n  Project Navigation - This element allows you to move between the different functions related to the project you have selected, to view the Dashboard, use the Objects to interact with data stored on Storj DCS through a web browser interface, create Access for native integrations and credentials for the hosted S3-compatible gateway, invite other developers to collaborate with you on your Project in Users, see Billing, check Resources and Quick Start, manage your Account in My Account.\\\n  Storage Utilization - This element displays the amount of storage utilized in the current month measured in GB hours.\\\n  Bandwidth Utilization - This element element displays the amount of download bandwidth utilized in the current month measured in GB.\\\n  Project Details - This element displays the number of users added to a project, the number of Access Grants, the current number of Buckets and the estimated charge for the current month.\\\n  Bucket Information - This element displays the names of Buckets, the current month usage on Buckets and the number of objects in Buckets.\n  Here are some links to help you get a better understand of your Satellite Admin Console and Storj DCS constructs:\nLearn more about Key Architecture Constructs under Concepts.\nLearn more about Project Usage Limits under Concepts.\nLearn more about adding a Payment Method under Billing, Payment \u0026amp; Accounts.\nLearn how usage and billing are calculated under Billing, Payment \u0026amp; Accounts.\n Next we\u0026rsquo;ll learn about creating/deleting buckets, uploading, downloading, viewing the object map, and sharing access to objects through the Object Browser in the Satellite Admin Console.\n"},{"id":82,"href":"/dcs/getting-started/quickstart-uplink-cli/interacting-with-your-first-object/delete-an-object/","title":"Delete an Object","first":"/dcs/","section":"Interacting With Your First Object CLI","content":"Delete an Object #  Delete our object #  Let\u0026rsquo;s delete our photo with the following command:\nWindows ./uplink.exe rm sj://cakes/cheesecake.jpg Linux uplink rm sj://cakes/cheesecake.jpg macOS uplink rm sj://cakes/cheesecake.jpg  Result\n"},{"id":83,"href":"/dcs/concepts/encryption-key/design-decision-server-side-encryption/","title":"Design Decision: Server-side Encryption","first":"/dcs/","section":"Encryption","content":"Design Decision: Server-side Encryption #  Strong encryption is critical to decentralized projects especially where a significant part of the infrastructure is run by independent third-party providers. Encryption is especially important with data storage to deliver security + privacy and ensure developers are in control of their data.\nIt\u0026rsquo;s also important to enable the broadest range of use cases for object storage, and some of those use cases rely on server-side encryption for privacy and security.\nThe Storj DCS hosted S3-compatible gateway service uses server-side encryption, following the industry standard practices for managing access credentials.  When you generate a set of S3-compatible Gateway credentials from an Access Grant, your Access Grant is encrypted using your Access Key. That means that you are passing your decryption information to the Storj-hosted authservice running within GatewayMT. All data and metadata are still encrypted, and that encryption is compatible with the rest of the Storj encryption ecosystem.\nThat means even though your data is encrypted via the hosted gateway, client applications using Uplink CLI, libuplink library, a variety of developer tools including FileZilla and Rclone, and the self-hosted GatewayST can be used to interact with your data using an Access Grant with the same encryption passphrase.\nWhether you create an Access Grant in the Satellite Admin Console, or you use one of the uplink clients, you, and only you have access to your encryption key. Within the Storj DCS encryption ecosystem, all of the tools are interoperable and encryption is easily managed between tools.\nIn terms of interoperability, the Storj DCS S3-compatible gateway is also compatible with Amazon\u0026rsquo;s S3 encryption tools, and if you use the Amazon encryption tools, not only will your object data be encrypted, but also the path and metadata as well. One thing to remember is that if you use the Amazon encryption ecosystem, it will only be decryptable in the way you encrypted it, and the Uplink CLI and other tooling won\u0026rsquo;t be able to do it for you.\nOf course, if end-to-end encryption is important to your use case and you also want the convenience of the hosted gateway service, you can encrypt the data client-side using the encryption mechanism of your choice. In the near future, we plan to release an SDK to extend the Storj DCS encryption ecosystem to include end-to-end encryption with the hosted GatewayMT.\n"},{"id":84,"href":"/dcs/downloads/download-storj-client-libraries/","title":"Download Storj Client Libraries","first":"/dcs/","section":"Downloads","content":"Download Storj Client Libraries #  Creating Your Account Storj Client Libraries "},{"id":85,"href":"/node/resources/faq/check-my-node/","title":"How do I check my node when I'm away from my machine?","first":"/node/","section":"FAQ's","content":"How do I check my node when I\u0026rsquo;m away from my machine? #  In a future update, we\u0026rsquo;ll have e-mail notifications for when your Node goes down. Until then, a great way to stay updated with your Node\u0026rsquo;s status is to use UptimeRobot.\nUptimeRobot will check to see if your port is listening every x minutes. When your port is not listening, you will receive an e-mail alerting you that your port is closed, meaning there\u0026rsquo;s something going on with your Node. To get started, sign up for an account.\nOnce signed up, head over to your UptimeRobot dashboard, and on the top left of the screen, select Add New Monitor\nNext, select port in the Monitor Type dropdown, then enter your Node information\nSelect the e-mail you want the alerts to be sent to, then click Create Monitor\nYou will now receive e-mail alerts when the port closes or opens. You can view more information and history of downtime in the dashboard.\n"},{"id":86,"href":"/node/resources/faq/migrate-my-node/how-to-migrate-the-windows-gui-node-from-a-one-physical-location-to-other/","title":"How to migrate the Windows GUI node from one physical location to another?","first":"/node/","section":"How do I migrate my node to a new device?","content":"How to migrate the Windows GUI node from one physical location to another? #  To be able to move an existing node to a different physical location (different PC) we should transfer both the node\u0026rsquo;s identity and the data. It is neither sufficient to only migrate the identity nor to only move the data to the new location, we need to do both.\nFirst, we need to know where the identity is currently stored on your original machine. If you didn\u0026rsquo;t change the default path, your identity is usually located by default in \u0026quot;%AppData%\\Storj\\Identity\\storagenode\u0026quot;. You can open this path in the Explorer or with cmd.\nWe will assume that you can connect to your node directly via the local network.\nPlease, read documentation about a robocopy command: https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/robocopy``  Both PCs at the local network location #  Please create an identity folder on your new PC with the same name as the identity folder you had on your old PC, and share it to the network.\nFor example, assuming your new PC is called PC2 and the user on it is called user, then you can open cmd on the first PC and execute the command:\nrobocopy \u0026#34;%AppData%\\Storj\\Identity\\storagenode\u0026#34; \\\\pc2\\Users\\user\\AppData\\Roaming\\Storj\\Identity\\storagenode /MIR Using the same approach, you should also transfer the data. We will assume that the storage folder on the second PC is called storage. You can share that folder the same way as you did for the identity.\nPlease run this command while your node is running:\nrobocopy d:\\storagenode \\\\pc2\\storage /MIR Using the same approach, you also transfer the orders folder. By default the orders folder is located in the setup location, i.e. \u0026quot;%ProgramFiles%\\Storj\\Storage Node\\orders\u0026quot;. You need to share this folder on the destination PC in the same way as the previous folders.\nPlease run this command while your node is running:\nrobocopy \u0026#34;%ProgramFiles%\\Storj\\Storage Node\\orders\u0026#34; \\\\pc2\\orders /MIR While these commands are executing, you should make a port forwarding rule on your router for the future storagenode on the second PC, as described in Port Forwarding.\nAfter the above robocopy commands have finished executing for the first time, you should run them several more times until the difference will be negligible. Then you can stop the storagenode service on the first PC in the elevated cmd:\nnet stop storagenode Then run the commands for copying the data and orders one last time.\nNow you should uninstall the storagenode Windows GUI version from your first PC and install it on the new PC following these instructions: GUI Install - Windows, but please skip the steps for receiving the authorization token and generating the identity.\nYou must provide the correct path to the locations of your copied identity and to the copied data during the installation wizard.\nThe network-attached storage location could work, but it is neither supported nor recommended!  Remote location #  There are plenty of options how to transfer the identity and data to the remote location, here are some examples:\n  Storj DCS ftp service file sharing services BitTorrent Resilio sync Team Viewer  Depending on what type of data transfer you have selected, you can transfer the data while your node is running, but you need to sync for a second time after you have shut down the source node to transfer the last-changed pieces (BitTorrent and Resilio Sync).\nUnfortunately, most of these services will require you to first stop the source node while you transfer the data, in which case you should not run it again.\nIf your node would be offline to much (more than 288 hours) it can be suspended and if it would be offline more than 30 days it can be disqualified. So, you should try to bring your node online again as soon as possible.\n"},{"id":87,"href":"/node/dependencies/identity/","title":"Identity","first":"/node/","section":"Dependencies","content":"Identity #  Get an authorization token #  If you haven\u0026rsquo;t already, get an authorization token. This is required to continue.\nDownload the Identity Binary #  Open a terminal window as a usual user (not administrator or root) and paste the command for your OS:\nWindows PowerShell:\n[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12; curl https://github.com/storj/storj/releases/latest/download/identity_windows_amd64.zip -o identity_windows_amd64.zip; Expand-Archive ./identity_windows_amd64.zip . -Force Linux curl -L https://github.com/storj/storj/releases/latest/download/identity_linux_amd64.zip -o identity_linux_amd64.zip unzip -o identity_linux_amd64.zip chmod +x identity sudo mv identity /usr/local/bin/identity ARM-based OS #  Raspberry PI:\ncurl -L https://github.com/storj/storj/releases/latest/download/identity_linux_arm.zip -o identity_linux_arm.zip unzip -o identity_linux_arm.zip chmod +x identity sudo mv identity /usr/local/bin/identity Devices Capable of the AARCH64 Instruction Set:\ncurl -L https://github.com/storj/storj/releases/latest/download/identity_linux_arm64.zip -o identity_linux_arm64.zip unzip -o identity_linux_arm64.zip chmod +x identity sudo mv identity /usr/local/bin/identity pre-M1 macOS curl -L https://github.com/storj/storj/releases/latest/download/identity_darwin_amd64.zip -o identity_darwin_amd64.zip unzip -o identity_darwin_amd64.zip chmod +x identity sudo mv identity /usr/local/bin/identity M1 macOS curl -L https://github.com/storj/storj/releases/latest/download/identity_darwin_arm64.zip -o identity_darwin_arm64.zip unzip -o identity_darwin_arm64.zip chmod +x identity sudo mv identity /usr/local/bin/identity  Create an identity #  This can take several hours or days, depending on your machines processing power and luck.\nPlan to run your Node on a NAS, Raspberry Pi or similar? Create your identity on a more powerful machine and transfer it over.\n Windows PowerShell:\n./identity.exe create storagenode Command Prompt:\nidentity.exe create storagenode Linux identity create storagenode If you are unable to execute the command, be sure that you set your file permission to executable: chmod +x identity\nall macOS identity create storagenode Not working? Set your file permission to executable.\n 4. This process will continue until it reaches a difficulty of at least 36. On completion, it will look something like this:\nAuthorize the identity #  Copy your single-use authorization token from the Node Hosting signup page:\nAuthorize your Storage Node identity using your single-use authorization token (please, replace the placeholder \u0026lt;email:characterstring\u0026gt; to your actual authorization token):\nWindows PowerShell:\n./identity.exe authorize storagenode \u0026lt;email:characterstring\u0026gt; Command Prompt:\nidentity.exe authorize storagenode \u0026lt;email:characterstring\u0026gt; Linux identity authorize storagenode \u0026lt;email:characterstring\u0026gt; all macOS identity authorize storagenode \u0026lt;email:characterstring\u0026gt;  Confirm the identity #  Run the following command to confirm you have the required identity files:\nWindows PowerShell:\n(sls BEGIN \u0026#34;$env:AppData\\Storj\\Identity\\storagenode\\ca.cert\u0026#34;).count (sls BEGIN \u0026#34;$env:AppData\\Storj\\Identity\\storagenode\\identity.cert\u0026#34;).count Command Prompt:\nfindstr \u0026#34;BEGIN\u0026#34; \u0026#34;%APPDATA%\\Storj\\Identity\\storagenode\\ca.cert\u0026#34; | find /c /v \u0026#34;\u0026#34; findstr \u0026#34;BEGIN\u0026#34; \u0026#34;%APPDATA%\\Storj\\Identity\\storagenode\\identity.cert\u0026#34; | find /c /v \u0026#34;\u0026#34; Linux grep -c BEGIN ~/.local/share/storj/identity/storagenode/ca.cert grep -c BEGIN ~/.local/share/storj/identity/storagenode/identity.cert all macOS grep -c BEGIN ~/Library/Application\\ Support/Storj/identity/storagenode/ca.cert grep -c BEGIN ~/Library/Application\\ Support/Storj/identity/storagenode/identity.cert  The first command should return 2, and the second command should return 3:\nIf your numbers are different, then authorizing the identity was not successful. Please try again.\nMight move your storage node to another machine in the future? Back up your identity folder.\nBackup the identity #  Backup before you continue, it should be quick! 🙏\nThis allows you to restore your Node in case of an unfortunate hardware or OS incident.\n Use an external device and backup your identity folder:\nWindows Your identity folder is located in (PowerShell): start \u0026quot;$Env:APPDATA/Storj/Identity/storagenode\u0026quot;\nIn Command Prompt or Windows Explorer: start \u0026quot;%APPDATA%\\Storj\\Identity\\storagenode\u0026quot;\nLinux Your identity folder is located in:\n~/.local/share/storj/identity/storagenode\nOn Raspberry Pi, your identity folder is located in: /home/pi/.local/share/storj/identity/storagenode\nmacOS Your identity folder is located in: /Users/USER/Library/Application Support/Storj/identity/storagenode Optional: Move the identity to the subfolder in the storage location #  It\u0026rsquo;s not required, but could prevent the storagenode from start, if the mounted disk is inaccessible.\nUnfortunately this trick will not help, if the disk would disappear while the storagenode running.\nNext up, select your installation method: #  CLI Install GUI Install - Windows "},{"id":88,"href":"/dcs/api-reference/uplink-cli/import-command/","title":"import","first":"/dcs/","section":"Uplink CLI","content":"import #  Usage #  Windows ./uplink.exe import [flags] NAME (ACCESS | FILE) Linux uplink import [flags] NAME (ACCESS | FILE) macOS uplink import [flags] NAME (ACCESS | FILE)  Flags #     Flag Description     --help, -h help for import   --force, -f overwrite the existing access grant    Examples #  Import access grant from a file #  Windows ./uplink.exe import cheesecake cheesecake.access Linux uplink import cheesecake cheesecake.access macOS uplink import cheesecake cheesecake.access  Import access grant with a key #  Windows ./uplink.exe import cheesecake 13df....qa Linux uplink import cheesecake 13df....qa macOS uplink import cheesecake 13df....qa  These two commands will have the same output:\n"},{"id":89,"href":"/dcs/getting-started/quickstart-uplink-cli/sharing-your-first-object/import-access/","title":"Import an Access to an Object","first":"/dcs/","section":"Sharing Your First Object","content":"Import an Access to an Object #  Importing an access is done using the import command.\nImport from the file #  Windows ./uplink.exe access import cheesecake cheesecake.access Linux uplink access import cheesecake ~/cheesecake.access macOS uplink access import cheesecake ~/cheesecake.access  This should give you the following output:\nImport from the input #  Windows ./uplink.exe access import cheesecake 14dfgh....qr Linux uplink access import cheesecake 14dfgh....qr macOS uplink access import cheesecake 14dfgh....qr  Check list of Access grants #  You can list your available accesses using:\nWindows ./uplink.exe access list Linux uplink access list macOS uplink access list  CURRENT NAME SATELLITE * cheesecake us1.storj.io:7777 pumpkin-pie us1.storj.io:7777 tarte us1.storj.io:7777 To get more information on an access use the inspect command:\nWindows ./uplink.exe access inspect cheesecake Linux uplink access inspect cheesecake macOS uplink access inspect cheesecake  { \u0026#34;satellite_addr\u0026#34;: \u0026#34;12EayRS2V1kEsWESU9QMRseFhdxYxKicsiFmxrsLZHeLUtdps3S@us1.storj.io:7777\u0026#34;, \u0026#34;encryption_access\u0026#34;: { \u0026#34;default_path_cipher\u0026#34;: \u0026#34;ENC_AESGCM\u0026#34; }, \u0026#34;api_key\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;macaroon\u0026#34;: { \u0026#34;head\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;caveats\u0026#34;: [ { \u0026#34;not_after\u0026#34;: \u0026#34;2021-04-17T00:00:00Z\u0026#34;, \u0026#34;not_before\u0026#34;: \u0026#34;2021-04-18T00:00:00Z\u0026#34;, \u0026#34;nonce\u0026#34;: \u0026#34;...\u0026#34; } ], \u0026#34;tail\u0026#34;: \u0026#34;...\u0026#34; } } There is no command to delete an access. You can delete an access directly in your configuration file.  How to use an Access grant with commands #  You can now use this access setting the --access flag. For example, to copy the shared object to your current directory you would use:\nWindows ./uplink.exe --access cheesecake cp sj://cakes/cheesecake.jpg . Linux uplink --access cheesecake cp sj://cakes/cheesecake.jpg . macOS uplink --access cheesecake cp sj://cakes/cheesecake.jpg .  "},{"id":90,"href":"/node/before-you-begin/important-security-consideration/","title":"Important Security Consideration","first":"/node/","section":"Before You Begin","content":"Important Security Consideration #  Do not connect your computer directly to the internet without the assistance of a firewall. #  Make sure you\u0026rsquo;re connected to the Internet through a router and not through a modem without a firewall.  Our software serves requests from the Internet, but not all software you may have installed is designed to be exposed to the Internet directly. This is especially true for users on Windows with applications responding to requests on all IPs.\n"},{"id":91,"href":"/dcs/getting-started/quickstart-uplink-cli/interacting-with-your-first-object/","title":"Interacting With Your First Object CLI","first":"/dcs/","section":"Quickstart - Uplink CLI","content":"Interacting With Your First Object CLI #  Prerequisites List an Object Download an Object Delete an Object "},{"id":92,"href":"/dcs/concepts/key-architecture-constructs/","title":"Key Architecture Constructs","first":"/dcs/","section":"Concepts","content":"Key Architecture Constructs #  At a high level, object storage is a well-understood technology with established vendors and standards for integration. Storj DCS brings a number of new capabilities to make it easy for developers to build more secure and private applications and, as with any new and disruptive technology, the key to maximizing the potential value of that technology is understanding how that technology can be practically applied to solve real-world problems.\nThis section will orient you to some of the main constructs within the service and describes how to use Storj DCS in your application.\nBut first, a word on information architecture\u0026hellip; #  It\u0026rsquo;s important to understand the constructs of Storj DCS so that an application storing data is optimized depending on the requirements for privacy, access control, sharing, etc. The main constructs to understand are shown in the figure below:\nSatellite #  The Satellite is a set of hosted services that is responsible for a range of functions on the network, including the node discovery system, node address information caching, per-object metadata storage, storage node reputation management, billing data aggregation, storage node payment, data audit, and repair, as well as user account and authorization management.\nKey Point: You\u0026rsquo;ll create an account on a Satellite. We have them all over the world. You choose a Satellite based on where your data will be most frequently accessed, as Satellites are where metadata is stored and node selection takes place.  Read more about Satellites.\nDeveloper Account #  When you create an account on a Satellite, you add some basic contact information, including a payment method if you want to use the Paid Tier Service. You can create Projects and Access Grants/Gateway Credentials, view invoices, and track usage.\nKey Point: You can invite other developers with accounts on your Satellite to join one or more of your Projects, or be added to other Developers\u0026rsquo; Projects.  Read more about the Satellite Admin Console.\nProject #  A project is the basic unit for aggregating usage, calculating billing, invoicing fees, collecting payment, and handling access management. Users can create multiple projects and projects are invoiced separately. Within a project, usage is tracked at the Bucket level and aggregated for invoicing to the Project. Project names are not client-side encrypted so that they may be rendered in the Satellite user interface. There are two main drivers for creating multiple Projects: access management and billing.\nAccess Management #  For access management, Access Grants are instantiated at the project level. A primary Access Grant created in the Satellite admin console can perform any action on any bucket in a project, but Access Grants do not work across projects. If you are a managed service provider or you have multiple applications where it is important that there is no commonality between applications for access management (no single Access Grant can be used to manage data across applications) then you should create a separate project per application or customer.\nProjects are also useful for managing phases of software development across environments. You may want to use a separate project for development, staging, and production environments.\nKey Point: The key distinction is that you can create granular Access Grants within a Project with restricted access to only a single object or path, it\u0026rsquo;s also possible to create an Access Grant with all access to all buckets, paths, and objects within a Project. It is not possible to create an Access Grant with access to buckets, paths, and objects within more than one Project.  Billing #  From a billing perspective, if you only have one application, or you’re an individual using an app like FileZilla or Duplicati, you probably only need one project. If you are a managed service provider or systems integrator, and you have multiple applications or want separate invoices for each of the applications, customers, or environments you have, depending on what is relevant to your business, you’ll want to create multiple projects. Usage is itemized within projects at the bucket level, but projects have separate invoices.\nBucket #  A bucket is an unbounded but named collection of objects identified by paths. Every object has a unique path within a bucket.\nKey Point: Bucket names are not client-side encrypted so that they may be rendered in the Satellite user interface.  Bucket names are not client-side encrypted so that they may be rendered in the Satellite user interface.\nUsage is tracked and itemized on invoices at the Bucket level. One practical consideration when choosing how to map constructs in your application to constructs on Storj DCS is that large numbers of buckets will increase the size and complexity of your invoices.\nFrom an access management perspective, there’s really no difference between using a bucket and using a top-level path within a bucket. The only differences are that bucket names are unencrypted and appear on invoices while top-level path names are encrypted and billing is aggregated at the bucket level.\nIf you are an individual user with a single application, you may only need one Bucket. If you are building a multi-tenant or multi-user application, the best practice is to use a single bucket for the application, then create a separate top-level path to store object data associated with each tenant or user within the application. Each top-level path can be secured with a separate Restricted Access Grant.\nWith this structure, your application can manage the data for all of the tenants or users of your application, but 100% of the objects and object metadata will be encrypted, and the tenants or users of your application will not have any access to the data of their peers unless they specifically authorize that access.\nObject Key or Path (encrypted metadata) #  An object key (or path) is a unique identifier for an object within a bucket. An object key is an arbitrary string of bytes. Object keys resemble file system paths by containing forward slashes at access control boundaries. Forward slashes (referred to as the path separator) separate path components. An example path might be videos/carlsagan/gloriousdawn.mp4, where the path components are videos, carlsagan, and gloriousdawn.mp4. It is possible to share just the encryption keys for objects that have a common object key path component prefix.\nWhile many object storage platforms provide access management restrictions only at the bucket level, Storj DCS provides the tools to manage granular access at the path level.  Object (encrypted metadata) #  An object is the main data type in our system. An object is referred to by an object key, contains an arbitrary amount of bytes, and has no minimum or maximum size. An object is represented by an ordered collection of one or more segments. Segments have a fixed maximum size. An object also supports a limited amount of key/value user-defined fields called user-defined metadata.\nAccess Grant #  Storj DCS uses hierarchically deterministic Access Grants as an access management layer for objects. An Access Grant is a security envelope that contains a satellite address, a restricted API Key, and a restricted path component prefix-based encryption key—everything an application needs to locate an object on the network, access that object, and decrypt it. The key benefit of this approach is that these Access Grants and any associated restrictions can be entirely managed client-side, without a central Access Control List or other server-side mechanism involved in the access management process. We call this delegated authorization.\nThe most important thing to understand about Access Grants is, even though an Access Grant contains a serialized encryption key encapsulated in the Access Grant, that encryption key is NEVER passed to a Satellite. Storj DCS Uplink clients separate the API Key from the Access Grant and only pass the API Key to Satellites. That way, a Satellite can receive an access request, evaluate the validity of the API Key associated with that request, and respond with the metadata needed to retrieve the pieces of the object associated with that request without having any ability to decrypt the underlying data or metadata or have any information about the context of the user or application making the request.\nThe result is that the Storj DCS service allows you to create more private and secure applications. Below is a brief description of the three components within an Access Grant:\nSatellite Address #  The Satellite Address is contained within the Access Grant so that an Uplink client knows which Satellite to contact to retrieve the metadata associated with an object to be retrieved.\nAPI Key #  An Access Grant contains an API Key that is based on a type of bearer token called a Macaroon. API Keys are both hierarchically derived from a parent and may also also contain programmatic restrictions based on:\n Specific Operations: Caveats can restrict whether an Access Grant can permit any of the following operations: Read, Write, Delete, List Bucket: Caveats can restrict whether an Access Grant can permit operations on one or more Buckets. Object key and path prefix: Caveats can restrict whether an Access Grant can permit operations on Objects that share a common path component prefix. Time Window: Caveats can restrict when an Access Grant can permit operations on objects stored on the platform.  Restrictions applied to an API Key within an Access Grant are hierarchically derived from the access restrictions contained in the API Key within the Parent Access Grant from which the Restricted Access Grant was created. That means a Restricted Access Grant can have the same level of access or less access than its Parent Access Grant, but never more access.\nEncryption Key #  All data stored on Storj DCS is encrypted. By using hierarchically-derived encryption keys, it becomes easy to share the ability to decrypt a single object or set of objects without sharing the private encryption passphrase or having to re-encrypt objects. When you create a Primary Access Grant, you provide an encryption passphrase. All Restricted Access Grants derived from that Primary Access Grant contain a path-based hierarchically derived serialized encryption key (or set of encryption keys, based on what has been shared).\nIf you\u0026rsquo;re interested in learning more details, please read more about Encryption at your leisure.\nA unique encryption key can be derived client-side for each object. That unique key is generated automatically when sharing objects, allowing users to share single objects or object key prefixes, with the ability to decrypt just the objects that are shared, without having to worry about separately managing encryption access to objects that aren’t being shared.\nBut, your keys are your data. If you lose the encryption passphrase, you effectively lose the ability to recover your data. Satellites never have access to your encryption keys. Satellites can’t access your data so your privacy is ensured, but if you lose the key, that means we also can’t help you recover it. The point: don’t lose your encryption key.\nPrimary Access Grants #  Primary Access Grants are generated via the Satellite Admin Console. Note that only the API Key and Satellite address are generated by the Satellite. The actual Access Grant is created in the browser with the Encryption Passphrase provided by you. The Satellite does not store or retain the encryption passphrase or serialized encryption key encapsulated in the Access Grant. That is entirely handled in client-side Javascript in the browser. A Primary Access Grant can be created with no restrictions or can be created with one or more caveats.\nA Primary Access Grant can then be imported into an Uplink Client for use. The Uplink Client can then be used to create Restricted Access Grants derived from the Primary Access Key.\nRestricted Access Grants #  The Storj DCS service generates primary Access Grants, and restricted Access Grants are derived from a primary Access Grant or any other Access Grant via the Uplink Client. Parent-to-child, access may be further restricted, but not expanded. A restricted Access Grant can never have more access than its parent.\nA primary Access Grant is created in the admin console of the Satellite. A primary Access Grant has all permissions to all buckets within a project and can be used to create a child Access Grant.\nA restricted Access Grant may be created using the Satellite console or Uplink client. Restricted Access Grants are created in the context of creating an access. Essentially, you don’t explicitly create a Restricted Access Grant, the Uplink client creates one when you generate an access, which handles both access management and encryption, both restricted to the scope of access being shared.\nThe Storj DCS service also supports the revocation of Access Grants. Note that revoking an Access Grant adds that Access Grant to a revocation list and invalidates the Access Grant and any child Access Grant derived from the Access Grant that has been revoked. Revoking a primary access grant can be done in the UI, but currently revoking a restricted access grant can only happen via the Uplink CLI.\nSharing Access with Restricted Access Grants #  Sharing access to objects stored on Storj DCS requires sending encryption and authorization information about that object from one client to another. The information is sent in a construct called a Restricted Access Grant. As noted above, an Access Grant is a security envelope that contains a satellite address, a restricted API Key, and a restricted path-based encryption key—everything an application needs to locate an object on the network, access that object, and decrypt it.\nAn Access contains a bearer token that is generated client-side and transmitted client-to-client. When the Uplink client generates or uses an Access, only the API Key (the bearer token) from the Access is sent to a Satellite to retrieve an object. The encryption key is retained by the Client and used to decrypt objects, metadata, and paths client-side.\nTo make the implementation of these constructs as easy as possible for developers, the Storj DCS developer tools abstract the complexity of encoding objects for access management and encryption/decryption. A simple share command encapsulates the satellite address for an object’s metadata, an encryption key, and an API Key into an Access in the format of an encoded string that can be easily imported into an Uplink client. Imported Accesses are managed client-side and may be leveraged in applications via the Uplink client library.\nApplying the Storj DCS Information Architecture #  Once you understand the basic building blocks of Storj DCS, it’s pretty easy to build some fairly sophisticated privacy and security controls into your application.\n"},{"id":93,"href":"/dcs/getting-started/gateway-mt/","title":"Quickstart - AWS CLI and Hosted Gateway MT","first":"/dcs/","section":"Getting Started","content":"Quickstart - AWS CLI and Hosted Gateway MT #  Storj now offers a hosted multitenant gateway (Gateway MT) that is backward compatible with S3. This means you’ll be able to integrate with the Storj network via HTTP, and you won’t have to run anything extra on your end.\nBy using hosted Gateway MT you are opting in to server-side encryption.  Using Gateway MT with AWS CLI is a 2-step process:\n  Generate Credentials to the Gateway MT  Configure AWS CLI with your credentials  Generate Credentials to the Gateway MT #  Navigate to the Access page within your project and then click on Create Access Grant +. A modal window will pop up where you should enter a name for this access grant.\nAssign the permissions you want this access grant to have, then click on Continue in Browser:\nEnter the Encryption Passphrase you used for your other access grants. If this is your first access grant, we strongly encourage you to use a mnemonic phrase as your encryption passphrase (The GUI automatically generates one on the client-side for you.)\nClick on the Generate S3 Gateway Credentials link and then click on the Generate Credentials button.\nCopy your Access Key, Secret Key, and Endpoint to a safe location.\nNow you are ready to configure AWS CLI.\nConfigure AWS CLI with your credentials #  To continue make sure you have the AWS CLI installed on your machine.  Verify your AWS CLI version by running aws --versionin your terminal. AWS CLI current version is version 2. If you are using AWS CLI v1, you will need to install a plugin to be able to define the endpoint. See how here.\n2. Configure your AWS CLI with the gateway MT credentials from the previous step by running aws configure in your terminal:\n~ % aws configure AWS Access Key ID [e53q]: \u0026lt;\u0026lt;yourAccessKey\u0026gt;\u0026gt; AWS Secret Access Key [bbxq]: \u0026lt;\u0026lt;yourSecretKey\u0026gt;\u0026gt; Default region name [us-east-1]: Default output format [None]: ~ % 3. Optional but strongly recommended: Set the multipart threshold to 64 MB.\nYou can now use AWS CLI. Some examples of use:\nMake a bucket #  ~ % aws --endpoint-url=https://gateway.eu1.storjshare.io s3 mb s3://waterbear Make sure to adjust the endpoint URL to the one you have been given when creating your credentials.  Display buckets #  aws --endpoint-url=https://gateway.eu1.storjshare.io s3 ls Copy a file #  aws --endpoint-url=https://gateway.eu1.storjshare.io s3 cp /tmp/test.zip s3://waterbear List files in a bucket #  aws --endpoint-url=https://gateway.eu1.storjshare.io s3 ls s3://waterbear Copy a file from a bucket #  aws --endpoint-url=https://gateway.eu1.storjshare.io s3 cp s3://waterbear/test.zip /tmp/Archive.zip "},{"id":94,"href":"/dcs/billing-payment-and-accounts-1/requesting-a-refund/","title":"Requesting a Refund","first":"/dcs/","section":"Billing, Payment \u0026 Accounts","content":"Requesting a Refund #  We want all of our users to receive value when they choose the Storj DCS Platform for their storage needs. While we strive to provide world-class service it is possible that a customer may request a refund. Refunds will only be granted based on the circumstances set forth in the Terms of Use and Terms of Service. Coupons and Credits are not refundable under any circumstances. Prepayments made via STORJ token transactions are nonrefundable unless an account is being closed.\\\n"},{"id":95,"href":"/dcs/api-reference/s3-gateway/","title":"Self-hosted S3 Compatible Gateway","first":"/dcs/","section":"SDK \u0026 Reference","content":"Self-hosted S3 Compatible Gateway #  A download can become a chargeable event for 2 times the actual file size if the gateway is running on another cloud provider. We recommend interfacing with the network directly through the Uplink Library or using our hosted Gateway MT.  For a complete list of the supported architectures and API calls for the S3 Gateway, see Concepts: S3 Compatibility.\nMinimum Requirements #  ✅ 1 CPU\n✅ 2GB of RAM\nDepending on the load and throughput, more resources may be required.\nTo save on costs and improve performance, please see this important note on multipart upload part sizes.  Dependencies #  ✅ Storj DCS Satellite Account\n✅ Storj DCS Satellite token or Access grant\nSteps: #    Get and install Gateway ST  Configure Gateway ST  Run Gateway ST  Configure AWS CLI to use Gateway ST  Try it out!  Get and install Gateway ST #  Download, unzip, and install the binary for your OS:\nWindows Curl Download (PowerShell) #  curl https://github.com/storj/gateway-st/releases/latest/download/gateway_windows_amd64.exe.zip -o gateway_windows_amd64.exe.zip; Expand-Archive gateway_windows_amd64.exe.zip -Destination . -Force Direct Download #   Windows Gateway Binary\nLinux AMD64 #  Curl Download #  curl -L https://github.com/storj/gateway-st/releases/latest/download/gateway_linux_amd64.zip -O \u0026amp;\u0026amp; unzip gateway_linux_amd64.zip chmod 755 gateway sudo mv gateway /usr/local/bin/gateway Direct Download #   Linux AMD64 Gateway Binary\nARM #  Curl Download #  curl -L https://github.com/storj/gateway-st/releases/latest/download/gateway_linux_arm.zip -O \u0026amp;\u0026amp; unzip gateway_linux_arm.zip chmod 755 gateway sudo mv gateway /usr/local/bin/gateway Direct Download #   Linux ARM Gateway Binary\nmacOS Curl Download #  curl -L https://github.com/storj/gateway-st/releases/latest/download/gateway_darwin_amd64.zip -O \u0026amp;\u0026amp; unzip gateway_darwin_amd64.zip chmod 755 gateway sudo mv gateway /usr/local/bin/gateway Direct Download #   macOS Gateway Binary\nDocker docker pull storjlabs/gateway  Configure Gateway ST #  You have two ways to configure your Gateway ST:\n  Interactive Setup (only if it is your first setup)  Using an Access Grant  Interactive Setup #  1. Setup your S3 gateway by running the following command and following the instructions provided by the wizard:\nWindows PowerShell #   Navigate to the directory your gateway.exe file is located in.\n./gateway.exe setup Linux gateway setup macOS gateway setup Docker docker run -it --rm --mount type=bind,source=/path/to/gateway-config-dir/,destination=/root/.local/share/storj/gateway/ --name gateway storjlabs/gateway setup  2. Enter the numeric choice or satellite address corresponding to the satellite you\u0026rsquo;ve created your account on.\nThe satellite address should be entered as \u0026lt;nodeid\u0026gt;@\u0026lt;address\u0026gt;:\u0026lt;port\u0026gt; for example: 12L9ZFwhzVpuEKMUNUqkaTLGzwY9G24tbiigLiXpmZWKwmcNDDs@eu1.storj.io:7777, or just use the number from the list:\n3. Choose an access name (this step may not yet be implemented in the version of S3 Gateway you are using - if you don\u0026rsquo;t see this prompt, skip to step 5 below).\nIf you would like to choose your own access name, please be sure to only use lowercase letters. Including any uppercase letters will result in your access name not getting recognized when creating buckets.  4. Enter the API Key you generated:\n5. Create and confirm an encryption passphrase, which is used to encrypt your files before they are uploaded:\nPlease note that Storj Labs does not know or store your encryption passphrase, so if you lose it, you will not be able to recover your files.  6. Your S3 gateway is configured and ready to use!\nUsing an Access Grant #  You can use two methods to obtain an Access Grant:\n  Obtain Access Grant with Uplink CLI  Obtain Access Grant with a Satellite UI  Now we got our access grant and can configure the gateway as follows:\nWindows ./gateway setup --access 14aV.... --non-interactive Linux gateway setup --access 14aV.... --non-interactive macOS gateway setup --access 14aV.... --non-interactive Docker docker run -it --rm --mount type=bind,source=/path/to/gateway-config-dir/,destination=/root/.local/share/storj/gateway/ --name gateway storjlabs/gateway setup --access 14aV.... --non-interactive  This command will register the provided access as the default access in the gateway config file.\nIt is possible to have several access grants, see how here.\nRun Gateway ST #  The gateway functions as a daemon. Start it and leave it running.\nWindows ./gateway.exe run Linux gateway run macOS gateway run Docker docker run -it --rm -p 127.0.0.1:7777:7777 --mount type=bind,source=/path/to/gateway-config-dir/,destination=/root/.local/share/storj/gateway/ --name gateway storjlabs/gateway run If you want to connect to your S3 Gateway via the network, then you should replace the -p 127.0.0.1:7777:7777 port mapping with -p 7777:7777   The gateway should output your S3-compatible endpoint, access key, and secret key.\nIf you are interested in more running options, checkout the Gateway ST Advanced Usage page where you can find how to:\n  Run Gateway ST with an Access Grant  Run Gateway ST to host a static website  Run Gateway ST to host a static website with cache  Configure AWS CLI to use Gateway ST #  Please make sure you have AWS S3 CLI installed.  Once you do, in a new terminal session, configure it with your Gateway\u0026rsquo;s credentials:\n$ aws configure --- AWS Access Key ID: [Enter your Gateway\u0026#39;s Access Key] AWS Secret Access Key: [Enter your Gateway\u0026#39;s Secret Key] Default region name: [null] Default output format: [null] $ aws configure set default.s3.multipart_threshold 64MB Then, test out some AWS S3 CLI commands!\nSee also AWS CLI Advanced Options  Try it out! #  Create a bucket #  AWS CLI aws s3 --endpoint=http://localhost:7777/ mb s3://bucket-name  Upload an object #  AWS CLI aws s3 --endpoint=http://localhost:7777/ cp ~/Desktop/your-large-file.mp4 s3://bucket-name  List objects in a bucket #  AWS CLI aws s3 --endpoint=http://localhost:7777 ls s3://bucket-name/  Download an object #  AWS CLI aws s3 --endpoint=http://localhost:7777 cp s3://bucket-name/your-large-file.mp4 ~/Desktop/your-large-file.mp4  Generate a URL for an object #  AWS CLI aws s3 --endpoint=http://localhost:7777 presign s3://bucket-name/your-large-file.mp4  (This URL will allow live video streaming through your browser or VLC)\nDelete an object #  AWS CLI aws s3 --endpoint=http://localhost:7777 rm s3://bucket-name/your-large-file.mp4  All Commands #   cp - Copies a local file or S3 object to another location locally or in S3\n ls - List S3 objects and common prefixes under a prefix or all S3 buckets\n mb - Creates an S3 bucket\n mv - Moves a local file or S3 object to another location locally or in S3.\n presign - Generate a pre-signed URL for an S3 object. This allows anyone who receives the pre-signed URL to retrieve the S3 object with an HTTP GET request.\n rb - Deletes an empty S3 bucket\n rm - Deletes an S3 object\n sync - Syncs directories and S3 prefixes. Recursively copies new and updated files from the source directory to the destination. Only creates folders in the destination if they contain one or more files\nAnd that\u0026rsquo;s it! You\u0026rsquo;ve learned how to use our S3-compatible Gateway. Ideally, you\u0026rsquo;ll see how easy it is to swap out AWS for the Uplink, going forward.\n"},{"id":96,"href":"/node/setup/cli/software-updates/","title":"Software Updates","first":"/node/","section":"CLI Install","content":"Software Updates #  Automatic Updates #  You can set up automatic updates for your storagenode Docker container using watchtower. Watchtower will look for new updates to the Docker container on Docker Hub in a random interval between 12 and 72 hours and automatically update your storage node when it sees a new version.\nFirst, please pull the latest watchtower image from docker:\ndocker pull storjlabs/watchtower To set up auto-update for storagenode, please run the following command once:\ndocker run -d --restart=always --name watchtower -v /var/run/docker.sock:/var/run/docker.sock storjlabs/watchtower storagenode watchtower --stop-timeout 300s If you want to double check that watchtower is properly running, you can run the following command\ndocker ps -a You should see the \u0026ldquo;storjlabs/watchtower:latest\u0026rdquo; container with the uptime under the \u0026ldquo;status\u0026rdquo; column\nManual Updates #  Please use manual updates only if the automatic update method was unsuccessful, or is unavailable for your node configuration.\n1. Stop the running Storage Node container:\ndocker stop -t 300 storagenode 2. Remove the existing container:\ndocker rm storagenode 3. Pull the latest image from docker:\ndocker pull storjlabs/storagenode:latest 4. Start your storage node again by running the following command after editing WALLET, EMAIL, ADDRESS, STORAGE, \u0026lt;identity-dir\u0026gt;, and \u0026lt;storage-dir\u0026gt;: Running the Storage Node\nPrevious versions of thedocker run storagenodecommand that used the -v rather than the --mount option will not work properly. Copy the updated command below.  "},{"id":97,"href":"/dcs/api-reference/s3-compatible-gateway/supported-s3-commands/","title":"Supported S3 Commands","first":"/dcs/","section":"Storj-hosted S3 Compatible Gateway","content":"Supported S3 Commands #  The Storj DCS S3-compatible Gateway supports a RESTful API that is compatible with the basic data access model of the Amazon S3 API.\nThe Storj DCS S3 Gateway is well suited for many application architectures, but since the S3 standard was designed for centralized storage, there are a few areas where a decentralized architecture requires a different approach:\n   API Features Status (support/not supported) Remarks     List Buckets Supported    ListObjects Supported Object listing does not return a traditionally sorted list because keys are stored encrypted. However, the list order is deterministic.   ListObjectsV2 Supported    Delete Bucket Supported ​   Create Bucket Supported ​Note: Bucket names must follow the naming convention for the S3 standard. The S3 standard also requires a region be specified, but we ignore this attribute as data is globally distributed as opposed to stored in a single, regional data center.   Bucket Lifecycle Not Supported ​We support a per-object lifecycle throughout. Configuring an object\u0026rsquo;s \u0026ldquo;time-to-live\u0026rdquo; is done at upload time.   Policy (Buckets, Objects) Not Supported ​   Bucket Website Not Supported Bucket Website is not supported, but it is easy to host a static website using the Storj DCS Gateway   Bucket ACLs (Get, Put) Not Supported This part of the API is not supported, but granular access controls may be encoded into your S3 credentials generated in the Satellite Admin Console at the bucket, path prefix, action permission (read, write, list, delete, and date/time)   Bucket Location Not Supported ​Error handling related to this function is addressed in a basic implementation.   Bucket Notification Not Supported ​   Bucket Object Versions Not Supported ​   Get Bucket Info (HEAD) Supported ​   Bucket Request Payment Not Supported ​We do not have requester pays functionality   Put Object Supported    Delete Object Supported ​   Get Object Supported ​   Object ACLs (Get, Put) Not Supported Supported through restricted access grant only, configuration needed on the gateway   Get Object Info (HEAD) Supported ​Content   Copy Object Not Supported    Object Tagging Not Supported    Multipart Uploads Supported    Create Multipart Upload Supported    Upload Part Supported    Upload Part (Copy) Not Supported    Complete Multipart Upload Supported    Abort Multipart Upload Supported    List Parts Supported    List Multipart Uploads Supported     Since all data is encrypted, it\u0026rsquo;s important to be deliberate about whether and when to use multiple encryption keys.\n"},{"id":98,"href":"/dcs/billing-payment-and-accounts-1/pricing/usage-limit-increases/","title":"Usage Limit Increases","first":"/dcs/","section":"Billing, Payment and Accounts","content":"Usage Limit Increases #  If the default Project Limits do not appear to be appropriate for your use case, you may request a Project Limit Increase form. Increases in Project Limits may result in increased costs associated with your usage of Storj DCS.\nFree Plan #  The Free Tier is not eligible for Project Limit Increases.\nObject Storage #  When you request an increase to the Object Storage Project Limit, there is no additional fee beyond the cost for any incremental Object Storage utilized on Storj DCS.\nEgress Bandwidth #  When you request an increase to the Egress Bandwidth Project Limit, there is no additional fee beyond the cost for any incremental Egress Bandwidth utilized on Storj DCS.\nProject, Bucket, and API Rate Limits #  When you request an increase to the Project, Bucket, and API Rate Limits, there are no additional fees at this time. Before requesting Rate Limit Increases for Projects or Buckets, please review the Key Architecture Constructs section of this Documentation under Concepts.\nPer Segment Fee #  When you request an increase to the Per Segment Project Limit, you may be charged a Per Segment Fee for all Segments over the Segment Project Limit. For most users and most usage patterns, we do not expect a Per Segment Fee to be charged. Only when large numbers of segments are stored relative to a disproportionately small amount of data do we expect there to be a Per Segment Fee. Only use cases with large numbers of very small files or large numbers of very small Multipart Upload Parts are expected to be subject to the Per Segment Fee.\nDistributed object storage is optimized for large files (several MB or larger in size - the larger the better). Very small objects generate more overhead due to storage of the metadata for the file. This matters more than the actual size of the object stored when it comes to overhead. Consequently, we charge a nominal Per Segment Fee to account for that overhead. If a user is storing even large numbers of big files, the per segment fee will be inconsequential. If a user streams millions of small files, or configures an application to use Multipart Upload with a small part size, the Per Segment Fee will offset the cost associated with the greater metadata overhead and may significantly increase the overall fees charged.\nData stored on Storj DCS ordinarily does not incur any additional fees other than fees for Static Object Storage and Download Bandwidth. If you receive an increase in your Segment Project Limit, a Per Segment Fee will be applied to data stored on Storj DCS for all Segments above the default Segment Limit. This section describes how the Per Segment Fee is charged. The Per Segment fee is $0.0000088 Per Segment per Month.\nPer Segment Fee Calculation #  Each Segment stored on the network in excess of the default Segment Project Limit is charged a nominal Per Segment fee. The Per Segment Fee is $0.0000088 Per Segment Per Month. Distributed object storage is optimized for large files (several MB or larger - the larger the better). Very small objects generate more overhead with the storage of the metadata for the file than the actual storage of the object. Consequently, we charge a small Per Segment Fee to account for that overhead. If a user is storing even large numbers of big files, the Per Segment Fee will be inconsequential. If a user streams millions of small files, the Per Segment Fee will offset the cost associated with the metadata overhead.\nThe Storj DCS Platform distinguishes between two types of object storage: remote and inline. Remote objects are large enough to erasure code and store the pieces of the file on storage nodes. Inline objects are smaller than the metadata associated with the actual object. In the case of objects that are smaller than the associated metadata, it is more efficient to store the object inline in the satellite metadata database. When storing a large number of tiny files, the best practice is to employ a packing strategy to store larger blocks of small files as a single object.\nThe Per Segment Fee is priced per Segment per month in increments of Segment hours. The calculation of per segment fees is based on a standard 720-hour month. Actual Per Segment Fees are metered in Segments uploaded. Per Segment Fee hours are calculated based on the number of hours Segments are stored on the Storj DCS Platform from when a Segment is uploaded to when it is deleted. The number of hours each Segment is stored on the platform during the month then is multiplied by the Segment hour price. The segment hour price is derived from the Segment month price divided by the standard 720-hour month.\nThe Paid Tier includes 50,000 Segments per month, which is represented as 36,000,000 Segment Hours. Any increase in Segment Limit will be billed at a rate of $0.0000088 per Segment per month, equivalent to a rate of $0.00000001222 per Segment Hour\nAs described elsewhere in this documentation, objects stored on Storj DCS are encrypted and erasure coded, with the encrypted, erasure coded pieces stored on various Storage Nodes on the distributed and decentralized network. Each object stored on the network is represented as at least one Segment.\nA Segment is a single array of bytes, between 0 and a user-configurable maximum segment size. The default Segment size on Storj DCS Satellites is 64MB. A File smaller than 64MB is stored as one segment. Files larger than 64MB are stored in multiple 64MB Segments. Each Segment is stored as 80 pieces on the network. Only 29 Pieces of the 80 are required to reconstitute a Segment. All Segments are required to reconstitute a File.\nExamples:\n A single 1MB file is one segment A single 10MB file is 1 Segment A single 64MB file is 1 Segment A single 256Mb file is four 64MB Segments A single 300MB file is 5 Segments (four 64MB Segments, and one 44MB Segment) A single 1GB file is sixteen 64MB Segments  Important: Sixteen 1MB files use the same amount of metadata resources as a 1GB file.\nThe Cost examples below assume a Segment Limit Increase has been applied to a project and the Segment Project Limit is sufficient to provide for the usage as described. There is no charge for the first 50,000 Segments per month.\nCost Example 1:\nA user uploads 100,000 one GB files, a total of 100TB. Half way through the month, the user deletes the files. The 100,000 files are stored as 1,600,000 Segments. The files are stored for 360 hours. The files are stored for 576,000,000 Segment hours. The default 36,000,000 Segment Hours for the Paid Tier are subtracted, leaving 540,000,000 Segment Hours. The price per Segment month is $0.0000088. The price per Segment hour at 720 hours per month is $0.00000001222. The total amount charged for the monthly Per Segment Fee would be $5.40.\nCost Example 2:\nA user uploads 1,600,000 one MB files, a total of 1.6TB. Half way through the month, the user deletes the files. The 1,600,000 files are stored as 1,600,000 Segments. The files are stored for 360 hours. The files are stored for 576,000,000 Segment hours. The default 36,000,000 Segment Hours for the Paid Tier are subtracted, leaving 540,000,000 Segment Hours. The price per Segment month is $0.0000088. The price per Segment hour at 720 hours per month is $0.00000001222. The total amount charged for the per segment fee would be $5.40.\nNote that the number of segments for 100TB of 1GB files and 1.6TB of 1MB files is the same.\nMultipart Upload Impact on Segments #  When an object is uploaded using Multipart Upload, a file is first broken into parts, each part of a Multipart Upload is also stored as one or more Segments. With Multipart Upload, a single object is uploaded as a set of parts. Each part is an integral portion of the data comprising the object. The object parts may be uploaded independently, in parallel, and in any order. Uploads may be paused and resumed by uploading an initial set of parts, then resuming and uploading the remaining parts. If the upload of any part fails, that part may be re-uploaded without impacting the upload of other parts. All of these parts are broken into one or more Segments by the Storj DCS Gateway based on whether the Part Size is smaller or larger than the default Segment size. While Multipart Upload is most appropriate for files larger than the 64MB default Segment size, the Part Size is configurable in applications that use Multipart Upload.\nExamples:\nA single 128MB file using a 64MB Part Size is uploaded as 2 parts, with each part stored as one 64MB Segment\nA single 128MB file using a 5MB Part Size is uploaded as 26 Parts (25 5MB Parts and one 3MB Part), with each part stored as one Segment, for a total of 26 Segments.\nThe default Part Size for Multipart Upload for some applications is 5MB. The difference between using the default 5MB Part Size instead of the default size of 64MB for Storj DCS is impactful.\nAssuming a 1TB data set comprised of 1,000 1GB files is stored for an entire month, the difference between using 64MB Part Size vs. 5MB Part Size is described below:\n   Part Size Files Parts/ S Segment Hours Chargeable Segment Hours Monthly Cost of Per Segment Fee Monthly cost of storage     64MB 1,000 15,625 11.25M 0M $0.00 $4.00   5MB 1,000 200,000 144.0M 108.0M $1.32 $5.32    Multipart Cost Example 1:\nA user uploads 100,000 one GB objects using multipart upload with a 64MB Part Size and 10,000 100MB objects using a 5MB Part Size. Half way through the month, the user deletes the files. The 100,000 one GB files are stored as 1,562,500 Segments. The 10,000 100MB files are stored as 200,000 Segments. The files are stored for 360 hours. The 100,000 one GB files in 64MB Segments are stored for 562,500,000 Segment hours. The 10,000 100 MB files in 5MB Segments are stored for 72,000,000 Segment hours. The total is 634,500,000 Segment Hours. The default 36,000,000 Segment Hours for the Paid Tier are subtracted, leaving 598,500,000 Segment Hours. The monthly Per Segment fee is $7.31.\nMultipart Cost Example 2:\nA user uploads 1PB of one GB objects using multipart upload with a 5MB Part Size . Half way through the month, the user deletes the files. The 1PB of one GB files (1 million files) are stored as 200,000,000 Segments. The files are stored for 360 hours. The data is stored for 72,000,000,000 Segment hours. The price per Segment month is $0.0000088. The price per Segment hour at 720 hours per month is $0.00000001222. The monthly Per Segment Fee in this example would be $879.40.\n"},{"id":99,"href":"/dcs/concepts/access/","title":"Access Management","first":"/dcs/","section":"Concepts","content":"Access Management #  Distributed and decentralized cloud storage is a fantastic way to take advantage of underutilized storage and bandwidth, but in order to provide highly available and durable cloud storage, we needed to build in some fairly sophisticated security and privacy controls.\nSince we had to build with the assumption that any Peer Class besides the Uplink could be run by an untrusted person, we had to implement a zero-knowledge security architecture. This turns out to not only make our system far more resistant to attacks than traditional architectures but also brings significant benefits to developers building apps on Storj DCS.\nAccess Management Paradigm #  The access management paradigm for Storj DCS is based on a set of security and privacy principles that are incorporated at the code level either as a necessary component of the decentralized architecture or as a feature to enable developers to build more secure and private applications.\n Data and metadata stored on Storj DCS is encrypted and the Satellite never has access to encryption keys Authorization management should be delegated to the edge, but provide easy-to-use tools for granular levels of access control Identity and Access Management for users of applications that store data on Storj DCS should be handled by those applications  These security and privacy principles are ultimately manifested in Storj DCS as a set of tools developers can use for access management, providing granular control over how data is accessed and shared within their applications, on top of a decentralized, distributed system.\nAccess management on Storj DCS requires coordination of two parallel constructs:\n  Authorization - a determination of whether a particular request to perform an action on a resource is valid. Authorization management is implemented using hierarchically deterministic API Keys based on macaroons.\\\n  Encryption - Data and metadata stored on Storj DCS are encrypted using hierarchically deterministic Encryption Keys. Objects are encrypted with a randomized encryption key that is salted with a predetermined salt. Paths and randomized encryption keys are encrypted with a passphrase using AES 256 GCM or Secretbox.\n  Both of these constructs work together to provide an access management framework that is secure and private, as well as extremely flexible for application developers.\nTo make the implementation of these constructs as easy as possible for developers to use, the Storj DCS developer tools abstract the complexity of encoding objects for access management and encryption/decryption.\nUnderstanding how Authorization and Encryption work together is critical to designing an appropriate access management flow for an application  Combining Authorization and encryption Management: Access Grants #  Storj DCS uses hierarchically deterministic Access Grants as an access management layer for objects. An Access Grant is a security envelope that contains a satellite address, a restricted API Key, and a restricted path-based encryption key—everything an application needs to locate an object on the network, access that object, and decrypt it. The key benefit of this approach is that these Access Grants and any associated restrictions can be entirely managed client-side, without a central Access Control List or other server-side mechanism involved in the access management process. We call this delegated authorization.\nRead more about Access Grants.\n"},{"id":100,"href":"/dcs/getting-started/satellite-developer-account/objects/","title":"Buckets","first":"/dcs/","section":"Quickstart - Satellite Admin Console","content":"Buckets #  The Object Browser is a web interface for creating/deleting buckets, uploading, downloading, viewing the object map, and sharing access to objects stored in a Project on Storj DCS.\nThis is such a popular feature, it has its very own Quickstart Guide for the Object Browser.  In the Quickstart Guide for the Object Browser, you\u0026rsquo;ll learn:\n Setting your encryption passphrase How to create and delete Buckets How to upload objects to Buckets, download and delete them. How to create sharable links How to view the Object Map that displays the location of the storage nodes that are storing the encrypted and erasure coded pieces of your file.  Do not lose your encryption passphrase. You are responsible for your encryption keys. Learn more about our approach to encryption and managing your encryption keys.  Next, we\u0026rsquo;ll cover Access section.\n"},{"id":101,"href":"/dcs/concepts/access/capability-based-access-control/","title":"Capability Based Access vs Access Control Lists","first":"/dcs/","section":"Access Management","content":"Capability Based Access vs Access Control Lists #  From a security-design standpoint, the capability model introduces a fundamentally better approach to identity and access management than Public Cloud’s ACL framework.\nBy tying access to keys, rather than a centralized control system, capability-based models push security to the edge, decentralizing the large ACL attack vector and creating a more secure IAM system.\nThe capability-based model solves both the ambient authority trap and the confused deputy problem by design.\nWhat is a capability?\nOften referred to as simply a ‘key,’ a capability is the single thing that both designates a resource and authorizes some kind of access to it. The capability is an unforgeable token of authority.\nThose coming from the Blockchain world will be very familiar with the capability-based security model, as it is the model implemented in Bitcoin where “your key is your money”.\nThis gives the client-user full insight into their privilege set, illustrating the core tenet of the Capability Mindset: “ don’t separate designation from authority.”.\nSimilar to how in the Blockchain world, “your keys are your money,” with Storj DCS, your keys are your data, and access grants add additional capabilities that allow the owners of data to caveat it, or granularly delegate access for sharing, programmatically.\nKey-based ownership of object data will enable users to intuitively control their data as a first principle, and then delegate it as they see fit. The decentralized cloud eliminates the increasingly apparent risk of data loss/extortion due to holding data on one single provider (like Amazon, Google, or Microsoft).\nStorj DCS presents a better model where object data is encrypted, erasure-coded, and spread across thousands of nodes stratified by reputation whereby any and every computer can be the cloud.\n"},{"id":102,"href":"/dcs/billing-payment-and-accounts-1/closing-an-account/","title":"Closing an Account","first":"/dcs/","section":"Billing, Payment \u0026 Accounts","content":"Closing an Account #  We want all of our users to receive value when they choose the Storj DCS Platform for their storage needs, but it’s possible that a user may no longer need Storj DCS services. If a user wants to stop using an account and permanently delete the account, the user may submit a request for this only after following the steps outlined below to eliminate platform usage.\nThe process to eliminate platform usage starts with deleting all data from the platform, including all objects and buckets. Next, all Access Grants should be deleted. Once this is done, the user should submit a support ticket to remove all payment methods and delete the account.\nNote that in order to verify the deletion request is legitimate, the user will need to confirm they control the email address used for the account in question. Therefore, the user should file their support ticket while signed in to their Storj help desk account. If the user does not yet have a help desk account, they should sign up first before filing their account deletion request. Support agents will be happy to remove the help desk account once the Storj DCS account deletion has been completed.\nThe account deletion request ticket should specify the following:\n State that all data has been deleted (Buckets, Objects) State that all Access grants have been deleted Identify the  Satellite Project Name User Account Email Payment Method(s)   Confirm that the outstanding balance is $0.00 State that the account should be deleted  If you uses the Objects browser in the Admin console, then after deleting the last bucket please do not return to the Objects view, otherwise the demo-bucket will be created automatically. This bucket will prevent the account deletion. It also can create a linked Access grant, this will prevent the account deletion too.  Once the ticket is received and the information has been verified, the payment method(s) will be removed before the end of the next billing cycle. Once all payment methods are removed from the account, the account will be deleted per the request. Note: The user will be required to verify the request via the registered email address on the account.\n"},{"id":103,"href":"/node/setup/cli/dashboard/","title":"Dashboard","first":"/node/","section":"CLI Install","content":"Dashboard #  Storage Node Operator Web Dashboard #  If you want to access this dashboard from a device on your local network, in your docker run command, use -p 14002:14002 instead of -p 127.0.0.1:14002:14002\nIf you want to access this dashboard remotely, use this guide: How to remote access the web dashboard\n Open the following URL in your web browser:\nDirectly on your node: #  http://127.0.0.1:14002/ Device on local network: #  http://\u0026lt;your-nodes-local-ip\u0026gt;:14002/ Storage Node Dashboard Concepts #     Concept Description     Satellite Satellites act as the mediator between clients (people up- and downloading data) and Storage Node Operators (people storing data). Satellites facilitate the storage interaction and decide which storage nodes will store which pieces.   Bandwidth Used This Month The amount of total bandwidth you\u0026rsquo;ve provided to the network since the beginning of the current period.   Usage / Repair / Audit Usage bandwidth is the bandwidth a Storage Node uses so customers can download their data, for which a Storage Node Operator is paid $20/TB.\n\nRepair bandwidth is the bandwidth usage resulting from regenerating a bad Storage Node\u0026rsquo;s deleted data that is part of the repair process, for which a Storage Node Operator sending the data to new nodes is paid $10/TB.\n\nAudit bandwidth is the data downloaded from the Storage Node, which the Satellite uses to measure file durability and Node reputation.\n   Egress Ingress\n Egress is the data the customer downloads from the network.\nIngress is the data the network uploads to a Storage Node.\n   Disk Space Used This Month The amount of total disk space used on a storage node in the current monthly period, for which a storage node is paid $1.50/TB.\n   Uptime Checks Uptime checks occur to make sure a Storage Node is still online. This is the percentage of uptime checks a Storage Node has passed.   Audit Checks Audit checks occur to make sure the data sent to a Storage Node is still held on the node and intact. This is the audit score, represented as percentage. With score less than 60% node will be disqualified.    How to remote access the web dashboard CLI Storage Node Dashboard #  Run the following command to monitor the activity of your node with the CLI dashboard:\ndocker exec -it storagenode /app/dashboard.sh The dashboard may not load instantly.\nGive it some time to fully load. Also, it is not necessary to keep the dashboard constantly running. You can exit the dashboard with Ctrl-C and the Storage Node will continue running in the background.\n "},{"id":104,"href":"/dcs/billing-payment-and-accounts-1/storj-token/deleting-a-payment-method/","title":"Deleting a Payment Method","first":"/dcs/","section":"Payment Methods","content":"Deleting a Payment Method #  To use the Storj DCS Paid Tier, a user must have a valid payment method on their account. Users may delete payment methods:\n when substituting for another valid payment method; or when requesting the deprovisioning of the account; or when requesting the deletion of an account  The process for substituting valid payment methods is outlined above. Once a new payment method is added, the old payment method may be removed.\nIf a user wants either to stop using an account but not permanently delete the account, or permanently delete the account, the user may remove all payment methods only after following the steps outlined below.\nThe process to end your platform usage starts with deleting all data from the platform, including all objects and buckets. Next, all Access Grants must be deleted. Once this is done, the user should submit a ticket to remove all payment methods. The ticket should:\n State that all data has been deleted (Buckets, Objects) State that all Access Grants have been deleted Identify the  Satellite Project Name User Account Email Payment Method(s)   Confirm that the outstanding balance is $0.00 Indicate whether the account should be deleted or set to zero usage limits  Once the ticket is received, and we have confirmed there is no outstanding balance owed, the payment method(s) will be removed before the end of the next billing cycle. Once all payment methods are removed from the account, either the account limits will be set to zero or the account will be deleted per the request.\nNote: The user will be required to verify the request via the registered email address on the account.  "},{"id":105,"href":"/dcs/how-tos/fastly-integration/","title":"Fastly Integration","first":"/dcs/","section":"How To's","content":"Fastly Integration #  Prerequisites\nBefore you can connect your buckets to Fastly, you will first need a Fastly account. Once that is set up, follow the instructions below to configure a Fastly service with your buckets as an origin host.\nSetting Up Fastly as an Edge Server #  To make your buckets available through the Fastly edge cloud network, you will configure Storj as your Fastly service\u0026rsquo;s origin server.\nTo do that, you must first create a Fastly service using the following values as you go:\n For the new Fastly domain and host, set the Domain Name field to the hostname you will be using for your URL. For example, cdn.example.com When you get to the Hosts section on the Origins page, enter the address of the Storj gateway endpoint you would like to use for this service. Include the name of your bucket, for example: mybucket.gateway.us1.storjshare.io After adding the host, click the pencil icon to Edit this host and check that the following are correctly filled out:  Name: Any name you would like to use for your Fastly service Address: This should be the address of your gateway endpoint    Additionally, if you have enabled TLS for your gateway and wish to configure that, check the following fields as well:\n Enable TLS: The default for this is Yes SNI hostname: Select Match the SNI hostname to the Certificate hostname. The gateway address you filled out and double-checked above should automatically appear. Certificate hostname: This should be the same IP address or hostname of your gateway.  Note that if you are going to use AWS S3 integration, you may want to leave the Override host field blank when creating your service. Otherwise, it will override important authentication headers read by AWS (more on that below).\nVerifying the service is configured #  Fastly creates a DNS mapping from the domain you entered when creating your service to \u0026lt;domain\u0026gt;.global.prod.fastly.net. So following the above steps, this would be cdn.example.com.global.prod.fastly.net.\nCreate an alias in your own DNS settings for the domain name you are using. For example, create a CNAME record mapping cdn.example.com to global-nossl.fastly.net\nFastly caches all content by default for 1 hour. This can be modified by sending a Cache-Control header. If you are unsure whether you are sending any cache control headers, you can verify with a simple cURL command:\n$ curl -I https://cdn.example.com Accept-Ranges: bytes Content-Length: 250 Content-Type: application/xml Server: MinIO/DEVELOPMENT.GOGET Vary: Origin Date: Wed, 20 Oct 2020 05:56:29 GMT Since there are no Cache-Control headers in this example, the default cache of 1 hour will be applied.\nAdvanced Cache Control #  Fastly also has documentation on how different objects are cached. You can find more information in their cache freshness docs.\nUsing a bucket for origin hosting #  To integrate your S3 compatible gateway as an origin with Fastly, first create an access grant. You will need the Access key and Secret key, as well as your bucket name.\nOnce you have the access grant you will use, enable your Fastly service to support the latest version of Amazon\u0026rsquo;s header-based authentication by creating a custom VCL on Fastly.\nNext create a Fastly VCL snippet. Select the following options for your snippet:\n Type: select within subroutine In the dropdown box, select miss. Then, paste the following code into the VCL box. Update the variables where noted to the values from your access grant, bucket name, and bucket gateway:  declare local var.accessKey STRING; declare local var.secretKey STRING; declare local var.storjBucket STRING; declare local var.storjGateway STRING; declare local var.region STRING; declare local var.canonicalHeaders STRING; declare local var.signedHeaders STRING; declare local var.canonicalRequest STRING; declare local var.canonicalQuery STRING; declare local var.stringToSign STRING; declare local var.dateStamp STRING; declare local var.signature STRING; declare local var.scope STRING; set var.accessKey = \u0026#34;YOUR_ACCESS_KEY\u0026#34;; # Change this value to your own data set var.secretKey = \u0026#34;YOUR_SECRET_KEY\u0026#34;; # Change this value to your own data set var.storjBucket = \u0026#34;YOUR_BUCKET_NAME\u0026#34;; # Change this value to your own data set var.storjGateway = \u0026#34;STORJ-DCS_GATEWAY\u0026#34;; # Change this value to your own data set var.region = \u0026#34;decentralized\u0026#34;; if (req.method == \u0026#34;GET\u0026#34; \u0026amp;\u0026amp; !req.backend.is_shield) { set bereq.http.x-amz-content-sha256 = digest.hash_sha256(\u0026#34;\u0026#34;); set bereq.http.x-amz-date = strftime({\u0026#34;%Y%m%dT%H%M%SZ\u0026#34;}, now); set bereq.http.host = var.storjBucket \u0026#34;.\u0026#34; var.storjGateway; set bereq.url = querystring.remove(bereq.url); set bereq.url = regsuball(urlencode(urldecode(bereq.url.path)), {\u0026#34;%2F\u0026#34;}, \u0026#34;/\u0026#34;); set var.dateStamp = strftime({\u0026#34;%Y%m%d\u0026#34;}, now); set var.canonicalHeaders = \u0026#34;\u0026#34; \u0026#34;host:\u0026#34; bereq.http.host LF \u0026#34;x-amz-content-sha256:\u0026#34; bereq.http.x-amz-content-sha256 LF \u0026#34;x-amz-date:\u0026#34; bereq.http.x-amz-date LF ; set var.canonicalQuery = \u0026#34;\u0026#34;; set var.signedHeaders = \u0026#34;host;x-amz-content-sha256;x-amz-date\u0026#34;; set var.canonicalRequest = \u0026#34;\u0026#34; \u0026#34;GET\u0026#34; LF bereq.url.path LF var.canonicalQuery LF var.canonicalHeaders LF var.signedHeaders LF digest.hash_sha256(\u0026#34;\u0026#34;) ; set var.scope = var.dateStamp \u0026#34;/\u0026#34; var.region \u0026#34;/s3/aws4_request\u0026#34;; set var.stringToSign = \u0026#34;\u0026#34; \u0026#34;AWS4-HMAC-SHA256\u0026#34; LF bereq.http.x-amz-date LF var.scope LF regsub(digest.hash_sha256(var.canonicalRequest),\u0026#34;^0x\u0026#34;, \u0026#34;\u0026#34;) ; set var.signature = digest.awsv4_hmac( var.secretKey, var.dateStamp, var.region, \u0026#34;s3\u0026#34;, var.stringToSign ); set bereq.http.Authorization = \u0026#34;AWS4-HMAC-SHA256 \u0026#34; \u0026#34;Credential=\u0026#34; var.accessKey \u0026#34;/\u0026#34; var.scope \u0026#34;, \u0026#34; \u0026#34;SignedHeaders=\u0026#34; var.signedHeaders \u0026#34;, \u0026#34; \u0026#34;Signature=\u0026#34; + regsub(var.signature,\u0026#34;^0x\u0026#34;, \u0026#34;\u0026#34;) ; unset bereq.http.Accept; unset bereq.http.Accept-Language; unset bereq.http.User-Agent; unset bereq.http.Fastly-Client-IP; } Note that, as mentioned above, if you have an override host specified in your Fastly service settings, that value will cause the http.host header to be overwritten which could be invalid for AWS authentication.\nSee also Storj DCS Object Storage on Fastly.\n"},{"id":106,"href":"/dcs/api-reference/linksharing-service/","title":"Linksharing Service","first":"/dcs/","section":"SDK \u0026 Reference","content":"Linksharing Service #  Overview #  The Linksharing service provides an easy way to share an object via a web browser. Linkshare links may be shared with the following features:\n Storj DCS Linkshare web page - displays a preview of the shared object, including a streaming player for multimedia files along with a map displaying the geolocation of the storage nodes storing the encrypted and erasure coded pieces of the object Path-based linkshare - displays a list of objects with a shared path in a browser. This feature allows sharing a folder of objects. When clicked in a browser, any of the objects will be displayed individually on a Linkshare web page Direct download Linkshare - a URL to directly access and download an object via the internet without loading a web page Restricted Access - Linkshare links are read-only by default, but may be further restricted with any supported access restriction (bucket, path and prefix, time window) Revokable Access - Link share links may be revoked by deleting the associated Access Grant from the Auth Service or by revoking the Access Grant via the revocation service.  The Linkshare service is part of the Storj Edge Services and provides an additional way to access objects over the internet via a browser in addition to the S3-compatible gateway.\nNote: All of the Edge Services, including the Linksharing service use server-side encryption.  Linkshare Examples #  The Storj Linkshare web page and Path-based Linkshare web page are shown below:\nLinkshare QuickStart #  To accelerate your time to success we offer an Object Browser GUI that allows you to upload and share with no command line interface required. Check out the tutorial for the Object Browser.\nThe steps for sharing an object via the GUI are included in that tutorial.\nLinkshare via CLI (Advanced) #  If you prefer a command line interface (CLI) or wish to programmatically integrate to Storj DCS we have our CLI documentation in addition to our Client Libraries. You can also host full static websites via the Linksharing service.\nRegions of availability #     Region CNAME     Asia link.ap1.storjshare.io   EU link.eu1.storjshare.io   US link.us1.storjshare.io    "},{"id":107,"href":"/dcs/api-reference/uplink-cli/ls-command/","title":"ls","first":"/dcs/","section":"Uplink CLI","content":"ls #  Usage #  Windows ./uplink.exe ls [sj://BUCKET[/PREFIX]] [flags] Linux uplink ls [sj://BUCKET[/PREFIX]] [flags] macOS uplink ls [sj://BUCKET[/PREFIX]] [flags]  Flags #     Flag Description     --access string the serialized access, or name of the access to use   --encrypted if true, show paths as base64-encoded encrypted paths   --expanded, -x Use expanded output, showing object expiration times and whether there is custom metadata attached   --help, -h help for ls   --pending if true, list incomplete objects instead   --recursive, -r if true, list recursively    Examples #  We consider the following object hierarchy throughout these examples:\nWe assume the cakes/very-secret-recipe.txt object has been uploaded using a different encryption key than the other objects in the project.\nList buckets #  Windows ./uplink.exe ls Linux uplink ls macOS uplink ls  List objects in a bucket #  Windows ./uplink.exe ls sj://images Linux uplink ls sj://images macOS uplink ls sj://images  List by prefix #  Windows ./uplink.exe ls sj://images/cakes Linux uplink ls sj://images/cakes macOS uplink ls sj://images/cakes  List recursively #  Windows ./uplink.exe ls --recursive Linux uplink ls --recursive macOS uplink ls --recursive  List encrypted paths of all objects in a bucket #  Windows ./uplink.exe ls sj://recipes --encrypted --recursive Linux uplink ls sj://recipes --encrypted macOS uplink ls sj://recipes --encrypted  Notice that since sj://recipes/cakes/very-secret-recipe.txt was encrypted with a different key, we cannot view it using regular ls and the default access, but with --encrypted we can see that it is indeed stored in sj://recipes\n"},{"id":108,"href":"/dcs/getting-started/quickstart-aws-sdk-and-hosted-gateway-mt/","title":"Quickstart - AWS SDK and Hosted Gateway MT","first":"/dcs/","section":"Getting Started","content":"Quickstart - AWS SDK and Hosted Gateway MT #  Storj now offers a hosted multitenant gateway (Gateway MT) that is backward compatible with S3. This means you’ll be able to integrate with the Storj network via HTTP, and you won’t have to run anything extra on your end.\nBy using hosted Gateway MT you are opting-in to server-side encryption.  Using Gateway MT with AWS SDK is a 2-step process:\n  Generate Credentials to the Gateway MT  Configure AWS SDK with your credentials  Generate Credentials to the Gateway MT #  Navigate to the Access page within your project and then click on Create Access Grant +. A modal window will pop up where you can enter a name for this access grant.\nAssign the permissions you want this access grant to have, then click on Continue in Browser:\nEnter the Encryption Passphrase you used for your other access grants. If this is your first access grant, we strongly encourage you to use a mnemonic phrase as your encryption passphrase (The GUI automatically generates one on the client-side for you.)\nClick on the Generate S3 Gateway Credentials link and then click on the Generate Credentials button.\nCopy your Access Key, Secret Key, and Endpoint to a safe location for future use.\nNow you are ready to configure AWS SDK\nGateway MT with Amazon S3 SDK (Node.js) #  1. Install or include the Amazon S3 SDK #  e.g. with npm\nnpm install --save aws-sdk 2. Import the S3 client #  import S3 from \u0026#34;aws-sdk/clients/s3\u0026#34;; 3. Create client object with MT credentials #  const accessKeyId = \u0026#34;access key here\u0026#34;; const secretAccessKey = \u0026#34;secret access key here\u0026#34;; const endpoint = \u0026#34;https://gateway.us1.storjshare.io\u0026#34;;  const s3 = new S3({  accessKeyId,  secretAccessKey,  endpoint,  s3ForcePathStyle: true,  signatureVersion: \u0026#34;v4\u0026#34;,  connectTimeout: 0,  httpOptions: { timeout: 0 } }); 4. List objects and log to console #  (async () =\u0026gt; {   const { Buckets } = await s3.listBuckets({}).promise();   console.log(Buckets);  })(); 5. Upload an object #  (async () =\u0026gt; {   // `file` can be a readable stream in node or a `Blob` in the browser   const params = {  Bucket: \u0026#34;my-bucket\u0026#34;,  Key: \u0026#34;my-object\u0026#34;,  Body: file  };   await s3.upload(params, {  partSize: 64 * 1024 * 1024  }).promise();  })(); 6. Get URL that points to an object #  The getSignedUrl function creates a cryptographically signed url. No contact with the gateway is needed here; this happens instantaneously.\nconst params = {  Bucket: \u0026#34;my-bucket\u0026#34;,  Key: \u0026#34;my-object\u0026#34; }  const url = s3.getSignedUrl(\u0026#34;getObject\u0026#34;, params);  // e.g. create an \u0026lt;img\u0026gt; where src points to url "},{"id":109,"href":"/dcs/getting-started/quickstart-uplink-cli/sharing-your-first-object/revoke-an-access-to-an-object/","title":"Revoke an Access to an Object","first":"/dcs/","section":"Sharing Your First Object","content":"Revoke an Access to an Object #  You can revoke an access grant to an object at any time with the command uplink revoke.\nWindows ./uplink.exe revoke asdfRF... Linux uplink revoke asdfRF... macOS uplink revoke asdfRF...  The access will be revoked permanently for this parent access grant.\nIf you want to share this content again you should create a new access grant through the web interface.\n Revoke a named access grant #  Windows ./uplink.exe revoke --access access-name Linux uplink revoke --access access-name macOS uplink revoke --access access-name  "},{"id":110,"href":"/dcs/getting-started/quickstart-uplink-cli/sharing-your-first-object/","title":"Sharing Your First Object","first":"/dcs/","section":"Quickstart - Uplink CLI","content":"Sharing Your First Object #  Prerequisites Create an Access to an Object Import an Access to an Object Revoke an Access to an Object "},{"id":111,"href":"/node/dependencies/storage-node-operator-payout-information/","title":"Storage Node Operator Payout Information","first":"/node/","section":"Dependencies","content":"Storage Node Operator Payout Information #  Storage Node Operators are compensated for the resources that are used by Storj DCS Satellites for their nodes every month. Our payout policy and details can be found on the Storage Node Operator Terms and Conditions.\nMinimum payment thresholds #  All Storage Node payouts are subject to a per-wallet minimum threshold. We will not send a transaction where the fee for the transaction is more than 25% of the value of the transaction. The minimum threshold is calculated based on the average transaction fee value in USD from the previous 12 hours at the beginning of the payout process. For example, if the average transaction fee is the equivalent of $12.50, we’ll pay out all wallet addresses that have earned $50.00 and above.\nOne of the reasons our terms and conditions require you to share the same wallet address across any storage nodes you operate is to avoid missing the minimum payout threshold and to help you avoid transaction costs.  For wallet addresses that have earned less than the threshold, the earned payout will be included the following month, as long as the aggregate amount of payouts owed meets the minimum threshold at the time payouts are sent. In the example above, all Node Operators that earn less than $50.00 would have their payouts rolled into their payout the following month.\nIf a wallet address has no more active storage nodes associated with it (due to graceful exit, disqualification, etc), funds that did not clear previous thresholds will be dispersed.\nPayment options #  Storage node operators have two options for payment, and these options impact what the transaction fee is.\n Ethereum layer 1 - these are the default transactions, but have much higher fees, and therefore higher minimum payout thresholds ($50 wouldn\u0026rsquo;t be surprising here, depending on current fees). zkSync layer 2 - these are new. You can opt in to them, and the fees are much lower, therefore, there will be much lower minimum payout thresholds ($1 wouldn\u0026rsquo;t be surprising here, depending on current fees).  Both of these options will (for now) happen on a monthly schedule. We are committing to get the prior month\u0026rsquo;s payments out before the 15th of the following month.  If you are running multiple storage nodes, the payment method you select will apply to the individual storage nodes separately. For instance, if you have four nodes, two using default layer 1 transactions, and two using zkSync, then you will receive two payouts, one on layer 1 for those two nodes, and one through zkSync for the zkSync enabled ones. IMPORTANT: These will be considered two separate payouts for purposes of reaching the minimum threshold.  Ethereum layer 1 transactions #  The default behavior is for us to transmit funds using Ethereum layer 1 (standard ERC20 transactions) for our STORJ token. This fee is calculated using the Ethereum gas costs of similar transactions, the gas to ETH conversion prices, and the price of ETH.\nThese transaction fees are typically much higher than layer 2 transactions (see below), and thus incur a much higher minimum payout threshold.\nExample calculation for layer 1 transaction:\nAt a gas price of 274 GWei, with a per-transaction Gas cost of 36508, a transaction costs .01 ETH, which at an ETH price of $1714 is $17.14 per transaction. We will not send a transaction where the fee is more than 25% of the overall transmitted value. That means the minimum payout threshold would be $68.56.\n zkSync layer 2 transactions #  Any node operator running v1.22.2 or later also can opt into zkSync Layer 2 transactions to receive payouts. zkSync is new technology and comes with some additional risk. You can read more about why we\u0026rsquo;ve chosen zkSync here.\nWe will use this type of transaction when possible, but we may revert to layer 1 transactions (and associated minimum payout thresholds) if circumstances require.  The main benefit of zkSync is a much lower L2 transaction fee, and therefore a much lower minimum payout threshold. Low earning wallet addresses will get payouts at a more frequent schedule with zkSync.\nThe main consideration with this method is that if the node operator wants to withdraw their funds from layer 2 back to layer 1 (for an exchange address of an exchange that does not yet support zkSync or similar), they will have to pay a transaction fee for that withdrawal. This withdrawal fee can be paid in STORJ, but may be more than a standard layer ERC20 transfer.\nTransaction fees can be paid using STORJ in zkSync, so no ETH will be needed for zkSync transactions.    You can read about how to opt in to zkSync on this page.  You can read more about zkSync in general here.  General advice #  Always control your private keys to your wallet.  While it may be convenient to use an exchange address for your storage node payout, it\u0026rsquo;s always safest to use an address for which you control the private keys. If you opt to use zkSync, you definitely want to use an address for which you control the private keys. Withdrawing your funds from zkSync is designed for use with wallets for which you control the private keys. If you use an address from an exchange or for which you otherwise don\u0026rsquo;t have the private keys, you will be required to trigger an emergency withdrawal process and this will be significantly more costly for you.\n"},{"id":112,"href":"/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/upload-an-object/","title":"Upload an Object","first":"/dcs/","section":"Uploading Your First Object CLI","content":"Upload an Object #  Check Prerequisites.\nThe Object we\u0026rsquo;ll upload #  Right-click and save as cheesecake.jpg to your Desktop:\nUpload our object #  To upload our photo, let\u0026rsquo;s use the copy command:\nWindows ./uplink.exe cp ~/Desktop/cheesecake.jpg sj://cakes Linux uplink cp ~/Desktop/cheesecake.jpg sj://cakes macOS uplink cp ~/Desktop/cheesecake.jpg sj://cakes  Result\n"},{"id":113,"href":"/node/resources/faq/machine-restart-shutdown/","title":"What if my machine restarts or shuts down?","first":"/node/","section":"FAQ's","content":"What if my machine restarts or shuts down? #  If you have properly mounted your hard drive (if on Linux, using static mount via /etc/fstab), then your Node should start up again automatically after your machine has rebooted.\nHowever, power failures on machines not protected by a UPS or other abrupt disconnections may cause database corruption leading to errors such as: database error: database disk image is malformed or database: file is not a database in your logs).\nIncorrect mounting could lead the Node to not recognize the proper data location where the Node was previously storing the data, resulting in node crash.\nIn case of using the storagenode docker version, it will continuously restarted until the Node Operator would fix the problem.\nIn case of using the Windows GUI, the service will not restart automatically and the Node Operator should fix the problem and restart the service.\nSuch failures should be attended to immediately as they can lead to the node getting disqualified.\n Here are instructions on how to fix a malformed database disk image. If the database is not recognized (\u0026ldquo;file is not a database\u0026rdquo;), recovery of this database will be impossible, and you will need to recreate it.\nIf you need assistance, please ask on our forum.\n"},{"id":114,"href":"/dcs/getting-started/satellite-developer-account/access-grants/","title":"Access","first":"/dcs/","section":"Quickstart - Satellite Admin Console","content":"Access #  An Access Grant is a security envelope that contains a satellite address, a restricted API Key, and a restricted path-based encryption key - everything an application needs to locate an object on the network, access that object, and decrypt it.\nLearn more about Access Management and Access Grants or check out the FAQ on Access Grants and Encryption Keys.  The Access Grant screen allows you to create or delete Access Grants, and generate credentials for the Storj DCS S3-compatible Gateway from an Access Grant.\nLet\u0026rsquo;s start with creating an Access Grant. Click the Create Access Grant Button.\nGive your Access Grant a name:\nSet any access restrictions you want encoded into your Access Grant. Through the Satellite Admin Console, you can set basic restrictions on your Access Grant. You can get more sophisticated using the CLI and add further, more granular restrictions, for example, at the path prefix level within a Bucket.\nNext, enter an Encryption Passphrase for your Access Grant. Note that this encryption passphrase is handled by the browser and is not stored by the Satellite. You can either Generate Phrase or Enter Phrase.\nDo not lose your Encryption Passphrase. Storj DCS does not manage your encryption keys and if you lose your Encryption Passphrase and your Access Grant, you will not be able to decrypt your data.  Copy or download your Access Grant. Do not lose it, you only have one opportunity to do so. If you did not save it, please delete this Access Grant and create a new one and save it on this time.\nThis Access Grant can now be used to configure tools like the Storj DCS Uplink CLI, libuplink library, or apps like Rclone, FileZilla or Restic. You can also generate credentials for the Storj DCS S3-compatible Gateway.\nRemember, when you generate credentials for the Storj DCS S3-compatible Gateway from an Access Grant, you are opting in to server-side encryption.  When you generate credentials for the Storj DCS S3-compatible Gateway, the Admin Console will register your Access Grant with the Gateway auth service and display the credentials required to configure your client app to work with the Storj DCS S3-compatible Gateway.\nTo Delete an Access Grant, select an Access Grant and choose Remove Selected:\nThen confirm that you want to delete the Access Grant.\nImportant: If you delete an Access Grant from the Satellite user interface, that Access Grant will immediately cease to function, and all hierarchically derived child Access Grants and Storj DCS gateway access credentials based on that Access Grant will also cease to function. Any data uploaded with that Access Grant will persist on Storj DCS. If you didn\u0026rsquo;t back up the Encryption Passphrase used with the Access Grant you are deleting, you will not be able to decrypt that data without that Encryption Passphrase, and it will be effectively unrecoverable.  You don\u0026rsquo;t need to know everything in the whitepaper about our Access Grants, macaroon-based API Keys or our encryption implementation, but if you understand the general principles, you\u0026rsquo;ll find these are some very sophisticated (but easy to use) tools for creating more secure and private applications.\nNext we\u0026rsquo;ll cover adding and removing other developers to and from your project.\n"},{"id":115,"href":"/dcs/concepts/access/access-management-at-the-edge/","title":"Access Management at the Edge","first":"/dcs/","section":"Access Management","content":"Access Management at the Edge #  One of the areas where we’re seeing the strongest interest from developers, customers and partners building apps is our security model and access control layer. The security and privacy capabilities of the platform are some of the most differentiating features and they give our partners and customers some exciting new tools.\nDistributed and decentralized cloud storage is a fantastic way to take advantage of underutilized storage and bandwidth, but in order to provide highly available and durable cloud storage, we needed to build in some fairly sophisticated security and privacy controls. Because we had to build with the assumption that any Node could be run by an untrusted person, we had to implement a zero-knowledge security architecture. This turns out to not only make our system far more resistant to attacks than traditional architectures, but also brings significant benefits to developers building apps on the platform.\nDecentralized Architecture Requires Strong Privacy and Security #  From the network perspective, we need to make sure the data stored on our platform remains private and secure. At the most basic level, we need to ensure that pieces of files stored on untrusted Nodes can’t be compromised, either by accessing that data or preventing access to that data. We combine several different technologies to achieve data privacy, security and availability.\nFrom the client side, we use a combination of end-to-end encryption, erasure coding, and API Keys. Erasure coding is primarily used to ensure data availability, although storing data across thousands of diverse Storage Nodes does add a layer of security by eliminating any centralized honeypot of data.\nBy way of example, when a file or segment is erasure coded, it is divided into 80 pieces, of which any 29 can be used to reconstitute the (encrypted) file. With our zero-knowledge architecture, any Node Operator only gets one of the 80 pieces. There is nothing in the anonymized metadata to indicate what segment that piece belongs to, or where the other 80 pieces are etc. It’s worth noting that 80 pieces is the minimum number of pieces for a single object. Files larger than 64MB are broken up into 64 MB segments, each of which is further divided up into 80 pieces. A 1GB file for example is broken up into 16 segments, each with a different randomized encryption key, and each broken up into 80 pieces, for a total of 1,280 pieces.\nIf a hacker wants to obtain a complete object, they need to find at least 29 Nodes that hold a piece of each segment, compromise the security of each one (with each Node being run by different people, on different Nodes, using different firewalls, etc.). Even then, they would only have enough to reconstitute a file that is still encrypted. And, they’ll have to repeat that process for the next segment. Compare that to a situation (e.g. what was seen at Equifax a few years ago), where a simple misconfiguration gave access to hundreds of millions of individuals’ data, and you’ll see the power of this new model.\nJust storing data on Storj DCS provides significant improvements over centralized data storage in terms of reducing threat surfaces and exposure to a variety of common attack vectors. But when it comes to sharing access to data—especially highly sensitive data—developers really experience the advantages of our platform. Where we’re already seeing the most interest from partners on the combination of end-to-end encryption and the access management capabilities of our API Keys.\nSeparating Access and Encryption #  One of the great things about Storj DCS is that it separates the encryption function from the access management capabilities of the API Keys, allowing both to be managed 100% client-side. From a developer perspective, managing those two constructs is easy because all of the complexity is abstracted down to a few simple commands. What this enables developers to do is move access management from a centralized server to the edge.\nHierarchically Deterministic End-to-End Encryption #  All data stored on Storj DCS is encrypted from the client side. What that means is users control the encryption keys and the result is an extremely private and secure data store. Both the objects and the associated metadata are encrypted using randomized, salted, path-based encryption keys. The randomized keys are then encrypted with keys derived from the user’s encryption passphrase. Neither Storj nor any Storage Nodes have access to those keys, the data, or the metadata.\nBy using hierarchically derived encryption keys, it becomes easy to share the ability to decrypt a single object or set of objects without sharing the private encryption passphrase or having to re-encrypt objects. Unlike the HD API Keys, where the hierarchy is derived from further restrictions of access, the path prefix structure of the object storage hierarchy is the foundation of the encryption structure.\nA unique encryption key can be derived client-side for each object whether it’s a path or file. That unique key is generated automatically when sharing objects, allowing users to share single objects or paths, with the ability to encrypt just the objects that are shared, without having to worry about separately managing encryption access to objects that aren’t being shared.\nAccess Management with API Keys #  In addition to providing the tools to share the ability to decrypt objects, Storj DCS also provides sophisticated tools for managing access to objects. Storj DCS uses hierarchically deterministic API Keys as an access management layer for objects. Similar to HD encryption keys, HD API Keys are derived from a parent API Key.\nUnlike the HD encryption keys where the hierarchy is derived from the path prefix structure of the object storage hierarchy, the hierarchy of API Keys is derived from the structure and relationship of access restrictions. HD API Keys embed the logic for the access it allows and can be restricted, simply by embedding the path restrictions and any additional restrictions within the string that represents the API Key. Unlike a typical API key, the Storj DCS API Key is not a random string of bytes, but rather an envelope with access logic encoded in it.\nBringing it Together with the Access #  Access management on Storj DCS requires coordination of the two parallel constructs described above—encryption and authorization. Both of these constructs work together to provide an access management framework that is secure and private, as well as extremely flexible for application developers. Both encryption and delegation of authorization are managed client-side.\nWhile both of these constructs are managed client-side, it’s important to point out that only the API Keys are sent to the Satellite. The Satellite interprets the restrictions set by the client in the form of caveats, then controls what operations are allowed based on those restrictions. Encryption keys are never sent to the Satellite.\nSharing access to objects stored on Storj DCS requires sending encryption and authorization information about that object from one client to another. The information is sent in a construct called an Access. An Access is a security envelope that contains a restricted HD API Key and an HD encryption key—everything an application needs to locate an object on the network, access that object, and decrypt it.\nTo make the implementation of these constructs as easy as possible for developers, the Storj DCS developer tools abstract the complexity of encoding objects for access management and encryption/decryption. A simple share command encapsulates both an encryption key and an API Key into an Access in the format of an encoded string that can be easily imported into an Uplink client. Imported Accesses are managed client-side and may be leveraged in applications via the Uplink client library.\nWhy Security at the Edge Matters #  The evolution of cloud services and the transition of many services from on-premise to centralized cloud has massive increases in efficiency and economies of scale. That efficiency in many ways is driven by a concentration not only of technology, but expertise, and especially security expertise. That efficiency has also come at the cost of tradeoffs between security and privacy. Moreover, many new business models have emerged based almost entirely on the exchange of convenience for giving up the privacy of user data. In the cloud economy, user’s most private data is now more at risk than ever, and for the companies that store that data, new regulatory regimes have emerged, increasing the impact on those businesses if that data is compromised.\nThe Intersection of Cybersecurity Skill and Decentralized Data #  While the transition of on-premise to cloud has brought a reduction in the number and types of hacks, much of the vulnerability of on-premise technology was due in part to a lack of cybersecurity experience and expertise. A big part of the push to Gmail is the fact that it’s much less likely to get hacked than a privately operated mail server.\nThe transition to the cloud has resulted in a much greater separation of security expertise and technology use. The cost of best-in-class security expertise of cloud providers is, like the cost of infrastructure, spread across all customers. One additional consequence of that separation—the loss of cybersecurity expertise—is the lack of appreciation of the resulting tradeoff. That security does not come with transparency, and in fact, many times that security comes in exchange for a loss of privacy.\nThis is where a decentralized edge-based security model provides a similar security advantage but without the tradeoffs against transparency or privacy. With Storj, you get the benefit of the team’s distributed storage, encryption, security, and privacy expertise but you also get the full transparency of the open-source software. This ultimately enables the ability not only to trust but to verify the security of the platform, but that’s not where the difference ends. Storj provides all the security benefits of a cloud storage service, but provides the tools to take back control over your privacy.\nEdge-based Security + Decentralized Architecture = Privacy by Default #  Classic authorization technologies are built for client-server architectures. Web-centric authorization schemes such as OAuth and JWT are built for largely synchronous transactions that involve separating the resource owner and the authorization service. Each of these approaches depends on its success on a central authority. To truly maximize privacy and security at a massive scale, there is a need to efficiently delegate resource authorization away from centralized parties.\nMoving token generation and delegation closer to the edge of the architecture represents a fundamental shift in the way technologists can create verified trust systems. Having the ability in a distributed system to centrally initiate trust (via API Keys) and extrapolate specifically scoped keys from that trust allows systems to generate their own trust chains that can be easily managed for specific roles and responsibilities. Authorization delegation is managed at the edge but derived based on a common, transparent trust framework. This means that API Keys (access tokens) generated at the edge can be efficiently interpreted centrally, but without access to the underlying encrypted data.\nDistributed and decentralized environments are designed to eliminate trust by definition. By moving security, privacy, and access management to the edge, users regain control over their data. With tools such as client-side encryption, cryptographic audits and completely open-source architecture, trust boundaries and risk are mitigated not by the service provider, but by the tools in the hands of the user.\nA Different Approach Delivers Differentiated Value Out-of-the-box #  Storj DCS’s distributed cloud storage and edge-based security model provide easy tools for building applications that are more private, more secure, and less susceptible to the range of common attacks. With this approach, no incompetent or malicious operator can undermine security. By embracing decentralization and security at the edge, the system is architected to be resilient. Unlike other cloud storage providers, like the AWS Detective solution, Storj DCS integrates security features which are enabled by default. With Storj DCS, you don’t pay extra for security and privacy.\nReduced Risk - Common attacks (misconfigured access control lists, leaky buckets, insider threats, honeypots, man-in-the-middle attacks, etc.) depend for their success on breaching a central repository of access controls or gaining access to a treasure trove of data. The Storj DCS security model provides a way to architect out whole categories of typical application attack vectors.\nReduced Threat Surface - By separating trust boundaries and distributing access management and storage functions, a significant percentage of the typical application threat surfaces is either eliminated or made orders of magnitude more complex to attack.\nEnhanced Privacy - With access managed peer-to-peer, the platform provides the tools to separate responsibilities for creating bearer tokens for access management from encryption for use of the data. Separation of these concerns enables decoupling storage, access management and use of data, ensuring greater privacy with greater transparency.\nPurpose-Built for Distributed Data #  Distributed data storage architecture combined with edge-based encryption and access management stores your data as if it were encrypted sand stored on an encrypted beach. The combination of client-side HD Encryption keys and HD API Keys in an easy-to-use platform enables application developers to leverage the capability-based security model to build applications that provide superior privacy and security.\n"},{"id":116,"href":"/dcs/getting-started/quickstart-uplink-cli/generate-access-grants-and-tokens/","title":"Advanced Usage","first":"/dcs/","section":"Quickstart - Uplink CLI","content":"Advanced Usage #  Prerequisites Create Access Grant in CLI "},{"id":117,"href":"/dcs/how-tos/backup-with-duplicati/","title":"Backup With Duplicati","first":"/dcs/","section":"How To's","content":"Backup With Duplicati #  Overview #  Duplicati is a backup tool. It can group, dedupe, and compress small files into bigger blocks. It is a great tool for reducing the costs of cold storage. It also supports versioning.\nTo restore a small file, Duplicati has to download the entire block it is contained in. Therefore, the best fit for Duplicati are the following two backup use cases: 1. when none of the files contained in the same block need to be ever restored again in the future. 2. when all files in a block need to be restored at the same time.\nInstall #  Please note that the version used for writing this documentation is currently not yet released on the Duplicati homepage. Please download the canary version from https://github.com/duplicati/duplicati/releases or use the canary docker container.    Download and install the Duplicati installer file for your OS or run the docker container. Note warning above! Once installed, the software will open your browser to the local Duplicati dashboard. If not, it can be accessed at http://localhost:8200/  Setup #   On the Duplicati dashboard, click \u0026ldquo;Add Backup\u0026rdquo; and select \u0026ldquo;Configure a new backup\u0026rdquo;  2. Enter a name for this backup. For this example, we\u0026rsquo;ll call it \u0026ldquo;My Backup Job.\u0026rdquo; The Storj DCS plugin will encrypt all files before they get uploaded. We don\u0026rsquo;t want to encrypt each file twice. Disable the Duplicati encryption.\n3. Click the storage type dropdown and select \u0026ldquo;Tardigrade Decentralized Cloud Storage.\u0026rdquo; Select a satellite, then enter an API Key ( Access token), encryption passphrase, bucket and optionally, a folder. You can generate a new API Key at any time but please don\u0026rsquo;t lose the encryption key - keep a backup in a safe place.\n4. Next, press \u0026ldquo;Test Connection\u0026rdquo;\n5. On the next page, we will select the folder we want to backup. For this example, we will use \u0026ldquo;My Drive\u0026rdquo;\n6. Now we will create a schedule. In this example, we will run the backup once every day at 1:00 PM.\n7. On the next page, select the appropriate options for you.\nRecommended Options #     Option Description     asynchronous-concurrent-upload-limit=1 By default, Duplicati will transfer 4 files in parallel in order to speed up the transfer. The Storj protocol splits every file upload into many small pieces and uploads them in parallel. Even with only 1 concurrent upload it should max out most consumer connections.   backup-test-samples=0 The Storj protocol checks the hash at the end of every file upload. An additional test sample is not needed. Use list-verify-uploads instead.   list-verify-uploads=true If a file upload fails for any reason, a final listing would catch it.   no-auto-compact=true If a large number of small files are detected during a backup, or wasted space is found after deleting backups, the remote data will be compacted. This will cause a lot of unnecessary and expensive download traffic.   threshold As files are changed, some data stored at the remote destination may not be required. This option controls how much wasted space the destination can contain before being reclaimed. Depending on the use case, the threshold can be reduced to 0. Storj DCS doesn\u0026rsquo;t charge you for the additional delete and re-upload operations.   zip-compression-method This option can be used to set an alternative compression method, such as LZMA.    8. Click \u0026ldquo;Save\u0026rdquo; and you should see the \u0026ldquo;My Backup Job\u0026rdquo; we created on the Duplicati homepage.\n9. You can select \u0026ldquo;Run now\u0026rdquo; if you would like to do a backup instantly.\nCongrats, you\u0026rsquo;ve set up Duplicati Backup to Storj DCS! 🎉\n"},{"id":118,"href":"/dcs/billing-payment-and-accounts-1/data-retention-policy/","title":"Data Retention Policy","first":"/dcs/","section":"Billing, Payment \u0026 Accounts","content":"Data Retention Policy #  All unpaid balances must be paid via a valid payment method in a billing cycle. Note that if no valid payment method remains on an account, usage limits may be reduced to zero. If all credit cards expire or are invalid, or the STORJ token balance is exhausted and no new payment method is added on a project, after a reasonable time and opportunity to add a payment method, we reserve the right to reclaim the available resources (the storage space and bandwidth made available by Storage Node Operators to the Storj network) and delete the data from Storj DCS if a valid payment method is not added and any unpaid fees are not paid.\n"},{"id":119,"href":"/dcs/concepts/edge-services/","title":"Edge Services","first":"/dcs/","section":"Concepts","content":"Edge Services #  Overview #  Storj Edge Services are hosted components that provide S3 compatibility and publicly-available data sharing services and are composed of the Storj Gateway MT, the auth service and the link sharing service. Storj Edge Services use server-side encryption.\nNote: All of the Edge Services use server-side encryption.  Edge Services #  The three components comprising the edge services are:\nS3 Compatible Gateway\nStorj DCS includes a globally distributed, multi-region cloud-hosted S3-compatible gateway.\nStorj-hosted S3 Compatible Gateway Linkshare Service\nThe Storj DCS Linkshare service is a globally distributed, multi-region cloud-hosted gateway for standard HTTP requests, for sharing objects with users via a web browser.\nLinksharing Service Auth Service\nAuth Service Both the Storj S3-compatible gateway and the Linkshare service leverage the Auth Service for access management via a registered Access Grant.\\\n"},{"id":120,"href":"/dcs/billing-payment-and-accounts-1/storj-token/expired-credit-card/","title":"Expired Credit Card","first":"/dcs/","section":"Payment Methods","content":"Expired Credit Card #  If the default credit card on an account expires, an email will be sent to the registered email address on the account notifying the owner of the account that the credit card has expired. If a secondary credit card has been added to the account, the secondary card will be charged for any usage fees, and service will not be interrupted. If a secondary card is not added and the user did not enable a STORJ payment method, a valid payment method must be added to the account.\nAll unpaid balances must be paid via a valid payment method in a billing cycle. Note that if no valid payment method remains on an account, usage limits may be reduced to zero. Data stored on an account with no valid payment method is subject to the data retention policy described below.\\\n"},{"id":121,"href":"/node/resources/faq/system-maintenance/","title":"How do I shutdown my node for system maintenance?","first":"/node/","section":"FAQ's","content":"How do I shutdown my node for system maintenance? #  If you need to shutdown the Storage Node for maintenance on your system, run:\nCLI Install docker stop -t 300 storagenode GUI Windows Install Elevated PowerShell:\nStop-Service storagenode Or click the \u0026ldquo;Stop\u0026rdquo; button in the Windows Services applet on \u0026ldquo;Storj V3 Storage Node\u0026rdquo; service\n After you finished your maintenance, restart the Node with:\nCLI Install docker start storagenode GUI Windows Install Elevated PowerShell:\nStart-Service storagenode Or click the \u0026ldquo;Start\u0026rdquo; button in the Windows Services applet on \u0026ldquo;Storj V3 Storage Node\u0026rdquo; service.\n "},{"id":122,"href":"/dcs/api-reference/uplink-cli/uplink-mb-command/","title":"mb","first":"/dcs/","section":"Uplink CLI","content":"mb #  Usage #  Windows ./uplink.exe mb [flags] sj://\u0026lt;BUCKET\u0026gt; Linux uplink mb [flags] sj://\u0026lt;BUCKET\u0026gt; macOS uplink mb [flags] sj://\u0026lt;BUCKET\u0026gt;  Flags #     Flag Description     --access string the serialized access, or name of the access to use   --help, -h help for mb    Examples #  Create bucket #  Windows ./uplink.exe mb sj://cakes Linux uplink mb sj://cakes macOS uplink mb sj://cakes  Nested buckets are not supported.  Output:\n"},{"id":123,"href":"/dcs/getting-started/quickstart-uplink-cli/","title":"Quickstart - Uplink CLI","first":"/dcs/","section":"Getting Started","content":"Quickstart - Uplink CLI #  Learn how to use the Storj DCS CLI to store, retrieve and interact with objects on Storj DCS.\nThe Uplink CLI uses end-to-end encryption for your object data, including metadata and path data.  Prerequisites Uploading Your First Object CLI Interacting With Your First Object CLI Advanced Usage "},{"id":124,"href":"/dcs/api-reference/storj-client-libraries/","title":"Storj Client Libraries","first":"/dcs/","section":"SDK \u0026 Reference","content":"Storj Client Libraries #  Getting Started #  The libuplink developer library is written for the Go language. This will allow you to integrate with the object store programmatically.\nlibuplink contains a number of interesting components, including pre-written code and subroutines, classes, values or type specifications, message templates, configuration walkthroughs, and great documentation.\nStorj DCS currently has community contributed bindings for Python, Swift, .Net, PHP, and Node.js.\nBelow are Storj\u0026rsquo;s provided bindings:\n  GO  C  Android  Java  NodeJS  Please also take a look on third party libraries and tools: https://github.com/storj-thirdparty\n"},{"id":125,"href":"/dcs/getting-started/quickstart-uplink-cli/uploading-your-first-object/view-distribution-of-an-object/","title":"View Distribution of an Object","first":"/dcs/","section":"Uploading Your First Object CLI","content":"View Distribution of an Object #  Check Prerequisites.\nYou can view the geographic distribution of your object and generate a shareable URL via the Link Sharing Service. Run the uplink share --url command below.\nSee here for specifications on how to select an auth region and restrict the uplink share --url command.\nWindows ./uplink.exe share --url --not-after=+2h sj://cakes/cheesecake.jpg Linux uplink share --url --not-after=+2h sj://cakes/cheesecake.jpg macOS uplink share --url --not-after=+2h sj://cakes/cheesecake.jpg  Copy the URL that is returned by the uplink share --url command and paste into your browser window.\n=========== ACCESS RESTRICTIONS ========================================================== Download : Allowed Upload : Disallowed Lists : Allowed Deletes : Disallowed NotBefore : No restriction NotAfter : 2022-03-01 09:56:13 Paths : sj://cakes/cheesecake.jpg =========== SERIALIZED ACCESS WITH THE ABOVE RESTRICTIONS TO SHARE WITH OTHERS =========== Access : 1Dv4... ========== CREDENTIALS =================================================================== Access Key ID: jvw3fmzqyg2cvxm27qishw6y4qka Secret Key : ... Endpoint : https://gateway.us1.storjshare.io Public Access: true =========== BROWSER URL ================================================================== REMINDER : Object key must end in \u0026#39;/\u0026#39; when trying to share recursively URL : https://link.us1.storjshare.io/s/juexo54k2db7lt5fawuqkupqkcfa/cakes/cheesecake.jpg This is a real distribution of your file\u0026rsquo;s pieces that you uploaded to the network. You can share this file with anyone you\u0026rsquo;d like.\n"},{"id":126,"href":"/dcs/concepts/connectors/","title":"Connectors","first":"/dcs/","section":"Concepts","content":"Connectors #  Building Storj DCS Connectors #  Connectors bridge the gap between the applications we use every day and the underlying storage infrastructure on which application data is stored. Our team has worked with our partners to build a series of connectors between Storj DCS, our decentralized cloud storage service, and our partners’ applications.\nWhat is a Storj DCS Connector #  A Storj DCS connector is an application-level integration that enables the use of decentralized cloud storage in consumer software. Storj DCS connectors enable Satellites to measure the volume of data associated with a bucket and give attribution to them.\nPut simply, a Connector enables application developers to store, retrieve, and manage data on the decentralized cloud on behalf of the app users.\nStorj DCS connectors are different from standard libuplink integrations, as they provide application developers and open source projects a means to monetize their cloud usage programmatically on the Storj network.\nSo, using Storj DCS with your favorite open source project can help you lower your cloud storage costs, while also financially supporting the project itself.\nHow to build Connectors #  The Storj Connector Framework is a basic set of utility methods and operations to provide a consistent approach to integrating and orchestrating among data sources, endpoints, and the Storj DCS network. Some of the aspects addressed in the framework are:\n Buffering/resource management Abstraction Data transformation Configuration Authentication Logging  Generally, a Storj DCS connector will interface directly with libuplink, our native Go library, which enables you to programmatically interface with Storj. A basic architectural diagram of how a Storj Connector fits into the stack is located below:\nGet Started - and Monetize OSS! #  We hope that the Storj Connector Partner Program will empower a new class of entrepreneurs to ‘be the cloud’, and create more sustainable business models built on top of open-source software.\\\n"},{"id":127,"href":"/dcs/api-reference/uplink-cli/meta-command/","title":"meta","first":"/dcs/","section":"Uplink CLI","content":"meta #  Usage #  Windows ./uplink.exe meta [command] Linux uplink meta [command] macOS uplink meta [command]  Child commands #     Command Description     [`get`](meta-get-command.md) Get a Storj object\u0026rsquo;s metadata    Flags #     Flag Description     --help, -h help for meta    "},{"id":128,"href":"/dcs/how-tos/nft-storage/","title":"NFT storage for OpenSea","first":"/dcs/","section":"How To's","content":"NFT storage for OpenSea #  Storj DCS makes it easy for NFT (Non-Fungible Token) creators and developers to store their digital art, music, and videos on the decentralized cloud via Storj DCS.\nGetting Started with NFT Storage on Storj DCS #  In this tutorial, we’re going to cover three main steps:\n Upload your Digital Asset to Storj DCS Create a public LinkShare link to your digital asset Register your NFT with the link to the asset  Before we get started, here’s some background information on NFTs for context. Or skip the background and take me to the tutorial!\nBackground #  This tutorial covers a couple of approaches for developers and creators minting Non-Fungible Tokens (NFTs) for digital assets, to store and serve those digital assets from Storj DCS, the leading decentralized cloud storage provider. In this example, we’re going to use OpenSea for the NFT registration. In principle, the same method could also be applied to storing NFTs issued on any of the other available NFT minting platforms.\nRight now, any OpenSea developer can use the coupon code OPENSEA100 for $100 in STORJ Credit. This would be useful to any developer who signs up to use Storj DCS with the OpenSea SDK and wants to grow beyond our free 50GB tier. This promotion is available to the first 100 developers to register the coupon code on any Storj DCS Satellite account, and is good for two billing cycles. The coupon code is redeemable until December 31st, 2021.\nNFTs enable developers and creators to register ownership of a unique digital asset on the blockchain. NFTs are best understood as providing digitally native ownership of images, videos, or PFPs (Profile Pictures), or can be digital representations of asset ownership for real estate etc.\nAn NFT is just a registration of ownership - for digital objects, owners need a secure, decentralized method to store the associated jpg, mp4, or other files the NFT is to be associated with. Storj DCS provides secure, private storage on the decentralized cloud that is ideal for NFTs:\n Fully encrypted data and metadata for digital assets Multiple options for developing web or mobile apps for storing and sharing digital assets Ultra-secure and private file sharing options Simple public sharing URLs with revocable access Native support for media streaming  What is OpenSea? #  OpenSea is an open marketplace and developer toolkit for NFTs. NFTs are blockchain tokens associated with a cryptographic keypair to represent ownership of unique items.\nWhat are NFTs? #  NFT contracts such as ERC721 and ERC1155, let us tokenize things like art, collectibles, and even real estate. They can only have one official owner at a time and, in case of ERC721 and ERC1155, are secured by the Ethereum blockchain – no one can modify the record of ownership or copy/paste a new NFT into existence.\nWhat does the OpenSea SDK do? #  The OpenSea SDK enables developers to easily access the OpenSea orderbook, filter it, create buy orders (offers), sell orders (auctions) or collections of assets to sell at once (bundles), and to complete trades programmatically.\nWhy would I store my NFT on the decentralized cloud? #   It’s more available than centralized alternatives like AWS (data stored on Storj DCS is broken into redundant erasure codes across the globe) Storj DCS is faster than Kademlia-based networks (like BitTorrent or IPFS) It’s 80% less expensive than Amazon S3  Tutorial #  If you haven’t already registered for a Storj DCS account, you\u0026rsquo;re going to need to take care of that upfront. Instructions are easy to follow.\nNow that you’ve got your account squared away, let’s upload your digital asset and create a linkshare link. For this tutorial, we’re just going to use the web interface in the Satellite admin console, but you can also use our CLI, one of the libraries or bindings, our S3 compatible gateway, or an app like FileZilla or Rclone.\nIf you want a little more context about the different components and constructs in Storj DCS, you can read a quick article on the information architecture.\nOkay, let’s do this. #   Create a Storj DCS account. ( storj.io/signup) Create a Project Navigate to the Object Browser Create a Bucket Upload a file by dragging and dropping your digital asset into the bucket via the browser Generate a Linkshare for URL hosting using Storj Object Browser or CLI share command  Click the 3 dot button to the right of your object and choose Share Click Generate Share Link Copy the Share Link and click Done   You can interact with your digital asset across the Decentralized Cloud in a number of ways  ​​To download content directly, use /raw/ in the Linkshare URL ex: https://link.us1.storjshare.io /raw/ ju34skavohcqezr6vlfgshg5nmjq/dwebdemo/isthataquestion.mp4 To view the digital asset in the object map that shows the location of the storage nodes storing the encrypted and erasure-coded pieces, use /s/ in the Linkshare URL ex: https://link.us1.storjshare.io /s/ ju34skavohcqezr6vlfgshg5nmjq/dwebdemo/isthataquestion.mp4   Create the NFT on OpenSea and use the raw content link as external link for NFT Metadata  Use the OpenSea GUI: https://opensea.io/collections Or developer SDK here: ( https://docs.opensea.io/docs/2-adding-metadata)    "},{"id":129,"href":"/dcs/getting-started/satellite-developer-account/","title":"Quickstart - Satellite Admin Console","first":"/dcs/","section":"Getting Started","content":"Quickstart - Satellite Admin Console #  Introduction #  The Storj DCS Satellite Admin Console is the web interface for developers to interact with Storj DCS.\nIf you want to learn more about Developer Accounts, Projects, Access Grants, Buckets and how these architectural constructs relate and how they are used, check out the Key Architecture Constructs under Concepts.  The Storj DCS Satellite Admin Console is where you:\n  Register for a Developer Account Create and manage Projects  View your Project Dashboard for summary data on usage, billing and invoices. Use the Object Browser to drag and drop objects onto Storj DCS through the browser, see the map of storage nodes storing the pieces of your file, and create sharable links to your objects. Create Access Grants, the all-in-one bearer token for access management and encryption for configuring client tools and applications such as the CLI, developer library, self-hosted gateway, client apps like Rclone and FileZilla, or using the cloud-hosted S3-compatible gateway.  Invite other developers to collaborate on your project. Manage your account and payment method  The first step is to register for a developer account on a Satellite.\n"},{"id":130,"href":"/dcs/billing-payment-and-accounts-1/storj-token/reporting-a-payment-problem/","title":"Reporting a Payment Problem","first":"/dcs/","section":"Payment Methods","content":"Reporting a Payment Problem #  We use best-in-class partners for payment processing (Stripe for credit card payments and CoinPayments for STORJ utility token transactions). Still, sometimes unanticipated issues can arise.\nIf the issue is with a credit card payment, please start by submitting a support ticket through our Storj DCS support portal. We will attempt to resolve the issue prior to escalating with Stripe.\nIf the issue is with a STORJ token transaction, you should first contact CoinPayments to confirm that your STORJ token transaction was received by CoinPayments. Once you have documentation from CoinPayments that the STORJ token transaction was processed, submit a support ticket through our support portal, including any documentation you have for the STORJ token transaction.\nPlease note that “Smart Contract\u0026quot; wallets such as Argent Wallet, Authereum and Gnosis are not compatible for making deposits of STORJ token via CoinPayments. Please only send your STORJ from a wallet that does not use a smart contract by default to send the tokens. Any problems arising from attempts to send STORJ deposits to CoinPayments via a “Smart Contract” wallet should be resolved by contacting CoinPayments support.\\\n"},{"id":131,"href":"/dcs/getting-started/satellite-developer-account/users/","title":"Users","first":"/dcs/","section":"Quickstart - Satellite Admin Console","content":"Users #  If you need to collaborate with other developers on a project, you can add other developers who have an account on the same Satellite as your project.\nWhen you add another user to your project, that user will have full access to the Project Dashboard, Object Browser, and access Grants for your Project.  Navigate to the Users screen.\nSelect the Add button\nType in the email addresses that the users have registered with their Satellite Accounts. The Users will be added to the Project Team and notified via email.\n"},{"id":132,"href":"/node/resources/faq/other-commands/","title":"What other commands can I run?","first":"/node/","section":"FAQ's","content":"What other commands can I run? #  Run help to see other commands:\nCLI Install docker exec -it storagenode /app/storagenode help GUI Windows Install PowerShell:\n\u0026amp;\u0026#34;$env:ProgramFiles\\Storj\\Storage Node\\storagenode.exe\u0026#34; --help  Run the following to execute other commands:\nCLI Install docker exec -it storagenode /app/storagenode \u0026lt;\u0026lt;command\u0026gt;\u0026gt; GUI Windows Install PowerShell:\n\u0026amp;\u0026#34;$env:ProgramFiles\\Storj\\Storage Node\\storagenode.exe\u0026#34; \u0026lt;\u0026lt;command\u0026gt;\u0026gt;  "},{"id":133,"href":"/dcs/getting-started/satellite-developer-account/billing/","title":"Billing","first":"/dcs/","section":"Quickstart - Satellite Admin Console","content":"Billing #  Introduction #  The Billing screen allows you to see all your projects and their estimated charges for the Current Billing Period, the Invoices for a Previous Billing Period and Billing History. You can also see an Available Balance, Balance History and Free Credits. At the bottom you can see and add Coupons.\nView a Previous Billing Period and Invoices #  If you select the Current Billing Period on top right, you will see two additional options:\nView a Previous Billing Period #  After selecting a Previous Billing Period you can see how much you were billed in it.\nYou can expand any Project to see details of the charge. See How Billing is Calculated for details.\nView a Billing History #  Select a Billing History in the drop down menu Current Billing Period / Previous Billing Period.\nIn the Billing History screen you can see a list of paid invoices and can view/download a PDF file of the selected invoice.\nAdd a Payment Method #  To add a Payment Method you can select Add STORJ or Add Card.\nPlease read the Billing, Payment and Accounts section for details.\nAdding STORJ tokens #  You can select to Add STORJ on the Billing screen.\nYou need to choose the USD value you are willing to pay in STORJ tokens. After amount is selected you may select to Continue to Coin Payments. It will open a new page to our current Payments processor.\nPlease check their documentation, if you have any questions or problems with your payment.\nWhen you will pay the requested amount of STORJ, they will be added automatically to your Available Balance in USD value.\nPlease note, the payment will be accounted only after some amount of confirmations on the Ethereum network and then Payment processor will send them to your balance. This could take from minutes and up to 4 hours. If it took longer or you sent not enough - please contact the Payment processor support.  Viewing a Balance History #  You can see a Short Balance History right in the Billing screen below the Payment Method section.\nTo see a full Balance History you can select it in the drop down list Available Balance on top of the Billing screen.\nIt will show the Balance History page.\nYou can view a Checkout for your STORJ deposits.\nAdding a Card #  You can select to Add Card on the Billing screen. You will be prompted to specify Card details.\nPlease provide a valid Card number, expiration date and CVC, then confirm adding a Card with the Add Card button. We do not store your card details, they are used to register your Card on Stripe.\nAdd Coupons #  You can Add Coupon Code or scan it:\nThe added Coupon will be listed below.\n"},{"id":134,"href":"/dcs/concepts/data-structure/","title":"Data Structure","first":"/dcs/","section":"Concepts","content":"Data Structure #  Hierarchy of Data Storage #  Projects\nProjects allow you to invite team members, manage billing, and manage access for various apps or users.\nBuckets Buckets represent a collection of objects. You can upload, download, list, and delete objects of any size or shape.\nObjects\nEach object typically includes the data itself, a variable amount of metadata, and a globally unique identifier (Object key) which uniquely identifies the object in a bucket. Objects within buckets are represented by keys, where keys can optionally be listed using the \u0026ldquo;/\u0026rdquo; delimiter. Objects are always end-to-end encrypted.\nAdvanced Concepts #  Bucket: represent a collection of objects. You can upload, download, list, and delete objects of any size or shape.\nA bucket is an unbounded but named collection of files identified by object keys. Every object has a unique key within a bucket.\nObject: Each object typically includes the data itself, a variable amount of metadata, and a globally unique identifier (Object key) which uniquely identifies the object in a bucket. Objects within buckets are represented by keys, where keys can optionally be listed using the \u0026ldquo;/\u0026rdquo; delimiter. Objects are always end-to-end encrypted.\nObject Key: An object key is a unique identifier for a file within a bucket. An object key is an arbitrary string of bytes. Object keys contain forward slashes at access control boundaries. Unless otherwise requested, we encrypt the object key before they ever leave the customer’s application’s computer.\nSegment: represents a single array of bytes up to 64 MB.\nInline Segment: A inline segment is a file smaller than the meta data required to keep track of all of the pieces on the network for it. Since inline segments are smaller than the metadata for remote segments these inline segments are stored directly on the satellite. this means inline segments are NOT stored on the decentralized network.\nRemote Segment: A remote segment is a segment that will be erasure encoded and distributed across the network. A remote segment is larger than the metadata required to keep track of it on the network.\nStripe: a further subdivision of a segment. A stripe is a fixed amount of bytes that is used as an encryption and erasure encoding boundary size. Erasure encoding happens on stripes individually, whereas encryption may happen on a small multiple of stripes at a time. All segments are encrypted, but only remote segments erasure encode stripes. A stripe is the unit on which audits are performed. See section 4.8.3 in whitepaper for more details. \nErasure Share: When a stripe is erasure encoded, it generates multiple pieces called erasure shares. Only a subset of the erasure shares are needed to recover the original stripe. Each erasure share has an index identifying which erasure share it is (e.g., the first, the second, etc.).\nPiece: When a remote segment’s stripes are erasure encoded into erasure shares, the erasure shares for that remote segment with the same index are concatenated together, and that concatenated group of erasure shares is called a piece. If there are n erasure shares after erasure encoding a stripe, then there are n pieces after processing a remote segment. The nth piece is the concatenation of all of the with erasure shares from that segment’s stripes.\n"},{"id":135,"href":"/node/resources/faq/check-logs/","title":"How do I check my logs?","first":"/node/","section":"FAQ's","content":"How do I check my logs? #  You can look at your logs to see if you have some errors indicating that something is not functioning properly:\nCLI Install docker logs storagenode Use this command if you just want to see the last 20 lines of the log:\ndocker logs --tail 20 storagenode For CLI Docker install on Windows, if you have redirected your logs to a file, please execute the following command in PowerShell, inserting your actual path to your log file instead of \u0026ldquo;pathtologfile\u0026rdquo;\nGet-Content \u0026#34;pathtologfile\u0026#34; -Tail 20 -Wait For CLI Linux and MacOS install, if you have redirected your logs to a file, please use your preferred editor to view the contents of the log file.\nGUI Windows Install From PowerShell, to see the last 20 lines of the log:\nGet-Content \u0026#34;$env:ProgramFiles/Storj/Storage Node/storagenode.log\u0026#34; -Tail 20 -Wait If you have redirected your logs to a file, from PowerShell execute the command, inserting your actual path to your log file in \u0026ldquo;pathtologfile\u0026rdquo;:\nGet-Content \u0026#34;pathtologfile\u0026#34; -Tail 20 -Wait  "},{"id":136,"href":"/dcs/api-reference/uplink-cli/mv/","title":"mv","first":"/dcs/","section":"Uplink CLI","content":"mv #  Usage #  Windows ./uplink.exe mv SOURCE DESTINATION [flags] Linux uplink mv SOURCE DESTINATION [flags] macOS uplink mv SOURCE DESTINATION [flags]  The mv command is used to move or rename objects within the same Storj DCS project. The mv command uses a server-side move (rename) method, it does not incur a fee for downloading and will be performed with no delay.\nFlags #     Flag Description     --access string the serialized access, or name of the access to use   --help, -h help for mv    Examples #  Move an object within an existing bucket #  When the mv command is used to move a file within Storj DCS, the CLI will move (rename) the object using the server-side method to rename the object.\nTo move cheesecake.jpg within the existing bucket cakes, use the following command:\nWindows ./uplink.exe mv sj://cakes/cheesecake.jpg sj://cakes/New-York/cheesecake.jpg Linux uplink mv sj://cakes/cheesecake.jpg sj://cakes/New-York/cheesecake.jpg macOS uplink mv sj://cakes/cheesecake.jpg sj://cakes/New-York/cheesecake.jpg  You cannot use pattern expressions to specify which files to copy (e.g. uplink mv sj://cakes/cheese* sj://cakes/New-York/ will not work). Also, you can only specify one source at a time.  Sample Output:\nMove an object from a one bucket to another #  When the mv command is used to move an object from one Storj DCS bucket to another Storj DCS bucket, the CLI will use a server-side move method.\nTo create a new bucket, we will use the mb command, as a move is possible only to an existing bucket.\nWindows ./uplink.exe mb sj://new-recipes Linux uplink mb sj://new-recipes macOS uplink mb sj://new-recipes  Bucket new-recipes created Nested buckets are not supported, but you can use prefixes, as they would act almost like subfolders.  To move an object from one bucket to another, use:\nWindows ./uplink.exe mv sj://cakes/New-York/cheesecake.jpg sj://new-recipes/cakes/cheesecake.jpg Linux uplink mv sj://cakes/New-York/cheesecake.jpg sj://new-recipes/cakes/cheesecake.jpg macOS uplink mv sj://cakes/New-York/cheesecake.jpg sj://new-recipes/cakes/cheesecake.jpg  Sample Output:\nTroubleshooting move errors #  ERROR: duplicate key value violates unique constraint \u0026ldquo;primary\u0026rdquo; (SQLSTATE 23505) #  uplink mv sj://cakes/New-York/cheesecake.jpg sj://new-recipes/cakes/cheesecake.jpg Error: uplink: metaclient: metabase: unable to update object: ERROR: duplicate key value violates unique constraint \u0026#34;primary\u0026#34; (SQLSTATE 23505) This error means that the destination object already exists. You should either use a different destination name/prefix or remove the existing object from the destination.\nTo remove an object, use the uplink rm command.  "},{"id":137,"href":"/dcs/how-tos/sync-files-with-rclone/","title":"Sync Files With Rclone","first":"/dcs/","section":"How To's","content":"Sync Files With Rclone #  These \u0026lsquo;Getting Starting\u0026rsquo; tutorials will showcase the process for configuring Rclone with Storj DCS. We will cover only some of the basic features in this guide.\nFor making the most of Rclone, take a look at the complete Rclone command reference\nThese guides are experimental. The main functionality appears to work, but there are expected to be undiscovered issues (including issues around connection timeouts). Please report any issues you may run into on this forum thread.  You will need one of the following:\n  Access Grant that someone else shared with you, or  API Key ( Access token) of a Storj DCS project you are a member of.  Selecting an Integration Pattern #  Native #  Use our native integration pattern to take advantage of client-side encryption as well as to achieve the best possible download performance. Uploads will be erasure-coded locally, thus a 1gb upload will result in 2.68gb of data being uploaded to storage nodes across the network.\nUse this pattern for #   The strongest security The best download speed  Rclone with Native Integration Hosted Gateway #  Use our S3 compatible Hosted Gateway integration pattern to increase upload performance and reduce the load on your systems and network. Uploads will be encrypted and erasure-coded server-side, thus a 1GB upload will result in only in 1GB of data being uploaded to storage nodes across the network.\nUse this pattern for #   Reduced upload time Reduction in network load  By selecting this integration pattern you are opting in to server-side encryption.  Rclone with Hosted Gateway "},{"id":138,"href":"/dcs/how-tos/backup-with-restic/","title":"Backup With Restic","first":"/dcs/","section":"How To's","content":"Backup With Restic #  Overview #  Restic is a backup client written in Go language, it is highly secure and efficient. Each Restic backup is a snapshot of the server/files/directory, deduplicated from what was stored before. Any restore to a given backup will restore the server/files/directories to the exact state they were at that time.\nThis is a quick-start tutorial that covers Restic usage with Storj DCS.\nIn this guide, we will cover only some of the basic features of the tool. The complete documentation for Restic is located here, at their complete command reference.\nThis guide is experimental. The main functionality appears to work, but there are expected to be undiscovered issues. Please report any issues you may run into on this forum thread.  Before you begin #  If you haven\u0026rsquo;t yet, create a Storj DCS account before following the rest of the tutorial.\nPrerequisites You will need the following:\n Install and configure Rclone by following this walkthrough Read through the Restic-Rclone documentation here  The bucket for the backup needs to exist before using Restic. Use Rclone to create the bucket:\n$ rclone mkdir storj:bucket The general backend specification format is rclone:\u0026lt;remote\u0026gt;:\u0026lt;path\u0026gt;, the \u0026lt;remote\u0026gt;:\u0026lt;path\u0026gt; component will be directly passed to Rclone. When you configure a remote named foo, you can then call Restic as follows to initiate a new repository in the path bar in the repo:\n$ restic -r rclone:foo:bar init Restic will take care of starting and stopping Rclone for your backup  Setup #  First, install Restic for your operating system, then execute the init command:\nrestic --repo rclone:storj:bucket/my-backup init Flag --repo defines that we will use rclone as a tool for backup with storj configuration. The last part bucket/my-backup specifies where our backup will be stored remotely.\nThe label storj refers to the rclone configuration name which you chose during setup.\nNow, enter a password for your repository.\nRemembering your password is important! If you lose it, you won’t be able to access data stored in the repository.  Repository data will be created directly at the specified bucket prefix e.g. bucket/my-backup.\nNow you are ready to do your first backup!\nBacking Up #  Execute the backup command:\nrestic --repo rclone:storj:bucket/my-backup backup ~/directory-to-backup When backing up the root directory on Unix systems it is important to pass --one-file-system to prevent accidentally backing up virtual filesystems like/proc.  You will be able to see the progress of the backup and a summary at the end of the process.\nCleanup #  With every backup, Restic is creating a new snapshot with contents of a directory at the moment. To remove old and unused snapshots we need to execute the forget command:\nrestic --repo rclone:storj:bucket/my-backup forget --keep-last 2 --prune The --keep-last flag is for keeping last n snapshots. This command offers multiple flags for defining deletion rules. See restic help forget for more options.\nThe --prune flag is for removing unreferenced data. Without this option, the forget command will remove the snapshot but not the referenced data.\nCheck #  If you want to verify the consistency of your backup, run the check command:\nrestic --repo rclone:storj:bucket/my-backup check Restore #  To restore the latest snapshot of your backup:\nrestic --repo rclone:storj:bucket/my-backup restore latest --target ~/restore The latest option means we want to restore the latest snapshot.\nThe --target flag defines the directory where the backup will be restored.\nFor more detailed information around Restic usage, please visit the Restic documentation page.\n"},{"id":139,"href":"/dcs/concepts/encryption-key/","title":"Encryption","first":"/dcs/","section":"Concepts","content":"Encryption #  Separate and distinct from the topic of access management, a number of different levels and applications of strong encryption are applied throughout the Storj DCS service. Within the encryption paradigm, there are design choices that impact the level of privacy and security of an application.\nKey Point: Storj DCS provides several approaches to developing more secure and private applications and your choice of integration pattern will allow you to make an informed decision on the right type of encryption to provide the commensurate level of privacy and security for your application data.  "},{"id":140,"href":"/node/resources/faq/redirect-logs/","title":"How do I redirect my logs to a file?","first":"/node/","section":"FAQ's","content":"How do I redirect my logs to a file? #  1. To redirect the logs to a file, stop your Node:\nCLI Install docker stop -t 300 storagenode GUI WIndows Install Elevated PowerShell:\nStop-Service storagenode Or click the \u0026ldquo;Stop\u0026rdquo; button in the Windows Services applet on \u0026ldquo;Storj V3 Storage Node\u0026rdquo; service\n 2. Then edit your config.yaml (you can use nano or vi editor for Linux/MacOS or Notepad++ for Windows) to add (or change) the log location (see Where can I find a config.yaml?)):\nCLI Install log.output: \u0026#34;/app/config/node.log\u0026#34; You can find resulting log in the storage location.\nGUI Windows Install log.output: winfile:///X:\\Storagenode\\node.log  3. Start your Node again:\nCLI Install docker start storagenode When you use this option, docker logs commands no longer show your node log. Use the file instead.  GUI Windows Install Elevated PowerShell:\nStart-Service storagenode Or click the \u0026ldquo;Start\u0026rdquo; button in the Windows Services applet on \u0026ldquo;Storj V3 Storage Node\u0026rdquo; service\n "},{"id":141,"href":"/dcs/api-reference/uplink-cli/rb-command/","title":"rb","first":"/dcs/","section":"Uplink CLI","content":"rb #  Usage #  Windows ./uplink.exe rb sj://BUCKET [flags] Linux uplink rb sj://BUCKET [flags] macOS uplink rb sj://BUCKET [flags]  Flags #     Flag Description     --access string the serialized access, or name of the access to use   --force if true, empties the bucket of objects first   --help, -h help for rb    Examples #  Delete empty bucket #  Windows ./uplink.exe rb sj://cakes Linux uplink rb sj://cakes macOS uplink rb sj://cakes  Output:\nDelete bucket and all the objects it contains #  Windows ./uplink.exe rb sj://cakes --force Linux uplink rb sj://cakes --force macOS uplink rb sj://cakes --force  Output:\n"},{"id":142,"href":"/dcs/getting-started/satellite-developer-account/resources/","title":"Resources","first":"/dcs/","section":"Quickstart - Satellite Admin Console","content":"Resources #  Here you can select:\n  Docs  Forum  Support  "},{"id":143,"href":"/dcs/concepts/decentralization/","title":"Decentralization","first":"/dcs/","section":"Concepts","content":"Decentralization #  Decentralized data storage means more security and privacy. Decentralized cloud storage is more difficult to attack than traditional centralized data. On a decentralized network, files are broken apart and spread across multiple nodes. Storj DCS uses Erasure Coding to distribute file pieces over many nodes located in different physical locations around the world.\nThere are more than a number of reasons why you may wish to utilize decentralized storage over legacy alternatives, namely:\n Privacy \u0026amp; Security Simple and economical pricing Ease of integration  One of the main motivations for preferring decentralization is to drive down infrastructure costs for maintenance, utilities, and bandwidth. We believe that there are significant underutilized resources at the edge of the network for many smaller operators. In our experience building decentralized storage networks, we have found a long tail of resources that are presently unused or underused that could provide an affordable and geographically distributed cloud storage.\nOur decentralization goals for fundamental infrastructure, such as storage, are also driven by our desire to provide a viable alternative to the few major centralized storage entities who dominate the market at present. We believe that there exists inherent risk in trusting a single entity, company, or organization with a significant percentage of the world’s data. In fact, we believe that there is an implicit cost associated with the risk of trusting any third party with custodianship of personal data.\nUnique Advantages of Decentralized Storage  #  ​ Client-side encryption: the cryptographic technique of encrypting data on the sender\u0026rsquo;s side, before it is transmitted to a server such as a cloud storage service. Client-side encryption features an encryption key that is not available to the service provider (in this case, Storj), making it difficult or impossible for service providers to decrypt hosted data. Client-side encryption allows for the creation of applications whose providers cannot access the data its users have stored, thus offering a high level of privacy. (Source: https://en.wikipedia.org/wiki/Client-side_encryption)\n​ Erasure Coding: In coding theory, an erasure code is a forward error correction (FEC) code under the assumption of bit erasures (rather than bit errors), which transforms a message of k symbols into a longer message (code word) with n symbols such that the original message can be recovered from a subset of the n symbols. The fraction r = k/n is called the code rate. The fraction k’/k, where k’ denotes the number of symbols required for recovery, is called reception efficiency. (Source: https://en.wikipedia.org/wiki/Erasure_code).​\nYou can learn more about erasure codes in Storj DCS under the File Redundancy section under Concepts.\n​ Data Repair is necessary when the number of available pieces of a file still held on the network approaches the minimum threshold below which it would become impossible to recover the file. When we reach this threshold, the network will proceed to repair the data in such a way that the number of available pieces is always big enough to prevent the file from becoming irretrievable.\nYou can learn more about data repair in Storj DCS under the File Repair section under Concepts.\n​File Audit is the action of testing if a random piece can successfully be retrieved from a node that is storing it. File Audits are continually applied to assure the durability of the files on Storj DCS. The audit service is a highly scalable and performant analog to the consensus mechanism, typically a distributed ledger, used in other decentralized storage services.\n​\n"},{"id":144,"href":"/dcs/how-tos/set-up-filezilla-for-decentralized-file-transfer/","title":"FileZilla Native Integration","first":"/dcs/","section":"How To's","content":"FileZilla Native Integration #  The native integration uses end-to-end encryption for your object data, including metadata and path data.  This is the only integration available for the free version of Filezilla. If you wish to use the Hosted Gateway MT you will need the paid version of Filezilla.  Background #  The FileZilla Client is a fast and reliable cross-platform (Windows, Linux and Mac OS X) FTP, FTPS and SFTP client with lots of useful features and an intuitive graphical user interface.\nIt includes a site manager to store all your connection details and logins, as well as an Explorer-style interface that shows the local and remote folders and can be customized independently.\nWith the launch of the native Storj DCS Integration into the FileZilla client, developers can use the client configured to transfer files, point-to-point using the decentralized cloud.\nGetting Started #  Create an Access Grant #  Navigate to the Access page within your project and then click on Continue.\nGive your new Access Grant a name.\nAssign permissions to the Access Grant.\nIf you click \u0026ldquo;Continue in Browser\u0026rdquo;, our client-side javascript will finalize your access grant with your encryption passphrase. Your data will remain end-to-end encrypted until you explicitly register your access grant with Gateway MT for S3 compatibility. Only then will your access grant be shared with our servers. Storj does not know or store your encryption passphrase.\nHowever, if you are still reluctant to enter your passphrase into our web application, that\u0026rsquo;s completely understandable, and you should select \u0026ldquo;Continue in CLI\u0026rdquo; and follow these instructions.\nThe instructions below assume you selected \u0026ldquo;Continue in Browser.\u0026rdquo;\n Select a Passphrase type: Either create your own Encryption Passphrase or Generate a 12-Word Mnemonic Passphrase. Make sure you save your encryption passphrase as you\u0026rsquo;ll not be able to reset this after it\u0026rsquo;s created.\nThis passphrase is important! Encryption keys derived from it are used to encrypt your data at rest, and your data will have to be re-uploaded if you want it to change!\nImportantly, if you want two access grants to have access to the same data, they must use the same passphrase. You won\u0026rsquo;t be able to access your data if the passphrase in your access grant is different than the passphrase you uploaded the data with.\nPlease note that Storj does not know or store your encryption passphrase, so if you lose it, you will not be able to recover your files.\n Access Grant is generated. The Access Grant will only display once. Save this information in a password manager or wherever you prefer to store sensitive information.\nDownloading FileZilla #  To download the latest release of FileZilla, navigate to https://filezilla-project.org/download.php?show_all=1 and select the version appropriate for your operating system, then install FileZilla.\nCreating a new Site #  Open the Site Manager by clicking on the leftmost icon.\nSelect the \u0026lsquo;New Site\u0026rsquo; option\nConfigure the Satellite and Access Grant #  Next, select Protocol: \u0026ldquo;Storj - Decentralized Cloud Storage\u0026rdquo; from the Protocol dropdown in the \u0026ldquo;General\u0026rdquo; tab.\nNow enter the Satellite and Access Grant as shown below (Entering the port is not required)\n Use the Satellite URL from which you created the Access Grant.  us-central-1.tardigrade.io (us1.storj.io) asia-east-1.tardigrade.io (ap1.storj.io) europe-west-1.tardigrade.io (eu1.storj.io)   For Access Grant please enter the Access Grant you saved above.  After you enter the above information, hit the \u0026ldquo;Connect\u0026rdquo; button, and FileZilla will connect directly to the remote site. You should see a screen showing your local site vs. Storj DCS, like so:\nUploading a File #  To upload a file to your local machine, simply drag it from the local to the remote site (on the decentralized cloud), as shown below:\nDownloading a File #  To download a file to your local machine, simply drag it from the remote site to the local site, as shown below:\n"},{"id":145,"href":"/node/resources/faq/how-do-i-check-my-l2-payouts/","title":"How do I check my L2 zkSync payouts?","first":"/node/","section":"FAQ's","content":"How do I check my L2 zkSync payouts? #  If you opted-in for zkSync, you can check your payout in three ways:\nPayout section on the Dashboard #  You can click the View on zkScan button on the main page of your web-dashboard (see Dashboard for CLI and Dashboard for GUI), as shown in the screenshot of the dashboard Payout section below. This will send you to your zkSync wallet page where you can review the latest L2 payout transactions received.\nIf you do not see the notification \u0026ldquo;zkSync is opted-in\u0026rdquo;, then you did not enable zkSync in the configuration of the node. See Configuring zkSync Payments how to opt in.  Payout Information #  Open your web-dashboard (see Dashboard for CLI and Dashboard for GUI), navigate to the Payout section and click the Payout Information link. Scroll down to the Payout History section and expand any of the satellites. You should see a Transaction link. If you click on it - it will open the corresponding transaction on https://zkscan.io if you opted-in for zkSync and received payout on L2. If you did not opt in to zkSync, the Transaction link will show your L1 payout on https://etherscan.io.\nIf you do not see the Transaction link, then your node has not received payout receipts from the satellites yet. You need to wait at least 24 hours after the payout for the previous month has been completed to see it.  Check your wallet on zkscan.io #  And finally, you can open https://zkscan.io, put your wallet address in the search field and click the Search button.\n"},{"id":146,"href":"/dcs/getting-started/satellite-developer-account/quick-start/","title":"Quick Start","first":"/dcs/","section":"Quickstart - Satellite Admin Console","content":"Quick Start #  These Quick Start Wizards can help you to use the Satellite Admin Console.\n"},{"id":147,"href":"/dcs/api-reference/uplink-cli/rm-command/","title":"rm","first":"/dcs/","section":"Uplink CLI","content":"rm #  Usage #  Windows ./uplink.exe rm sj://BUCKET/KEY [flags] Linux uplink rm sj://BUCKET/KEY [flags] macOS uplink rm sj://BUCKET/KEY [flags]  Flags #     Flag Description     --access string the serialized access, or name of the access to use   --encrypted if true, treat paths as base64-encoded encrypted paths   --help, -h help for rm   --parallelism, -p int Controls how many removes to perform in parallel (default 1)   --pending Remove pending object uploads instead   --recursive, -r Remove recursively   --help, -h help for rm    Examples #  Delete an object #  Windows ./uplink.exe rm sj://cakes/cheesecake.jpg Linux uplink rm sj://cakes/cheesecake.jpg macOS uplink rm sj://cakes/cheesecake.jpg  Delete an encrypted object #  If an object has been created with another encryption key, you won\u0026rsquo;t be able to read it, but you can delete it. In order to delete an encrypted object, you have to know its encrypted path. To retrieve it, you can use the list command ls with the encrypted file. For instance, to list the encrypted path of the objects in a bucket sj://cakes you could use:\nWindows ./uplink.exe ls sj://cakes --encrypted Linux uplink ls sj://cakes --encrypted macOS uplink ls sj://cakes --encrypted  You can then use this path to delete the encrypted object:\nWindows ./uplink.exe rm --encrypted sj://cakes/Ao8rmi2hw5v8_SS2GRokJwqkzQ2j9wXRH2Ll-1owEGPwIWMyu8tj5YCCig== Linux uplink rm --encrypted sj://cakes/Ao8rmi2hw5v8_SS2GRokJwqkzQ2j9wXRH2Ll-1owEGPwIWMyu8tj5YCCig== macOS uplink rm --encrypted sj://cakes/Ao8rmi2hw5v8_SS2GRokJwqkzQ2j9wXRH2Ll-1owEGPwIWMyu8tj5YCCig==  "},{"id":148,"href":"/dcs/concepts/file-redundancy/","title":"File Redundancy","first":"/dcs/","section":"Concepts","content":"File Redundancy #  Durability and expansion factor  #  In a decentralized storage network, any storage node could go offline permanently at any time. A storage network’s redundancy strategy must store data in a way that provides access with high probability, even though any given number of individual nodes may be in an offline state. To achieve a specific level of durability (defined as the probability that data remains available in the face of failures), many products in this space (Filecoin, MaidSafe, Siacoin, GFS, Ceph, IPFS, etc.) by default use replication, which means simply having multiple copies of the data stored on different nodes.\nUnfortunately, replication anchors durability to the network expansion factor, which is the storage overhead for reliably storing data. If you want more durability, you need more copies. For every increase of durability you desire, you must spend another multiple of the data size in bandwidth when storing or repairing the data, as nodes churn in and out of the network.\nFor example, suppose your desired durability level requires a replication strategy that makes eight copies of the data. This yields an expansion factor of 8x, or 800%. This data then needs to be stored on the network, using bandwidth in the process. Thus, more replication results in more bandwidth usage for a fixed amount of data.\nOn the one hand, replication does make network maintenance simpler. If a node goes offline, only one of the other storage nodes is needed to bring a new replacement node into the fold. On the other hand, for every node that is added to the redundancy pool, 100% of the replicated data must be transferred.\nErasure Code  #  Erasure codes are another redundancy approach, and importantly, they do not tie durability to the expansion factor. You can tune your durability without increasing the overall network traffic!\nErasure codes are widely used in both distributed and peer-to-peer storage systems. While they are more complicated and possess trade-offs of their own, the scheme we adopt, Reed-Solomon, has been around since 1960 and is used everywhere from CDs, deep space communication, barcodes, advanced RAID-like applications–you name it.\nAn erasure code is often described by two numbers, k and n. If a block of data is encoded with a k, n erasure code, there are n total generated erasure shares, where only any k of them are required to recover the original block of data! It doesn’t matter if you recover all of the even numbered shares, all of the odd numbered shares, the first k shares, the last k shares, whatever. Any k shares can recover the original block.\nIf a block of data is s bytes large, each of the n erasure shares are roughly s/k bytes. For example, if a block is 10 MB, and you’re using a k = 10, n = 20 erasure code scheme, each erasure share of that block will only be 1 MB. This means that with k = 10, n = 20, the expansion factor is only 2x. For a 10 MB block, only 20 MB total is used, because there are twenty 1-MB shares. The same expansion factor holds with k = 20, n = 40, where there are forty 512-KB shares.\nInterestingly, the durability of a k = 20, n = 40 erasure code is better than a k = 10, n = 20 erasure code, even though the expansion factor (2x) is the same for both. This is because the risk is spread across more nodes in the k = 20, n = 40 case. To help drive this point home, we calculated the durability for various erasure code configuration choices in a network with a churn of 10%. We talked more about the math behind this table in section 3.4 of our paper, and we’ll discuss more about churn in an upcoming blog post, but suffice it to say, we hope these calculated values are illustrative:\nNotice how increasing the amount of storage nodes involved increases the amount of durability significantly (each new 9 is 10x more durable), without a change in the expansion factor. We also put this data together in a graph:\nAdmittedly, this graph is a little disingenuous: the chances of you caring about having thirty-two 9s of durability is… low, to say the least. The National Weather Service estimates the likelihood of you not getting hit by lightning this year at only six 9s after all. But you should still be able to see that a k = 2, n = 4 is less durable than a k = 16, n = 32 configuration.\nIn contrast, replication requires significantly higher expansion factors for the same durability. The following table shows durability with a replication scheme:\nComparing the two tables, notice that replicating data at 10x can’t beat erasure codes with k = 16, n = 32, which is an expansion factor of only two. For durable storage, erasure codes simply require ridiculously less disk space than replication.\nIf you want to learn more about how erasure codes work, you can read this introductory tutorial I co-wrote last year.\nOkay, erasure codes take less disk space. But isn’t repairing data more expensive?  #  It’s true that replication makes repair simpler. Every time a node is lost, only one of the remaining nodes is necessary for recovery. On the flip side, erasure codes require several nodes to be involved for each repair. Though this feels like a problem, it’s actually not.\nTo understand why, let’s set up both scenarios, replication at 9x and erasure codes at k = 18, n = 36, and consider what it costs us. These numbers are chosen because they have similar durability (9x replication has six 9s of durability assuming 10% of node churn, and k = 18, n = 36 erasure coding has seven). We’ll consider what happens when we are storing a data block that is 18 MB and we suddenly lose one-third of our nodes.\nAt 9x, replication in our model of course has an expansion factor of 9. Once again, replication is the simplest to implement. If we lose one-third of our nine nodes we will need to spin up three new nodes. Each new node transfers a copy of the lost data, which means that each node must transfer 18 MB. That’s a total of 54 MB of bandwidth usage for repair. No intensive CPU processing was needed.\nWith k = 18, n = 36 erasure codes (with an expansion factor of only two), losing one-third of our nodes means we now only have 24 nodes still available and need to repair to twelve new nodes. The data each node is storing is only 1 MB each, but eighteen nodes must be contacted to rebuild the data. Let’s designate one of the nodes to rebuild the data. It will download eighteen 1 MB pieces, reconstruct the original file, then store the missing twelve 1 MB pieces on new nodes. If this designated node is one of the new nodes, we can avoid one of the transfers. The total overall bandwidth used is at most 30 MB, which is almost half of the replication scenario. This advantage in bandwidth savings becomes even wider with higher durabilities.\nDownsides?  #  Erasure coding did require more CPU time, that’s true. Still, a reasonable erasure encoding library can generate encoded data at at least 650 MB/s, which is unlikely to be the major throughput bottleneck over a wide-area network with unreliable storage nodes.\nErasure coding also required a designated node to do the repair. While this complicates architectures in untrusted environments, it is not an unsolvable problem. It simply requires the addition of hashes, signatures, and retries in a few new places. This is something we’ll talk about more down the road. We have a lot of blog posts to write!\nNotably, erasure coding does not complicate streaming. Remember how I said erasure codes are used for satellite communication and CDs? As long as erasure coding is batched into small operations, streaming continues to work just fine. See Figure 4.2 and sections 4.1.2 and 4.8 in our white paper for more details about how we can pull native video streaming off.\nUpsides?  #  Comparing 9x replication and k = 18, n = 36 erasure coding, the latter uses less than half the overall bandwidth for repair. It also uses less than a third of the bandwidth for storage and takes up less than a third of the disk space. It is roughly ten times more durable! Holy crap!\nOh, and did I mention this also enables us to pay storage node operators more? Specifically over three times more? Because the disk-space usage is more efficient, there is more money available for each storage node operator. The income is less diluted across storage nodes, you could say.\nIt’s worth re-reading those last two paragraphs. These gains are significant. Hopefully by this point you’re convinced that erasure codes are better.\nIn this edition we dive deeper into why nodes joining and leaving the network - also known as churn - has a much more significant (and also bad) impact on a redundancy strategy that relies on replication. We make the case that using replication in a high-churn environment is not only impractical, but inevitably doomed to fail. Quoting Blake and Rodrigues, “Data redundancy is the key to any data guarantees. However, preserving redundancy in the face of highly dynamic membership is costly.”\nAn Aside on Dynamics  #  Before diving into the exciting math parts, we need to quickly define a couple of concepts relating to network dynamics. The lifetime of a node is the duration between it joining and leaving the system for whatever reason. A network made of several nodes has an average lifetime, commonly called the mean time to failure (MTTF). The inverse of mean time to failure is churn rate or frequency of failure-per-unit-of-time. It’s an important relationship to understand, especially when MTTF is a unit of time much greater than the units needed for a specific problem.\nDistributed storage systems have mechanisms to repair data by replacing the pieces that become unavailable due to node churn. However, in distributed cloud storage systems, file repair incurs a cost for the bandwidth utilized during the repair process. Regardless of whether file pieces are simply replicated, or whether erasure coding is used to recreate missing pieces, the file repair process requires pieces to be downloaded from available nodes and uploaded to other uncorrelated and available nodes.\nAfter reading part one of this series, it’s clearly not feasible to rely on replication alone, but some projects have proposed combining erasure coding and replication. Once you’ve erasure coded a file and distributed it across a set of nodes, it’s going to have a defined durability for a given level of node churn. If you want to increase that durability for that given level of node churn, you have two choices: increase the erasure code k/n ratio or use replication to make copies of the erasure coded pieces. These two strategies are very different and have a huge impact on the network beyond just increasing durability.\nOur Hypothetical Networks  #  So, let’s define two hypothetical distributed storage networks, one using erasure coding alone for redundancy (the approach used on Storj’s V3 network), and one using erasure coding plus replication for redundancy (which is the approach used by Filecoin as well as the previous, depreciated Storj network). We will assume nodes on both networks are free to join and leave at any time, and that uptime for nodes can be highly variable based on the hardware, OS, available bandwidth, and a variety of other factors. When a node leaves a network, the pieces of data on that node become permanently unavailable. Of course, if nodes fall below a certain threshold of availability in a given month, the impact on the overall availability of files is effectively equivalent to the node leaving the network altogether.\nLet’s also assume both hypothetical networks use a 4⁄8 Reed-Solomon erasure code ratio and have 99.9% durability with node churn at 10%. Both networks want to achieve eleven 9s of durability though. One is going to achieve it through erasure coding alone, and the other is going to combine erasure coding with replication.\nAnd Now, Some Math  #  As it turns out, if you know the target durability, you know the MTTF for nodes, and you know the erasure coding scheme, you can calculate the amount of data churn in a given time period. The formula for calculating data churn is:\nwhere is the churn rate, B is the number of bytes on the network, n is the total number of erasure shares, m is the repair threshold, and k is the number of pieces needed for rebuild.\nFor example, on our hypothetical erasure-coding network, even if we use a 30⁄80 Reed-Solomon scheme (which is much more durable than the 4⁄8 scenario listed above), a MTTF of 9 months would mean you would have to repair 35% of your data every month to achieve a durability of 99.999999999%!\nThis shows node churn is the single most impactful factor in file availability. Increasing node churn dramatically decreases file availability and durability. Strategies like erasure coding and replication are means of insulating against the impact of node churn, but without a mechanism to replace the data, file loss is simply a factor of the rate of churn.\nSo, let’s take that math and apply it to our two hypothetical networks. The first thing we need to do is calculate how we get to eleven 9s of durability for each of the two scenarios:\n For the erasure code-only scenario, calculate the k/n ratio that will deliver the target durability for the defined rate of churn. For the erasure code+replication scenario, calculate the number of times the erasure coded pieces need to be replicated to deliver the target durability for the defined rate of churn.  To calculate the durability of a replicated or erasure-coded file, we consider the CDF of the Poisson distribution, given by the formula:\nwhere D is the event that the most n-k pieces have been lost. In the case of simple replication, k=1, so a file is still recoverable when at most n-1 of the pieces have been lost; that is, if at least one of the copies is still on the network, the data is still accessible. When considering replication on a file that has already been subjected to erasure encoding, the calculation changes.\nSuppose that a file undergoes k=4, n=8 erasure-encoding (where 8 pieces are created and only 4 are needed for a rebuild), and then suppose further that each of the 8 erasure shares are replicated r=10 times each, for a total of 80 pieces. These 80 pieces are special in that not any 4 can be used to rebuild the file, so they should really be thought of as 80 pieces contained in 8 sets of 10 copies each. To rebuild a file, 4 of the sets must still each have at least 1 piece each.\nThus, rather than having a single factor of P(D) determining the durability (with at most n-1 pieces being lost), one has a factor of P(D) for each unique set required for rebuild, since there are now k sets of which each one must not have lost more than r-1 pieces, where the expansion factor r determines the number of copies that are made (with there being r-1 copies made to achieve an expansion factor of r, including the original file). Calculating this probability requires the use of the Binomial distribution, where we let p be the probability that at most r-1 copies have been lost from a set. Then, to calculate the probability that there are at least k sets containing at least 1 copy each, we find the area of the upper tail of the Binomial CDF:\nLet’s first look at the impact of node churn on durability based on the two hypothetical scenarios, one using replication+erasure coding, and the other optimizing for erasure coding alone. Based on the above formulas, the results are as follows:\nAs it turns out (predictably) the increased durability can be achieved in the erasure-code-only scenario with no increase in expansion factor. Adding replication to already-erasure-coded data is much more efficient that just straight up replicating the original file, (which requires 17 copies to achieve), but has triple the expansion factor of erasure codes alone.\nIn an environment where churn is even higher, or highly variable, durability is impacted significantly in either scenario:\nIn these unpredictable or highly variable environments, it becomes necessary to address the worst case scenario in order to maintain a constant level of durability. Again, as is clear from the table below, node churn has a massive impact, and when using replication, that massive impact translates directly into increases in the expansion factor. In the table below you can see the impact of churn on expansion factor when trying to maintain a minimum durability of eleven 9s:\nSo, what do these tables tell us? Well, there are a number of interesting observations to be drawn:\n At higher rates of churn, replication dramatically increases the expansion factor and, as we learned in the previous blog post, requires much higher bandwidth utilization for repair. Erasure coding can be used to achieve higher rates of durability without increasing either the expansion factor or the amount of bandwidth used for repair.  Just to drive that point home, let’s first look at how a file actually exists on the two hypothetical networks:\nIt is worth understanding the differences in how repair actually behaves on our two networks, because the process for replication is very different compared to erasure codes. Continuing the example of the 1 TB file above, let’s examine how repair actually looks when 1⁄3 of nodes storing the data exit the network:\nOne other important thing to remember about distributed storage networks is that the amount of data the network can store isn’t constrained by the amount of available hard drive space on the nodes. It’s constrained by the amount of bandwidth available to nodes. Allow me to explain.\nThe following variables and calculated values are used in determining the amount of data and bandwidth a storage node operator can contribute:\nVariables\n  Storage per storage node operator - The amount of hard drive space available to share by a storage node.\n  Download speed - The downstream bandwidth available on the network on which the storage node is operating, measured in Mbps.\\\n  Upload speed - The upstream bandwidth available on the network on which the storage node is operating, measured in Mbps.\n  ISP bandwidth cap - The maximum amount of bandwidth a storage node operator can utilize in a month before being subjected to a bandwidth cap enforcement action such as incurring a financial penalty or being subjected to bandwidth throttling from an ISP.\n  Storage node operator bandwidth utilization percentage - The percentage of the total monthly bandwidth cap that a user will dedicate to be used by their storage node, assuming some percentage of bandwidth will be utilized for other services.\n  Egress bandwidth percentage - The average amount of egress traffic from client downloads based on the use cases we support.\\\n  Repair bandwidth ratio (as a percent of storage) - The percentage amount of repair traffic on the network, primarily driven by node churn, software or hardware failure. While actual nodes may experience higher or lower repair traffic based on the pieces they hold, this is the average across the network.\n  Ingress bandwidth percentage - The amount of bandwidth available for uploads of new data from clients.\\\n  Calculations\n Total available upload bandwidth based on download speed (excluding cap) - The maximum amount of data available for ingress, based on download speed in Mbps multiplied by number of seconds in a month. Total available download bandwidth based on upload speed (excluding cap) - This calculation is the percent of the bandwidth cap a user is willing to dedicate to the Storj network multiplied by the bandwidth cap for ingress. Maximum data uploaded per month (TB) based on BW cap x percent available for upload - This calculation is the amount of data that can be uploaded irrespective of the cap, based on download speed in Mbps multiplied by seconds in a month. Maximum data uploaded per month (TB) based on download speed x seconds in month - This calculation is the percent of the bandwidth cap a user is willing to dedicate to the Storj network multiplied by the bandwidth cap. Maximum data downloaded per month (TB) based on BW cap x percent available for download - This calculation is the amount of data that can be downloaded irrespective of the cap, based on upload speed in Mbps times seconds in a month. Maximum data downloaded per month (TB) based on upload speed - This calculation is the percentage of the bandwidth cap required to dedicate to Storj repair traffic times the bandwidth cap. Maximum repair traffic per month (TB) based on BW cap - This calculation is the amount of data for repair traffic irrespective of the cap based on upload speed in Mbps times seconds in a month. Maximum repair traffic per month (TB) based on upload speed - This is how many months it will take to fill the available hard drive space at the lesser ingress rate of percent of available BW cap or actual throughput.  Although download speed is typically higher in asynchronous internet connections, from the perspective of a person uploading a file to, or downloading a file from, a decentralized file system, the upload and download from the client’s perspective are the inverse of the storage node. For example, when a client uploads data to the network, it is technically downloaded to the storage node. Similarly, when data is downloaded by a client, it is technically being uploaded by the storage node.\nThe following examples are based on two different storage nodes with different bandwidth caps. Note that the amount of data stored is inclusive of the expansion factor.\nBandwidth has a significant impact across the board. It’s generally finite and has to be split between ingress, egress and repair. As the expansion factor increases, the actual amount of bandwidth consumed for those functions increases at the same rates. Lower bandwidth caps further lower the actual amount of data the network can store with a given number of nodes. Increase the amount of bandwidth required for file repair and that number gets lower still.\nLet’s look at the impact on bandwidth available for repair if you also constrain nodes practical limits of shared storage space. In the scenario above where nodes have:\n  2 TB bandwidth cap\n  100 Mbps down/10 Mbps up asynchronous bandwidth\n  2 TB of shared storage on average\\\n  50% of data downloaded per month\n  40% data uploaded per month\n  10% churn\n  Nodes operate at 100% bandwidth capacity and storage\n  Each node has less than 0.12 TB of bandwidth available for repair. And that’s in an environment storing archival data without a lot of download bandwidth. When scaling that distributed storage network up to an exabyte of data stored, you really see the impact of the expansion factor.\nUltimately, the result is an exponential increase in the number of nodes required to support a given network size. Higher bandwidth use cases further exacerbate the problem by increasing the number of nodes required to service a given amount of stored data. That given network size has a finite amount of revenue associated with it, which is then spread over an increasing number of storage node operators, meaning that over time the amount of money earned by storage node operators decreases.\nRapidly increasing demand for more storage node operators, combined with decreasing payouts per node, results in increased node churn, which only accelerates the cycle. Once again, increased churn increases the expansion factor from replication, increasing bandwidth utilized for repair, which further erodes the bandwidth available for storage and egress.\nWhat this means is that in the debate over any reliance on replication versus erasure coding alone, in an environment that must relentlessly optimize for bandwidth conservation, erasure coding without replication is the clear winner. Replication, and proof-of-replication approaches like that used in the Filecoin network, simply cannot sustain an acceptable level of durability with a corresponding expansion factor and repair rate that can operate in a bandwidth constrained environment. Just imagine that same network above with 25% churn where the replication example requires a 1,400% expansion factor to maintain sufficient durability. Sorry if I scared you with that one.\nWhen you have to factor in the reality that in the current storage market customers only pay for storage based on the actual, pre-erasure coded or replicated volume of data, and egress bandwidth, when it comes to the dollars, replication makes even less sense.\n"},{"id":149,"href":"/dcs/how-tos/filezilla-pro-integration-guide/","title":"Filezilla Pro Integration Guide","first":"/dcs/","section":"How To's","content":"Filezilla Pro Integration Guide #  How to Integrate Filezilla Pro with Storj DCS to Easily Find, Transfer and Download All Of Your Files #  At a quick glance, FileZilla provides a fast and reliable cross-platform (Windows, Linux and Mac OS X) FTP, FTPS and SFTP client that supports connections, file transfers and file structure browsing for many of today’s cloud data storage services, including Storj. This integration is beneficial for developers as it allows them to use the FileZilla Pro client to transfer files point-to-point using the decentralized cloud.\nHowever, there are some caveats around using FileZilla that Storj DCS users should take into consideration, namely what version of FileZilla supports integration with Storj.\nWhat Is the Difference Between FileZilla and FileZilla Pro and How Does This Influence Integration with Storj? #  FileZilla is available in a free version, known as FileZilla Standard. It not only supports Storj DCS and FTP, but also FTP over TLS (FTPS) and SFTP. It is open-source software distributed free of charge under the terms of the GNU General Public License.\n FileZillaPro is a paid upgrade which delivers all of the base functionality of FileZilla Standard, while adding additional support for many of today’s popular cloud data storage services like Amazon S3, OneDrive, Dropbox, WebDAV, Microsoft Azure, OneDrive for Business and SharePoint, Google Cloud, Backblaze and, of course, Storj DCS.\nYou can use FileZilla Standard with the Storj DCS native connector—as long as you don’t use a package manager to download it. To learn how to use the Storj integration with FileZilla Standard, check out this how-to doc. To leverage the FileZillaPro functionality, you can integrate it with Storj using a native connector or our backwards S3-compatible Gateway MT. Below we will focus on the integration between Storj and FileZilla Pro.\nStorj + FileZilla Pro Integration #  FileZilla Pro gives users the option to send files to a Storj DCS account in two ways, either via native uplink or via Gateway MT. Let’s take a look at some of the specs of both Native Uplink and Gateway MT, providing a clearer understanding of which integration method will work better for unique use cases.\nNative Uplink specs regarding integration with Storj DCS: #   Native Integration (Fastest for downloading large files) Encrypt, erasure code, and transfer from the storage nodes directly from your computer. This is ideal for downloading large files fast. Supports parallelism for downloads Has a 2.68x upload multiplier for uploads and does not support segment parallelism  GatewayMT specs regarding integration with Storj DCS: #   Gateway MT (Fastest for uploading large files) Encryption, erasure coding, and upload to storage nodes occur server side Supports parallelism for upload and multi-transfer for download A 1GB upload will result in 1GB of data being uploaded to storage nodes across the network, based on S3 standard  There are benefits to each method of integration. To provide users with the best value as they look to make the most out of their Storj and FileZillaPro integration, we’ve put together dedicated sections on integrating Storj DCS with FileZillaPro via native uplink as well as through Gateway MT.\nStorj + FileZilla Pro Via Native Uplink #  Navigate to the Access page within your project and then click on ‘Create Access Grant’\nthen give your new Access Grant a name.\nAssign permissions to the Access Grant.\nIf you click Continue in Browser, our client-side javascript will finalize your access grant with your encryption passphrase. Your data will remain end-to-end encrypted until you explicitly register your access grant with Gateway MT for S3 compatibility. Only then will your access grant be shared with our servers.\nStorj does not know or store your encryption passphrase. However, if you are still reluctant to enter your passphrase into our web application, that’s completely understandable, and you should instead select Continue in CLI and follow these instructions.\nThe instructions below assume you selected Continue in Browser.  Select a passphrase type:\n Either create your own encryption passphrase or generate a 12-word mnemonic passphrase.  Make sure you save your encryption passphrase as you’ll not be able to reset this after it’s created.\nThis passphrase is important! Encryption keys derived from it are used to encrypt your data at rest, and your data will have to be re-uploaded if you want to change the passphrase!\nImportantly, if you want various access grants to have access to the same data, they must use the same passphrase. You won’t be able to access your data if the passphrase in your access grant is different from the passphrase you uploaded the data with.\n Please note that Storj does not know or store your encryption passphrase, so if you lose it, you will not be able to recover your files. Please store it in a safe place.  Now that the Access Grant has been generated, this will allow for integration with FileZilla Pro via native uplink. Let\u0026rsquo;s take a look.\nOnce the FileZilla Pro client is open, select the Open the Site Manager icon at the top left of the FileZilla Pro client. Once open, start by selecting the New Site button and Storj - Decentralized Cloud Storage as the protocol.\nNow, add the appropriate Satellite url (without adding htpps://) and simply copy your Access Grant that was previously generated within your Storj DCS account to the Access Grant field:\nHit Connect, and access to your Storj DCS account should be established.\nStorj + FileZilla Pro via Gateway MT #  In this section, we’ll go through the Storj FileZilla Pro integration leveraging Gateway MT.\nNavigate to the Access page within your project and then click on Create Access Grant +. A modal window will pop up where you should enter a name for this access grant.\nAssign the permissions you want this access grant to have, then click on Continue in Browser:\nEnter the encryption passphrase you used for your other access grants (click on the Enter Phrase option at the top far right to do so.) If this is your first access grant, use the Generate Phrase option as shown below - we strongly encourage you to use a mnemonic phrase as your encryption passphrase (The GUI automatically generates one for you on the client-side.)\nWhen you are ready - click Next.\nClick on the Generate S3 Gateway Credentials link and then click on the Generate Credentials button.\nCopy your Access Key, Secret Key, and Endpoint to a safe location. Now you are ready to use FileZillaPro to work with Gateway MT.\nSetting up regions #   In the FileZilla menu bar, click on Edit \u0026gt; Settings \u0026hellip; Select Transfers \u0026gt; S3: Providers from the menu on the left.\\  3. Click on the Add button under the Providers list.\n4. Enter Storj as the name of the hosting provider.\n5. Press ENTER.\n6. Highlight the new hosting provider (Storj).\n7. Click on the Add button under the Regions list.\n8. Enter a name (for example, \u0026ldquo;US1\u0026rdquo;) for the region and then press ENTER.\n9. Optionally, you can enter a description here.\n10. Click on the Endpoints column of the new region row and enter the Endpoint address for Storj DCS, generated earlier during the creation of the Storj Gateway MT credentials (without adding https://).\n11. Click on OK.\nAdding a new site to FileZillaPro #   In the menu bar, click on File \u0026gt; Site Manager … Click on New Site. Enter a name for the Site - for example, US1 GatewayMT. In the protocol section, select S3 - Amazon Simple Storage Service from the Protocol drop-down list.  5. Provide the **Access key ID ** and Secret Access Key in the parameters for the new Site.\n6. Click Connect to connect to Storj via Gateway MT.\nIf this is the first time you connect, you may see a message like this:\nYou will need to confirm it by clicking the OK button.\n Now you should be able to see your buckets now:\n"},{"id":150,"href":"/node/resources/faq/held-back-amount/","title":"How does held back amount work?","first":"/node/","section":"FAQ's","content":"How does held back amount work? #  The held back amount (staked) component provides a preferred way for Node Operators to exit the network. This model optimizes liveliness by deterring Nodes to exit the network without transferring their pieces (thus limiting repair costs). Importantly, this strikes an equilibrium between a very low cost of entry for Nodes while also way to insulate against the cost of data repair.\nNodes don\u0026rsquo;t need to provide any up-front stake to start earning STORJ tokens as a Storage Node Operator. Rather, during the first nine months of Storage Node Operation, a percentage of earnings are placed in a holding account. These funds are held until a Storage Node Operator chooses to leave the network. After the 15th month, a portion of the balance is returned to the Storage Node Operator, while the remainder is held indefinitely.\nIf the Storage Node Operator uses the Graceful Exit function when leaving one or more satellites, the funds corresponding to the satellite(s) they exited will be returned in full after the exit is complete. If the Storage Node Operator exits the network abruptly without completing the Graceful Exit, the held back funds for all the satellites their node was operating on at the time of abrupt exit will be forfeited to offset the cost of data repair.\nThe withholding function is structured with a tiered reduction in withholdings as the amount of time the Node is active on the network increases. Note that the node age used to calculate the applicable held amount percentage will be calculated separately for each satellite, so if a satellite is added to the network after the node first started operating, the node age for that new satellite will start counting from zero at the time the new satellite was added. The withholding model is as follows:\n Months 1-3: 75% of Storage Node revenue is withheld, 25% is paid to the Storage Node Operator Months 4-6: 50% of Storage Node revenue is withheld, 50% is paid to the Storage Node Operator Months 7-9: 25% of Storage Node revenue is withheld, 75% is paid to the Storage Node Operator Months 10-15: 100% of Storage Node revenue is paid to the storage node operator Month 16: 50% of total withholdings are returned to Storage Node Operator, with the remaining 50% held until the node gracefully exits the network  The withholding model is designed to incentivize and reward both-long term reliable Storage Nodes as well as Nodes that, when they do choose to leave the network, exit in a way that is least damaging to the network.\n"},{"id":151,"href":"/dcs/getting-started/satellite-developer-account/my-account/","title":"My Account","first":"/dcs/","section":"Quickstart - Satellite Admin Console","content":"My Account #  Introduction #  The My Account will allow you to read documentation about the Satellite, change your Account Settings or Logout from the Satellite Admin Console.\nIf you click the (i) icon on the right of the Satellite short name, you will be redirected to the Key Architectural Construct Satellite.\nAccount Settings #  The Account Settings page allows you to Edit Profile, Change Password or enable/disable Two-Factor Authentication.\nEdit Profile #  Modify your Full Name and click the Update button to confirm and save your changes.\nIf you selected a Professional Account during creation, you will have much more fields to edit. They can be used in Invoice forms in the future.  Change Password #  Provide an Old Password, your New Password and Confirm Password. Select Update to change and save the New Password.\nTwo-Factor Authentication #  After enabling of Two-Factor Authentication you will be asked for the one-time code from your favorite TOTP during the login next time.\n"},{"id":152,"href":"/dcs/api-reference/uplink-cli/setup-command/","title":"setup","first":"/dcs/","section":"Uplink CLI","content":"setup #  Usage #  Windows ./uplink.exe setup [flags] Linux uplink setup [flags] macOS uplink setup [flags]  Flags #     Flag Description     --help, -h help for setup    Example #  1. Start the CLI wizard.\nWindows PowerShell #   Navigate to the directory your uplink.exe file is located in.\n./uplink.exe setup Linux uplink setup macOS uplink setup  2. Enter the numeric choice or satellite address corresponding to the satellite you\u0026rsquo;ve created your account on (ie - enter the number 1, 2, or 3).\nSelect your satellite: [1] us1.storj.io [2] ap1.storj.io [3] eu1.storj.io Enter number or satellite address as \u0026#34;\u0026lt;nodeid\u0026gt;@\u0026lt;address\u0026gt;:\u0026lt;port\u0026gt;\u0026#34; 3. Choose an access name, by default this should be left blank, so hit \u0026rsquo;enter'\nIf you would like to choose your own access name, please be sure to only use lowercase letters. Including any uppercase letters will result in your access name not getting recognized when creating buckets.  Choose an access name [\u0026#34;default\u0026#34;]: 4. Enter the Access grant you generated:\nEnter your Access Grant: 1Dv41bZAnPGpR... 5. Create and confirm an encryption passphrase. This is used to encrypt your files on your machine before they are uploaded to the network:\nEnter your encryption passphrase: Please note that Storj does not know or store your encryption passphrase, so if you lose it, you will not be able to recover your files.  6. Your Uplink is configured and ready to use!\n"},{"id":153,"href":"/node/resources/faq/how-do-i-change-my-parameters-such-as-payout-address-allotted-storage-space-and-bandwidth/","title":"How do I change values like wallet address or storage capacity?","first":"/node/","section":"FAQ's","content":"How do I change values like wallet address or storage capacity? #  1. Stop and remove the running Storage Node Docker container (CLI) or stop the service (GUI Windows):\nCLI Install docker stop -t 300 storagenode docker rm storagenode GUI Windows Install Elevated PowerShell:\nStop-Service storagenode Or click the \u0026ldquo;Stop\u0026rdquo; button in the Windows Services applet on \u0026ldquo;Storj V3 Storage Node\u0026rdquo; service\n 2. Run your Storage Node again after editing needed parameters:\nCLI Install Parameters are described in the article Storage Node concepts. If you need to specify some parameters like a wallet options (i.e. zkSync) in the config.yaml, you can find it in the storage location).\nHow to run your Storage Node with modified parameters from the CLI: Running Storage Node\nGUI Windows Install Open the config file \u0026quot;%ProgramFiles%\\Storj\\Storage Node\\config.yaml\u0026quot; with a text editor (we recommend to use Notepad++, as the regular Notepad will not work) and modify needed parameters. Save the configuration file and restart the Storj V3 Storage Node service.\nOr in the elevated PowerShell:\nRestart-Service storagenode  "},{"id":154,"href":"/dcs/how-tos/mongodb-ops-manager-backup/","title":"MongoDB Ops Manager Backup","first":"/dcs/","section":"How To's","content":"MongoDB Ops Manager Backup #  Overview #  MongoDB is a powerful, flexible, and scalable general purpose database. It combines the ability to scale out with features such as secondary indexes, range queries, sorting, aggregations, and geospatial indexes.\nOps Manager is a management platform that makes it easy to deploy, monitor, back up, and scale MongoDB on your own infrastructure.\nMongoDB Enterprise supports a variety of cloud-native deployment options. This gives you and your apps access to locally deployed MongoDB clusters alongside direct access to MongoDB clusters running in the MongoDB Atlas cloud.\nFor the complete documentation for the service, see: https://github.com/jasonmimick/total-cluster\nBefore you begin #  If you haven\u0026rsquo;t yet, create a Storj DCS account before following the rest of the tutorial. Make sure to create an Access Grant and bucket.\nPrerequisites Procure a Kubernetes cluster #  Your first step is to procure a Kubernetes cluster. This full example will require 5 to 7 worker nodes with 2 nodes having at least 8 gb of ram.\nNext, set up your S3-compatible Gateway (formerly known as Tardigrade gateway) for total-cluster or you can use an our Storj-hosted S3 Compatible Gateway. Check out the details for that over in the gateway chart in total-cluster.\nInstalling the chart (Note: requires Helm V3),\n➜ addons git:(master) ✗ helm install s3-gateway tardigade-gateway NAME: s3-gateway LAST DEPLOYED: Sat May 30 07:52:43 2020 NAMESPACE: total-cluster STATUS: deployed REVISION: 1 TEST SUITE: None Here, we can inspect the cluster and see our local gateway running. This service provides a local secure connection for MongoDB Ops Manager to write database backups.\n ➜ addons git:(master) ✗ kubectl get all --selector=\u0026#39;app=s3-gateway-tardigrade-gateway\u0026#39; NAME READY STATUS RESTARTS AGE pod/s3-gateway-tardigrade-gateway-68fbf4b4d7-4qbvt 1/1 Running 0 104s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/s3-gateway-tardigrade-gateway-svc ClusterIP 10.43.65.192 \u0026lt;none\u0026gt; 7777/TCP 104s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/s3-gateway-tardigrade-gateway 1/1 1 1 104s NAME DESIRED CURRENT READY AGE replicaset.apps/s3-gateway-tardigrade-gateway-68fbf4b4d7 1 1 1 104s Once your gateway is setup and running, install the entire MongoDB data platform with the total-cluster chart.\ngit clone https://github.com/jasonmimick/total-cluster helm install mongodb total-cluster The entire chart takes between 5 to approx 10 mins to finish installing. This is because it’s installing quite a few components.\nOps Manager needs to be configured to use the gateway. You can connect to your Ops Manager with a basic port-forward command like this,\nkubectl port-forward mongodb-ops-manager-0 8080:8080 Connect your browser to localhost:8080 and use admin@mongodb.com and MongoDB12345% to login.\nNote: These credentials are stored in a secret, called mongodb-ops-manager-secret. You can update this in the chart. Also note the “mongodb” in the secret name comes directly from the Helm release name  You should see your MongoDB cluster once connected:\nSetting up the S3 Blockstore pointing to local gateway #  You can configure backups for your app databases following this guide: Backup Overview. \\\nFinally, you can see the backup data blocks stored in the Storj DCS bucket. You can even use the local tardigrade-gateway with command s3 command line tools:\n$ kubectl port-forward tardigrade-gateway-tardigrade-gateway-75cfdb889-nz2n4 7777:7777 \u0026amp; $ export AWS_ACCESS_KEY_ID=XXX $ export AWS_SECRET_ACCESS_KEY=XXX $ aws --endpoint-url http://localhost:7777 s3 ls test Conclusion #  We look forward to seeing what MongoDB users do with Storj DCS (formerly known as Tardigrade).\nWe regularly hear from users who would like cloud backup services that provide end-to-end encryption, multi-region redundancy, as well as lower and more predictable costs compared to centralized services.\nStorj DCS delivers on all of these needs, while still offering the same enterprise service level agreements users absolutely require.\nLearn more\n  MongoDB Ops Manager  Deploying MongoDB in Kubernetes   "},{"id":155,"href":"/dcs/concepts/multiregion-availability/","title":"Multiregion Availability","first":"/dcs/","section":"Concepts","content":"Multiregion Availability #  Breaking the Storj DCS service into multiple peer classes on the network provides the maximum flexibility to network participants. While the majority of the network is operated by third parties, the components and architecture are all designed to be inherently multi-region to meet strict SLAs for availability and durability.\nStorage Nodes #  One key to the durability of the network is the distribution of data over a heterogeneous network of statistically uncorrelated nodes. Since the storage nodes are operated by different people, in different regions, on different power supplies, with different internet connections, etc., the risk of failure is highly distributed. While the risk of failure for any individual node is very low, the risk of 51 out of 80 nodes failing simultaneously is astronomically low.\nSatellites #  While the network of Storage Nodes is inherently multi-region, customers frequently ask whether the Satellites that host the metadata and are critical to the storage and retrieval of data are also multi-region. Storj DCS Satellites operated by Storj are also inherently multi-region.\nA specific Satellite instance does not necessarily constitute one server. A Satellite may be run as a collection of servers and be backed by a horizontally scalable trusted database for higher uptime. Storj operates clusters of Satellites in regions, with all Satellites in a region sharing a multi-region, distributed back end. This configuration provides a highly resilient and available architecture in which the loss of any Satellite service, an entire Satellite or the unavailability of a facility hosting a Satellite has no impact on the availability of data stored on the network.\nGateways #  Storj also operates a global network of highly available, distributed S3-compatible gateways. Each gateway is operated in a high availability environment, typically in Equinix data centers, with BGP-enabled global routing, where the loss of any one gateway has no impact of the availability of data. Applications are always routed to the closest available gateway to optimized for low latency and high performance.\\\n"},{"id":156,"href":"/node/resources/faq/linux-static-mount/","title":"How do I setup static mount via /etc/fstab for Linux?","first":"/node/","section":"FAQ's","content":"How do I setup static mount via /etc/fstab for Linux? #  First, run the following command and find the name of the hard drive you wish to use (for example sda2)\nlsblk Once you find it, we will now get the Unique ID (UUID) of the hard drive\nlsblk -d -fs /dev/\u0026lt;location (example: sda2)\u0026gt; Copy the UUID, as well as the FSTYPE. We’ll need that later.\nNext, we will create a new directory in the /mnt folder where you want your Storj files to be stored; you can name this directory whatever you would like.\nsudo mkdir /mnt/\u0026lt;directory name\u0026gt; Next, we will add our hard drive to the etc/fstab file\nsudo nano /etc/fstab Add the following line at the end of the file:\nUUID=\u0026lt;your HD UUID\u0026gt; /mnt/\u0026lt;directory name\u0026gt; \u0026lt;FSTYPE\u0026gt; defaults 0 2 To save changes, press Ctrl-X, Y, Enter\nOnce saved, run the following command:\nsudo mount -a That’s it!\nIf you’d like to confirm, you can run this command again and your new mount point will be updated in the mount column\nlsblk -d -fs /dev/\u0026lt;location (example: sda2)\u0026gt; Congrats, you’ve successfully static mounted your hard drive!\n"},{"id":157,"href":"/dcs/how-tos/tesla-sentry-mode-teslausb/","title":"Store Tesla Sentry Mode \u0026 Dashcam videos on Storj DCS","first":"/dcs/","section":"How To's","content":"Store Tesla Sentry Mode \u0026amp; Dashcam videos on Storj DCS #  In this How To, we\u0026rsquo;ll demonstrate how to automatically transfer Tesla Sentry Mode and Dashcam video clips over WiFi to Storj DCS and make room for more videos the next day. We\u0026rsquo;ll use a Raspberry Pi (a small, low cost, low power computer) plugged into the USB port in the dashboard to store the video files. When the Tesla pulls into your garage at night, the Raspberry Pi will connect via WiFi and upload all the videos to Storj DCS, then clear off the drive for use the next day. This will also work for videos recorded in Track Mode if you have one of the performance models, and you can easily share any of the videos with your friends.\nWhat you\u0026rsquo;ll need #  You\u0026rsquo;ll need some hardware, some software and a Storj DCS account for this project.\nHardware required #  All in, you’re looking at right around $60 of hardware to get going (prices as of April 2021). Here’s the hardware you’ll need:\n  Raspberry Pi Zero W : ID 3400 : $10.00 (We used a different model, but this is better)  Adafruit Raspberry Pi Zero Case : ID 3252 : $4.75 (It should look good - you can 3d print your own for extra credit)  SanDisk 256GB High Endurance Video microSDXC Card $37 (Very important to have high quality storage with high write endurance. This gives you room for a few days in case you don’t connect to WiFi and won\u0026rsquo;t wear out too quickly)  USB Cable to plug into the car USB A to Micro-B - 3 foot long  Storj DCS cloud storage 150GB free and only $0.004 per GB-month after that! Secure, private and decentralized.  Optional hardware for easier setup:\n  Mini HDMI to HDMI Cable - 5 feet : ID 2775: $5.95 Makes it easier to set everything up by connecting the Pi to a monitor  Software required #  The code used in this tutorial is open source and uses, among other things, Rlcone which includes native support for Storj DCS. The GitHub Repository for the code is available at: https://github.com/marcone/teslausb and the project was originally described on the /r/teslamotors subreddit.\nStorj DCS Account #  If you have not yet signed up, please do so now.\nStep-by-step guide #  Using the software in the teslausb project with Storj DCS is a multipart process, we will accomplish the following:\n Generate Credentials for Storj DSC  This occurs through the storj.io website   Configure your Raspberry Pi with the teslausb kit  Part 1 - One-step setup  This is done by flashing a preconfigured Raspbian image and then filling out a config file.   Part 2 - Rclone Configuration with Storj DCS via hosted Gateway MT  This is done by accessing the Pi via SSH and installing/configuring Rclone      Sign Up for a Storj DCS Account #  If you have not yet signed up, please head here to sign up for the free tier.\nPlease consult Satellite Quickstart for details on setting up an account and using the Satellite Admin Console.  Generate Credentials to the Gateway MT #  Navigate to the Access page within your project and then click on Create Access Grant +. A modal window will pop up where you should enter a name for this access grant.\nAssign the permissions you want this access grant to have, (in this case, just select the default permissions) then click on Continue in Browser:\nEnter the Encryption Passphrase you used for your other access grants. If this is your first access grant, we strongly encourage you to use a mnemonic phrase as your encryption passphrase (The GUI automatically generates one on the client-side for you, as shown below.)\nThis passphrase is important! Encryption keys derived from it are used to encrypt your data at rest, and your data will have to be re-uploaded if you want it to change!\nPlease note that Storj does not know or store your encryption passphrase, so if you lose it, you will not be able to recover your files.\n Please save this Encryption Passphrase in a safe place as it will be required to decrypt and view your sentry mode videos from the web.  Click ** **Next to proceed to Access Grant generation. Please then save the Access Grant in a safe place. This is usable for other integration paths not covered in this guide.\nClick on the Generate S3 Gateway Credentials link above and then click on the Generate Credentials button as seen below.\nCopy your Access Key, Secret Key, and Endpoint to a safe location. We will use this information later to configure Rclone with the hosted Gateway MT.\nNow we have our credentials and can move on to configuring teslausb.\nOverview of credentials collected #   Encryption Passphrase  We will need this to view and/or retrieve clips in the browser later   Access Grant  Not used for this lab but please record it anyway for your reference   Gateway MT (S3) Credentials  Used to connect to the hosted gateway via Rclone  Access Key Secret Key Endpoint      Learn more about Access Management on Storj DCS and using Access Grants.  Configure teslausb #  Teslausb is the open source software that will run on the Raspberry Pi to send your videos to Storj DCS. Now that you have your gateway credentials for Storj DCS, you need to set up teslausb on the Raspberry Pi and configure it for use with those Storj DCS credentials.\nPart 1 - One-step setup #  You\u0026rsquo;ll find the one-step setup guide for testlausb in this GitHub repo: https://github.com/marcone/teslausb/blob/v2.5/doc/OneStepSetup.md\nQuick Instructions\n Download the latest release of the pre-built image from the repository’s releases page: https://github.com/marcone/teslausb/releases/latest Flash the image to the SD card you plan on using in your Raspberry Pi. For this we can use belena Etcher ( https://www.balena.io/etcher/) or any similar image flashing software, you can find some examples here. Mount the SD card to edit the initial configuration file located in the boot directory. Create a teslausb_setup_variables.confA sample config file is located in the boot folder on the SD card. The latest sample is also available from GitHub via pi-gen-sources/00-teslausb-tweaks/files/teslausb_setup_variables.conf.sample. Specifically, set the `ARCHIVE_SYSTEM` to `none`. This is because we need to boot the Pi to install some additional software to push our sentry clips in Storj DCS. Please also set your Wifi settings. Save the file when you are done editing.  Below is a small portion of the config file showing \u0026rsquo;export ARCHIVE_SYSTEM=none\u0026rsquo; as well as the wifi settings.\n# Variables for CIFS (Windows/Mac file sharing) archiving export ARCHIVE_SYSTEM=none export ARCHIVE_SERVER=your_archive_name_or_ip export SHARE_NAME=your_archive_share_name export SHARE_USER=username export SHARE_PASSWORD=password # the cifs options below usually don\u0026#39;t need to be specified # export SHARE_DOMAIN=domain # export CIFS_VERSION=\u0026#34;3.0\u0026#34; # export CIFS_SEC=\u0026#34;ntlm\u0026#34; # Wifi setup information. Note that Raspberry Pi Zero W only supports 2.4 GHz wifi. # If you are you are trying to connect to a network with a _hidden_ SSID, # edit /boot/wpa_supplicant.conf.sample and un-comment the indicated line. export SSID=\u0026#39;your_ssid\u0026#39; export WIFIPASS=\u0026#39;your_pass\u0026#39; Part 2 - Rclone Configuration #  Now that Raspbian is installed and configured, it\u0026rsquo;s time to set up Rclone, the software that will actually transfer the files from the Raspberry Pi to Storj DCS. (If you\u0026rsquo;re not familiar with Rclone, it\u0026rsquo;s Rsync for cloud storage.) Boot up that Pi and let\u0026rsquo; s keep it moving.\nThe Rclone Setup Guide for teslausb is available at: https://github.com/marcone/teslausb/blob/v2.5/doc/SetupRClone.md\nThe steps you need to follow are also provided below:\nSSH into the Pi, become root and remount the file system\u0026rsquo;s read-write:\nsudo -i /root/bin/remountfs_rw Install Rclone:\ncurl https://rclone.org/install.sh | sudo bash Configure Rclone with the settings and Storj DCS gateway credentials created above:\n# setup rclone rclone config # select n (New Remote) # name storj-dcs-us1-gateway # select 4 (4 / Amazon S3 Compliant Storage Provider) 4 # select 13 (13 / Any other S3 compatible provider) 13 # select 1 (1 / Enter AWS credentials in the next step \\ \u0026#34;false\u0026#34;) 1 # enter access key \u0026lt;access_key\u0026gt; # enter secret key \u0026lt;secret_key\u0026gt; # select 1 ( 1 / Use this if unsure. Will use v4 signatures and an empty region.\\ \u0026#34;\u0026#34;) 1 # enter endpoint (use your own endpoint, the example shows the Americas region gateway) https://gateway.us1.storjshare.io # use default location_constraint # use default ACL # edit advanced config n # review config and select default # quit config q Create a bucket tesla-m3-cam for saving clips:\n# make bucket rclone mkdir storj-dcs-us1-gateway:tesla-m3-cam Update your teslausb_setup_variables.conffile in the boot directory with the following information. Ensure your Wifi variables are also set. If your variables have any spaces, please put your variables in quotes like this: \u0026ldquo;variable\u0026rdquo;\n# Variables for rclone archiving export ARCHIVE_SYSTEM=rclone export RCLONE_DRIVE=storj-dcs-us1-gateway export RCLONE_PATH=tesla-m3-cam # The following is optional #export RCLONE_FLAGS=() Shut down your Pi and plug it into your Tesla: If you are using a RPi4, you only need to plug the Pi via its USB-C cable into one of the Tesla\u0026rsquo;s USB-C ports, this will provide power and data transfer. If using a RPi Zero W, you will need to use the usb/data USB port on the Pi.\nTo test that everything is set up correctly, you can navigate to the Pi’s web interface at \u0026lt;rpi ip address\u0026gt;:80. Enable Honk to Save Clips on your Tesla, and then under the Tools page of your Raspberry Pi’s web interface, click the Trigger archive/sync button.\nFollow along on the Archive log page to see your clips being acknowledged and pushed to Storj DCS.\nFinally, log in to your Storj DCS account and take advantage of the new file browser to manage and share your clips!\n"},{"id":158,"href":"/dcs/concepts/limits/","title":"Usage Limits","first":"/dcs/","section":"Concepts","content":"Usage Limits #  Usage Limits allow us to ensure a consistent level of service for all customers. We have limits for usage established per Project on all Storj DCS Satellites. All limits are set to default values as follows:\nPRO Account (Paid Tier) #  Credit Card Payment method #  Adding a credit card as payment method will result in your per project limits being automatically raised to Pro Account limits:\n 3 projects 100 buckets per project 25 TB storage per project 100 TB egress bandwidth per project 100 request per second rate limit  STORJ token Payment method #  However, if your only payment method on file is STORJ token, you can submit a request to increase your projects limits.\nNote that you could avoid having to wait for a manual project limit increase to be applied by adding a credit card as backup payment method. In this case, your usage would still get paid from your STORJ balance if it is sufficient to cover your outstanding charges. Only usage that exceeds what can be covered by your STORJ balance at time of invoicing would get charged to the credit card.  Free Tier #   1 Project 100 Buckets per Project 150 GB Storage per Project 150 GB Egress Bandwidth per Project 100 request per second rate limit 10,000 Segments per Month  If you would like to increase your limits to higher values and your only payment method is STORJ token, you may contact our support team through the Storj DCS support portal.​\nRationales behind limits #  Storage and bandwidth limits are imposed by most cloud infrastructure providers as a normal part of capacity planning and to ensure achievement of SLAs. In distributed and decentralized storage systems they are equally important, if not more so. Just like any provider, the aggregate amount of available storage and bandwidth must be shared across all users. With a distributed and decentralized storage system like Storj DCS, the storage and bandwidth are provided by a network of third parties running storage node software. One of the key aspects to success is the balance of supply and demand. If there are too many users over-utilizing available resources, the user experience will be poor.\nIf there are too many storage nodes, there won’t be enough use to provide a meaningful ROI for Storage Node Operators. This can lead to storage node churn, increasing load on the network, and potentially impacting durability. Usage limits are one of the tools that maintain the balance.\nWe set rate limits between the uplink and the satellite to ensure a good quality of service for all uplink users. Without the rate limit it would be possible for users to inadvertently consume most of the database resources available on the satellite and cause issues for other users. We selected a limit that would be mostly unnoticed by end users (as the typical use case shouldn’t hit the limit). The current default limits are based on requests per second for all meta info calls: list, get, delete, put.\nWe set the default limits for the number of buckets per project to ensure performance.\nIn addition, we limit the number of Projects per Developer Account to minimize complexity.\nWe have also set the default limit for the number of segments to a level that is healthy for the network. Note that increasing the Segment Project Limit may incur additional fees. Read more about Usage Limit Increases.\nCustomers can request a limit increase when needed by filling out the limit increase request form on our Storj DCS support portal if their only payment method on file is STORJ token.\nAn automatic limit increase to Pro Account can be accomplished by adding a credit card as payment method. Please only make such requests if your use case really requires more than the current default limits. Requests will be evaluated taking into account the intended use case and availability on the network.\n"},{"id":159,"href":"/node/resources/faq/how-do-i-estimate-my-potential-earnings-for-given-amount-of-space-and-bandwidth/","title":"How do I estimate my potential earnings for a given amount of space and bandwidth?","first":"/node/","section":"FAQ's","content":"How do I estimate my potential earnings for a given amount of space and bandwidth? #  We have a great Community Earnings Estimator to give you an idea how much time it may take to fill your provided free disk space and how much you may earn.\nPlease be aware - this is only an estimator, not a calculator. Unlike for mining cryptocurrencies, the estimated potential earnings are not guaranteed because the space and bandwidth are used by real people, not machines.\n"},{"id":160,"href":"/dcs/how-tos/how-to-connect-s3fs-to-storj-dcs/","title":"How to connect s3fs to Storj DCS","first":"/dcs/","section":"How To's","content":"How to connect s3fs to Storj DCS #  Prerequisites #  Before you get started, you need to have already made a satellite account, created an access grant and registered on the self-hosted S3-Compatible Gateway or generated credentials for Storj-hosted S3-Compatible Gateway.\nInstall s3fs #  See https://github.com/s3fs-fuse/s3fs-fuse for installation instructions for your OS.\nConfigure s3fs to use Storj DCS S3 Gateway #  You can use either Self-hosted S3 Compatible Gateway or Storj-hosted S3 Compatible Gateway. You only need S3 compatible credentials and endpoint.\nCreate the credentials file #  Please substitute your own Access Key ID instead of ACCESS_KEY_ID and your own Secret Access Key instead of SECRET_ACCESS_KEY in the example below:\necho ACCESS_KEY_ID:SECRET_ACCESS_KEY \u0026gt; ${HOME}/.passwd-s3fs chmod 600 ${HOME}/.passwd-s3fs Create a bucket #  You can create a bucket with uplink, with AWS S3 CLI or with Objects browser.\nMount a bucket to the folder #  Create a folder:\nsudo mkdir /mnt/my-bucket sudo chown myuser /mnt/my-bucket Mount a bucket to the folder. We will assume that you created a bucket my-bucket earlier. We will use the endpoint https://gateway.us1.storjshare.io here, but you can choose the right one for you.\nIf you use the Self-hosted S3 Compatible Gateway, then the endpoint could be http://localhost:7777 (depends on your configuration and infrastructure).\ns3fs my-bucket /mnt/my-bucket -o passwd_file=${HOME}/.passwd-s3fs -o url=https://gateway.us1.storjshare.io/ -o use_path_request_style Mount a bucket to the folder on boot #  You should make sure that the credentials file is available for the root on boot time. You can create it in the /etc/ folder:\nsudo echo ACCESS_KEY_ID:SECRET_ACCESS_KEY \u0026gt; /etc/passwd-s3fs sudo chmod 600 /etc/passwd-s3fs Then add the following to /etc/fstab:\nmy-bucket /mnt/my-bucket fuse.s3fs _netdev,allow_other,use_path_request_style,url=https://gateway.us1.storjshare.io/ 0 0 Check that it is working - The command:\nsudo mount -a should not return any error. Next, check that your Storj DCS bucket is mounted:\ndf -HT Now you can use the mounted bucket almost as any folder.\nSee also #  We would recommend to have a look at rclone and its rclone mount command as well.\nPlease note - you can configure a native connector in rclone and use End-to-end Encryption, unlike Storj-hosted S3 Compatible Gateway which uses Server-side Encryption to provide a S3-compatible protocol (the S3 protocol does not use client side encryption by design).\n"},{"id":161,"href":"/dcs/concepts/satellite/","title":"Satellite (Metadata Region)","first":"/dcs/","section":"Concepts","content":"Satellite (Metadata Region) #  The Satellite is a set of hosted services that is responsible for a range of functions on the network, including the node discovery system, node address information caching, per-object metadata storage, storage node reputation management, billing data aggregation, storage node payment, data audit and repair, as well as user account and authorization management.\nUsers have accounts on and trust specific Satellites. Any user can run their own Satellite, but we expect many users will choose to avoid the operational complexity and create an account on another Satellite hosted by a trusted third party such as Storj Labs, a friend, group, or workplace. Storj Labs satellites are operated under the Storj DCS brand. This component has a number of key responsibilities:\n Developer account registration \u0026amp; management API credential \u0026amp; access management Object metadata storage Billing \u0026amp; payment Audit \u0026amp; repair  Users of the network will have accounts on a specific Satellite instance, which will: store their file metadata, manage authorization to data, keep track of storage node reliability, repair and maintain data when redundancy is reduced, and issue payments to storage nodes on the user’s behalf.\nChoosing a metadata region #  While Storj DCS doesn\u0026rsquo;t have \u0026ldquo;regions\u0026rdquo; like other cloud storage providers who operate data centers in one or more geographic location, the closest thing to a \u0026ldquo;region\u0026rdquo; is a satellite. While your data is stored across a globally distributed network of storage nodes, the encrypted metadata is stored across multiple satellites in a region.\nWhen selecting the Satellite for your project, you\u0026rsquo;ll want to choose the geographic region where the majority of the end users of your service who will be interacting with the objects on Storj DCS will be located.  Importantly, a specific Satellite instance does not necessarily constitute one server. A Satellite may be run as a collection of servers and be backed by a horizontally scalable trusted database for higher uptime. Storj operates clusters of Satellites in regions, with all Satellites in a region sharing a multi-region, distributed back end. This configuration provides a highly resilient and available architecture in which the loss of any Satellite service, an entire Satellite or the unavailability of a facility hosting a Satellite has no impact on the availability of data stored on the network.\nStorj implements a thin-client model that delegates trust around managing files’ location metadata to the Satellite service which manages data ownership. Uplinks are thus able to support the widest possible array of client applications, while Satellites require high uptime and potentially significant infrastructure, especially for an active set of files. Like storage nodes, the Satellite service is being developed and will be released as open source software. Any individual or organization can run their own Satellite to facilitate network access.\nWith respect to customer data, the Satellite is never given data unencrypted and does not hold encryption keys. The only knowledge of an object that the Satellite is able to share with third parties is its existence, rough size, and other metadata such as access patterns. This system protects the client’s privacy and gives the client complete control over access to the data, while delegating the responsibility of keeping files available on the network to the Satellite.\nUplink Clients may use Satellites run by a third-party. Because Satellites store almost no data and have no access to keys, this is a large improvement over the traditional data-center model. Many of the features Satellites provide, like storage node selection and reputation, leverage considerable network effects. Reputation data sets grow more useful as they increase in size, indicating that there are strong economic incentives to share infrastructure and information in a Satellite.\nThe Satellite instance is made up of these components:\n A full node discovery cache A per-object metadata database indexed by encrypted path An account management and authorization system A storage node reputation, statistics, and auditing system A data repair service A storage node payment service  "},{"id":162,"href":"/dcs/how-tos/how-to-set-up-video-analytics-and-video-management-with-kerberos-vault/","title":"How to set up video analytics and video management with Kerberos Vault","first":"/dcs/","section":"How To's","content":"How to set up video analytics and video management with Kerberos Vault #  Integration with Kerberos.io #  The Kerberos.io project (not to be confused with the authentication protocol of the same name) offers an open-source platform for video analytics and monitoring. With a modular system design focused on minimal startup requirements and additional components available to add later, Kerberos.io is built to work for everything from small, personal systems to complex enterprise configurations. This makes it a useful solution that is relevant at all scales.\nThe modularity of Kerberos.io includes optional components that help integrate it into any cloud architecture. One of these components is Kerberos Vault, which provides a flexible and extensible storage solution for video files. Kerberos Vault is designed to work with several different cloud providers, including Storj DCS, to allow for customized storage options where users can bring their own providers.\nStorj DCS and Kerberos #  Storj\u0026rsquo;s decentralized cloud storage platform offers a great video storage backend for integration with Kerberos.io and Kerberos Vault. This is because the distributed storage design that Storj DCS is built on offers both high availability access to video files (thanks to its network of nodes across multiple regions) and secure, reliable hosting with no single-point-of-failure.\nConveniently, it is fairly simple to configure Storj DCS to work with Kerberos Vault. This doc will show the steps necessary to do so.\nPrerequisites #  Before starting the steps in this outline, ensure you have the following:\n A Storj account. You can create your Storj DCS account for free at Storj.io/signup.\n A Kubernetes cluster. Kerberos.io is best deployed as a container in Kubernetes. You can create a Kubernetes cluster locally or on any service provider that offers Kubernetes such as Google Cloud\u0026rsquo;s GKE or Amazon EKS. Kerberos Vault installed in your Kubernetes cluster. Kerberos.io provides documentation on how to install Kerberos Vault in Kubernetes, both for public and private cloud options.  With these prerequisites satisfied, we can begin configuring Kerberos Vault to use Storj in our Kubernetes cluster.\nCreating a Storj Bucket and Access Credentials #  The first step in configuring Storj as the storage backend for Kerberos.io is to create a bucket in your Storj DCS account and generate S3 access credentials for the bucket.\nKerberos Vault will then use the bucket information and access grant to connect with Storj.\nFor this, do the following steps:\n Log in to your Storj DCS account On the main dashboard, click \u0026ldquo;New Bucket\u0026rdquo; Give your bucket a descriptive name in the text box, for example \u0026ldquo;kerberosvault\u0026rdquo; Click Create Bucket Once back on the main dashboard, select your new bucket On the left-hand side, click Access In the top-right, click Create Access Grant In the new window, give your access grant a descriptive name, for example \u0026ldquo;kerberoskey\u0026rdquo;, and click Next Choose the appropriate permissions you wish to grant Kerberos Vault for this bucket:  Download / Update / List / Delete - these are the actions that Kerberos.io will be able to perform Duration - this is the time until this access grant will expire Buckets - this sets which bucket (or buckets) Kerberos.io will have access to.   Click Continue in browser Enter your encryption passphrase for this access grant and click Next Save the access grant key which is displayed and click Generate S3 Gateway Credentials On the next page, click Generate Credentials Save the Access Key, Secret Key, and End Point to use with Kerberos Vault in a safe place  After completing these steps, you are ready to configure Kerberos Vault with your new bucket\u0026rsquo;s access credentials.\nConfiguring Kerberos Vault to Use Storj #  Now it is time to tell Kerberos where to store videos (your Storj bucket) and how to access that location (with the access grant created above). These steps can be completed from the Kerberos Vault web panel within your running instance of Kerberos inside a Kubernetes cluster.\n From the main Kerberos Vault dashboard, select Storage Providers (on the left menu) In the window that pops up, in the drop-down menu under Select Storage Provider, choose Storj. Under Provider Name, enter a descriptive name for this provider to be referred to in your Kerberos instance (for example, \u0026ldquo;storjdcs\u0026rdquo;) For Bucket Name, enter the same bucket name as the one created above (in this tutorial, that would be \u0026ldquo;kerberos-vault\u0026rdquo;) Region this is not relevant for Storj or an edge deployment and can be left blank Hostname is the gateway hostname (without https://) for your Storj bucket\u0026rsquo;s region (for example, \u0026ldquo;gateway.us1.storjshare.io\u0026rdquo;) Under Storj Credentials, enter the Access Key and Secret Key you saved earlier when creating your access credentials. Finally, click Validate to ensure your access is correct and Add Integration to finish setup.  Summary #  The flexibility of Kerberos.io and its components like Kerberos Vault are what make it a versatile platform for video monitoring and analytics. From single-camera setups to advanced cloud-based enterprise installations, the Kerberos.io video technology is adaptable to any configuration. This adaptability includes the option to customize your choice for video storage with the platform, which is where Storj DCS makes an excellent choice.\nIn this tutorial, we demonstrated the steps to set up a Storj bucket and create access credentials for that bucket. We then showed how to update a Kerberos Vault installation to use Storj as a storage provider. Doing all of this allows Kerberos.io to leverage the distributed storage network provided by Storj DCS, taking advantage of all the benefits it provides. Your video monitoring and analytics solution is now enhanced with the power of decentralized media storage.\n"},{"id":163,"href":"/dcs/concepts/s3-compatibility/","title":"S3 Compatibility","first":"/dcs/","section":"Concepts","content":"S3 Compatibility #  The Storj DCS S3-compatible Gateway supports a RESTful API that is compatible with the basic data access model of the Amazon S3 API.\nThe Storj DCS S3 Gateway is well-suited for many application architectures, but the S3 standard was designed for centralized storage and there are a few areas where a decentralized architecture requires a different approach.\nStorj DCS offers two options for S3 compatibility: GatewayMT, a hosted S3-compatible service and GatewayST, a self-hosted S3-compatible binary to run your own endpoint (see the compatibility table for GatewayST in our GitHub repo).\nWhen to use GatewayMT #  If you have an existing application that is using an S3-compatible object storage service and you want to switch to Storj DCS, the easiest way to switch is to use the hosted S3-compatible service. The main design decision you need to be aware of is that GatewayMT uses server-side encryption. You can learn about the supported commands and endpoints for S3 compatibility under the SDK \u0026amp; Reference section for the S3 compatible gateway.\nWhen to use GatewayST #  If you have an hybrid cloud architecture, are working with on-premise data, or have other needs to host your own S3-compatible object storage service you may want to use the self-hosted GatewayST. The two main design decisions you need to be aware of are that:\n GatewayST uses end-to-end encryption. When you host your own Gateway, that gateway is handling the erasure coding and direct peer-to-peer transfer of data to storage nodes. You will need to account for the upstream bandwidth associated with the expansion factor related to erasure coding the data and any associated overhead related to concurrent connections with storage nodes related to parallel transfers.  You can learn about the supported commands and endpoints for S3 compatibility under the SDK \u0026amp; Reference section for the S3 compatible gateway.\n"},{"id":164,"href":"/node/resources/faq/storing-more-data/","title":"Why am I not storing more data?","first":"/node/","section":"FAQ's","content":"Why am I not storing more data? #  The most important aspect to increase the amount of data stored on your Node (and thus maximizing payout) is to build reputation of your Node’s ID over an extended period of time**.**\nWhen a Node first joins the network, there is a probationary period, during which the Node has to prove itself (e.g. maintaining a certain uptime and performance levels, passing all content audits). During that vetting period, the Node only receives as small amount of non-critical data (but still gets paid for this data). Once vetted, a Node can start receiving more data (and not just test data), but must continue to maintain uptime and audit requirements to avoid disqualification.\nThe filtering system blocks bad Storage Nodes from participating. Additional actions that are disqualifying include: failing too many audits; failing to return data, with reasonable speed; and failing too many uptime checks.\nAfter disqualified Storage Nodes have been filtered out, remaining statistics collected during audits are used to establish a preference for better Storage Nodes during uploads. These statistics include performance characteristics such as throughput and latency, history of reliability and uptime, geographic location, and other desirable qualities. They are combined into a load-balancing selection process, such that all uploads are sent to qualified Nodes, with a higher likelihood of uploads to preferred Nodes, but with a non-zero chance for any qualified Node.\n"},{"id":165,"href":"/dcs/how-tos/how-to-hod-rod-file-transfer-performance-on-storj-dcs/","title":"How to Hod Rod File Transfer Performance on Storj DCS","first":"/dcs/","section":"How To's","content":"How to Hod Rod File Transfer Performance on Storj DCS #  Many of the performance benefits of decentralized storage are achieved through distributed, redundant segmentation of files on the network. This redundancy and segmentation allows platforms like Storj to implement parallelism in the network for an even greater performance boost. In the context of file transfers, parallelism is the concept of uploading or downloading different pieces of a file simultaneously (in parallel). As a user, there are ways to optimize these parallel transfers in order to achieve more efficient throughput.\nThe exact optimal settings for parallel file transfers vary based on your own compute resources and network bandwidth. The size of the files being transferred also influences the optimal configuration, as this can affect which tools are capable of providing improved performance at that size. To provide an example walkthrough for a single approach, this section will demonstrate how to achieve better performance for the transfer of many small- to medium-sized files using Rclone.\nParallelism on Storj #  Storj DCS achieves decentralization by breaking files into chunks and distributing those chunks among nodes. First, files are broken into 64MB segments. Each segment is then distributed as 80 smaller pieces to different nodes. These chunks of segments and pieces are the fundamental ideas that form the basis for parallelism in file transfers with the Storj network.\nWhen files are transferred between your local system and the Storj network, each segment of the file is usually sent to the network serially. However, within each segment, the individual pieces are transferred in parallel. This is Storj\u0026rsquo;s Base Parallelism model for parallel transfer.\nThere is another parallelism model on top of Base Parallelism that can also be used for improved performance. In this approach, the 64MB segments of a file are also transferred in parallel. This is known as Segment Parallelism, and it is how to achieve the fastest transfer throughput.\nWith these two concepts in mind, we can begin considering the available tools for uploading and downloading files on the Storj network.\nTools #  The two main tools available to transfer files between a local system and Storj DCS are Uplink CLI and Rclone.\nUplink is Storj\u0026rsquo;s command line tool for transferring directly with the Storj network. As a native CLI tool, Uplink is able to bypass the Storj network\u0026rsquo;s edge service layer and connect directly to nodes on the network. This allows for more efficient transfers and higher throughput.\nHowever, Uplink is limited in some of its options for parallelism. For instance, when files are uploaded to Uplink, erasure coding occurs on the client side resulting in a 2.68x upload multiplier. This means that users have to upload 2.68 times the original data to use the Uplink service. Further, when uploading 64MB or smaller files, multi-transfers need to occur creating the parallelism effect, since Uplink does not support moving more than one file at a time.\nSo, for the purposes of demonstrating uploads and downloads for many smaller files, we will use Rclone. Rclone is a tool intended to manage files on various cloud storage providers, and it works very well with Storj, too. While files smaller than 64MB will still not be able to take advantage of segment parallelism, Rclone offers the ability to process multiple file transfers at once.\nUploading files with Rclone #  When working with small and medium-sized files, the optimal parallelism is limited by the segment, or \u0026ldquo;chunk\u0026rdquo;, size. With Rclone and usage of Gateway MT, this segmentation is referred to as \u0026ldquo;concurrency.\u0026rdquo; So for example, a 1GB file would be optimally uploaded to Storj with the following command:\nrclone copy --progress --s3-upload-concurrency 16 --s3-chunk-size 64M 1gb.zip remote:bucket In this example, there are a couple additional flags. Here\u0026rsquo;s what those do:\n --progress shows the current progress of the transfer --s3-upload-concurrency sets the number of segments to transfer with. Since this is a 1GB file, it can only be broken into a maximum of 16 segments when each segment is 64MB (1024/64=16). --s3-chunk-size defines the size of the segments for this transfer. Since the Storj network uses 64MB chunks, that is the size set here.  With larger files, your local hardware starts to become the limiting factor for transfer speed. Multiple concurrent segment transfers each require their own chunk of memory to complete, so determining peak performance relies on calculating the amount of memory that can be dedicated to the transfer. Since the segment size is a constant 64MB, the rest of the math is simple.\nFor example, a 10GB file could theoretically be transferred with 160 concurrency segments, since 64MB * 160 equals 10GB. However, this optimal parallelism requires enough memory that matches the size of the file (10GB). So it may not actually be the best option on every system.\nRclone also offers the advantage of being able to transfer multiple files in parallel with the --transfers flag. For example, multiple 1GB files could be transferred simultaneously with this command, modified from the single file example above:\nrclone copy --progress --transfers 4 --s3-upload-concurrency 16 --s3-chunk-size 64M 1gb.zip remote:bucket This command sets --transfers 4 to upload 4 files at once. With a concurrency of 16 and our constant chunk size of 64MB this command will consume 4GB of RAM to complete.\nThe relationship of constant chunk size to variable file size is the determining factor for realizing peak performance. While there is no single solution to optimize all scenarios, experimenting with different numbers of concurrency segments allows for measurable performance improvements. These principles also apply to downloads.\nDownloading Files with Rclone #  The same basic mathematical calculations for uploads are also relevant for downloads. However, since the Uplink CLI supports parallelism with downloads, it is often the better choice for performance. This can be achieved using the --parallelism flag, as shown below:\nuplink cp sj://bucket/bighugefile.zip ~/Downloads/bighugefile.zip --parallelism 4 Because Uplink bypasses the Storj edge network layer, this is the best option for downloading large files. This example command sets --parallelism 4 which will consume 4 CPU cores to download the file.\nWith small files, Rclone with a native integration is still the best option to use for downloads as well. This is again thanks to the --transfers flag that allows Rclone to download multiple files in parallel, taking advantage of concurrency even when files are smaller than the Storj segment size. To download 10 small files at once with Rclone, the command would be:\nrclone copy --progress --transfers 10 remote:bucket /tmp Summary #  Decentralized cloud storage provides benefits for distribution and highly available access. We have shown in this article how to leverage these storage advantages with transfers as well as using parallelism.\nParallel transfers, either of single large files or multiple concurrent file uploads, demonstrate the highest performance possibilities. In fact, the only limiting factors for these transfers are often the network and hardware available to the user. By experimenting with different parallelism settings for different environments, you can see this optimal performance in your own transfers.\nYou can also read a more detailed explanation here: https://forum.storj.io/t/hotrodding-decentralized-storage/15228\nSee also #    Hot Rodding Decentralized Storage Part 1  Hot Rodding Decentralized Storage - Part 2  Hot Rodding Decentralization - Part 3  "},{"id":166,"href":"/node/resources/faq/storage-bandwidth-usage/","title":"Why is the network not using all of my storage and bandwidth?","first":"/node/","section":"FAQ's","content":"Why is the network not using all of my storage and bandwidth? #  The quick answer is, your Node\u0026rsquo;s reputation is still growing. The Node isn\u0026rsquo;t yet storing stripes of frequently accessed pieces. Luckily for Storage Node Operators, we’ve optimized the network to provide the maximum amount of bandwidth allocation based on the operation of a single node in a bandwidth-constrained environment.\nThis means that running a single Node per location (where one location = a discrete network with separate IP address and bandwidth/bandwidth cap) will yield the best results (highest reputation, most storage contracts from satellites and most earned STORJ tokens) for Storage Node Operators.\nBandwidth volume measures how much storage can be uploaded and downloaded from a Storage Node. Upload bandwidth speed is critical to delivering data to developers, partners, and other companies/ individuals storing data on the network, with low latency. Fast upload bandwidth has the potential to greatly impact earnings for storage node operators.\n"},{"id":167,"href":"/dcs/how-tos/backups-with-hashbackup/","title":"Backups with HashBackup","first":"/dcs/","section":"How To's","content":"Backups with HashBackup #  One of the most important things you can do to maintain the integrity and accessibility of your files is to routinely perform backups. Data backups are a familiar concept for many, allowing for efficient recovery from lost or damaged files.\nThe best way to benefit from data backups is to do them regularly. However, frequent backups can quickly start requiring significant storage space. This is especially true for long-term backups. They also create copies of your files, which must be securely protected just as much as with your current live files.\nThese are some of the problems that HashBackup tries to solve. HashBackup addresses the issues of security and storage by not only encrypting backups but also compressing them. This approach offers an efficient, reliable solution for performing backups regularly which doesn\u0026rsquo;t need compromise due to storage or compute limitations.\nThe benefits of HashBackup can be enhanced with the supplemental storage and accessibility offered by Storj DCS. This document will describe how to configure Storj as a backup destination for HashBackup.\nRunning HashBackup with Storj as a Destination #  While HashBackup compresses backups to limit the storage requirements necessary for local storage, there are still use cases for backing up data externally to cloud services like Storj. For example, the decentralized network of Storj DCS nodes increases availability and security when accessing your backups.\nHashBackup allows the configuration of additional storage destinations through a dest.conf file which contains information about the destination and access keys to upload data. Before that, however, you must first ensure that HashBackup is properly installed to perform local backups.\nTo install HashBackup on your local system, follow these steps as shown in the HashBackup docs:\n Download and extract the hb installer binary for your system architecture from the HashBackup Downloads page Run the binary with ./hb to install HashBackup Move the hb binary to your local search path (on Linux, sudo mv hb /usr/local/bin/)  Now, we must create a bucket in Storj DCS that will hold the backup data uploaded by HashBackup. While doing this, it will be important to also create access keys for the bucket which can be used by HashBackup to securely access the Storj bucket.\n If you haven\u0026rsquo;t already, create an account at Storj.io/signup. Once logged in, select a project and choose Buckets, followed by Create New Bucket. Give the bucket a descriptive name, for example \u0026ldquo;hashbackup\u0026rdquo;. When the bucket has been created, go to the Access panel on the left-hand side and choose Create Access Grant. Give the new access grant a descriptive name and appropriate permissions. When you are finished, choose Generate S3 Gateway Credentials. Save the Access Key, Secret Key, and End Point in a safe location as these will be used later to authenticate HashBackup with Storj.  Now it is time to create the local backup directory with HashBackup. This step will initialize a local HashBackup folder which could normally be used to maintain encrypted backups on your personal machine. However, we will provide the additional configuration necessary to enable remote backups with Storj.\n Create a backup directory with hb init -c backup (this creates a folder called backup which will hold all of your backup config settings) Create a file called dest.conf within the backup directory  The dest.conf file will hold all of the external configuration details as listed in the HashBackup S3 Destination docs. For Storj DCS, this file should look as follows:\ndestname storj type s3 host \u0026lt;End Point\u0026gt; partsize 64m secure accesskey \u0026lt;Access Key\u0026gt; secretkey \u0026lt;Secret Key\u0026gt; bucket \u0026lt;Bucket Name\u0026gt; This file contains a few key definitions:\n destname: this is the destination name which will be referred to by HashBackup type: should be set to \u0026ldquo;s3\u0026rdquo; for Storj host: this points to the gateway endpoint created with our S3 gateway access credentials, for example \u0026ldquo;gateway.us1.storjshare.io\u0026rdquo;. partsize: Storj processes files in 64MB segments, so declaring this value here (as \u0026ldquo;64m\u0026rdquo;) optimizes backups to work with Storj accesskey: this is the Access Key saved from earlier when creating the S3 gateway credentials for our access grant secretkey: this is the Secret Key from earlier bucket: the name of the bucket in Storj DCS, for example \u0026ldquo;hashbackup\u0026rdquo;  With HashBackup now configured to send remote backups to Storj, any new files that are backed up will create a local copy as well as a remote copy. For example, running the following command will back up a local directory called data both in the local backup folder as well as in the hashbackup Storj DCS bucket we just configured:\nhb backup -c backup data If the local backup is lost, damaged, or pruned but later needed, the remote backup can be recovered from Storj following the standard destination recovery steps documented on HashBackup\u0026rsquo;s website.\nConclusion #  Storj DCS offers an excellent option for decentralized, distributed, cloud-based file storage. With a global distribution of network nodes and inherently secure decentralized access, Storj is a great platform for hosting files and data. These same benefits extend to backups of data, which is where an automated tool like HashBackup can combine with Storj to ensure that your files are always accessible locally. In this article, we showed how to do just that by installing HashBackup and configuring it to use Storj as a remote destination. Now, the power of encrypted and compressed backups are able to be hosted on the distributed Storj platform, with all of the benefits that brings.\n"},{"id":168,"href":"/node/resources/faq/low-payouts/","title":"Why are my payouts so low?","first":"/node/","section":"FAQ's","content":"Why are my payouts so low? #  If you believe you should have gotten higher payouts, please run the earnings estimator and then compare each Satellite’s payouts to the amount you received in the transactions sent to your payout address for the month in question. Please wait to submit any support ticket until we have announced that all payouts have been completed for the month. Payouts are sent monthly, in the first two weeks after the month being paid is done. Please also consider that part of your payouts will be held back during the first nine months of operation, as explained here.\nThe bottom section of the earnings estimator shows the amount you can expect to receive immediately vs the amount held back, depending which stage you\u0026rsquo;re in (months 1-3, months 4-6, or months 7-9). If your Node has been running more than nine months already, you\u0026rsquo;ll receive the full payout with no held back amount.\nYour payouts could also be low because your Node didn\u0026rsquo;t receive much traffic that month. Please note that we cannot guarantee a certain amount of data being constantly received by your Node, as real customers are uploading and downloading their files when they need to for their own purposes. This is not like cryptocurrency mining where if you provide a certain amount of bandwidth you would have the bandwidth completely saturated at all times. Make sure your Node has a good internet connection so it\u0026rsquo;s online 24/7 and is not failing audits, which could put it in containment mode (and not receive more data until it no longer fails audits).\n"},{"id":169,"href":"/node/resources/faq/estimate-payouts/","title":"How do I estimate my payouts per Satellite?","first":"/node/","section":"FAQ's","content":"How do I estimate my payouts per Satellite? #  If you would like to estimate how much you can expect to get paid for running your Node during a given month, please follow the instructions here.\nThis information is also available on the Web-dashboard (docker) or Web-dashboard for Windows/Linux GUI.\nThe script above can be used to check the estimations displayed on the dashboard and correctness of payout. If your node is working properly, the values for both sources should almost be the same. If you find significant discrepancies - please ask on the forum.\nPlease note that this script won\u0026rsquo;t give you values as exact as what is shown on the dashboard until the payout is finished for the month in question; your actual payout may be slightly different from what you calculated for each Satellite. Also note that the script will estimate what payout you\u0026rsquo;ll receive depending on how long you already have been running the Node on a Satellite. This also takes into account the amount withheld during the initial months which is not immediately paid out.\nPlease see more details about held amounts in this blog post.\nSee also Storage Node Operator Payout Information regarding the minimum payout threshold and payment options.\n"},{"id":170,"href":"/dcs/how-tos/how-to-use-plex-and-storj-dcs-as-a-private-streaming-service/","title":"How to use Plex and Storj DCS as a private streaming service","first":"/dcs/","section":"How To's","content":"How to use Plex and Storj DCS as a private streaming service #  Video Storage #  Managing a multimedia library can be tedious and time-consuming. Organizing your saved movies and TV shows into a streamable format requires effort to index, sort, and optimize the video files for playback. Fortunately, programs like Plex exist to make this easier. Plex is a media player platform that provides clients for desktop, mobile, and TV devices. These clients can stream from online services or your own downloaded videos.\nVideo Playback from Anywhere #  With the Storj DCS decentralized video storage service, your multimedia objects are available anywhere, at any time. Decentralized video storage offers many advantages over traditional centralized storage. These include higher availability, better security, and a lower operating budget.\nWith decentralized storage, your videos are distributed across a global network of nodes. This ensures more reliable access with fewer single points of failure. For that same reason, your files are not vulnerable to the security measures that may or may not exist for one central data repository. Additionally, Storj\u0026rsquo;s network eliminates many of the maintenance and engineering costs of centralized object storage services.\nThe ability to store and retrieve your video files from anywhere using Storj\u0026rsquo;s global network is great for hosting the files. But what about streaming and playback? Connecting Storj as the multimedia backend to a service like Plex would give the advantages of both services. Specifically, that means we want reliable, decentralized storage with convenient streaming playback. Thankfully this is all possible!\nSetting up Storj with Plex Using Rclone #   Rclone is a program that helps with the management of files on various cloud storage services. It provides a command called rclone mount that allows files on cloud storage to be mounted as a local filesystem.\nFor this guide, we are going to show how to set up Rclone to sync your local files with Storj DCS. This can then serve as your storage backend for a service like Plex.\nGetting Started #  You should start by reading the Storj docs on setting up Rclone integration. Follow the steps to configure your Storj access credentials using the rclone config command.\nThese docs show how to run basic commands to sync files from your local system to Storj buckets, but we would like to go a step further and mount a Storj bucket as its own filesystem. This will provide persistent storage for our video streaming service.\nMounting the Filesystem #  The rclone mount command can be used to mount a cloud storage service as a local filesystem. With this enabled, Rclone can bidirectionally copy new files to Storj and read them from your buckets for playback in the Plex client.\nMount your file directories with this command:\nrclone mount Storj:media X:\\ --vfs-cache-mode full --dir-cache-time 1h --read-only --no-checksum --no-modtime --rc This command does a few things:\n rclone mount Storj:media X:\\ maps to rclone mount \u0026lt;remote\u0026gt;:\u0026lt;path/to/files\u0026gt; \u0026lt;path/to/local/mount\u0026gt;, where \u0026lt;remote\u0026gt; is the Storj interface configured with Rclone following the steps in Getting Started, \u0026lt;path/to/files\u0026gt; is the location in the Storj bucket where media files will be hosted, and \u0026lt;path/to/local/mount\u0026gt; is the directory on your local system that the Storj service should be mounted as. --vfs-cache-mode full sets the VFS file cache settings for optimal file cache buffering --dir-cache-time 1h sets the directory cache time to one hour --read-only specifies that the mount should be read-only --no-checksum disables checksums for better performance --no-modtime prevents modification times from being written so that Plex doesn\u0026rsquo;t pick them up as changed files --rc enables remote control of Rclone via its API. This will be used to issue commands to sync the Rclone mount below.  Syncing New Files to Storj With Rclone #  Even though your Storj bucket is now mounted to your local filesystem, you may want to add new files to your Plex setup. In order to do this, we can move the files to the mounted buckets and tell Rclone to sync the entire system while it is running.\nTo move new files to your Storj bucket, we can use the rclone move command like so:\nrclone move -P D:\\shows\\ Storj:media/shows/ --delete-empty-src-dirs --fast-list --drive-chunk-size=64M --max-backlog=999999 --transfers=8 --checkers=4 --no-traverse The flags in this command such as --transfers may require some tweaking to work best based on your available CPU and memory. But the command is essentially copying files from the local D:\\shows\\ directory to the media/shows/ folder in our Storj bucket.\nRefreshing the Local Sync Location #  Now that the files have been copied, we need to notify the mounted Rclone directory to refresh its contents. We can issue commands directly to the Rclone mount while it is running thanks to the --rc flag that was passed to it when we set up the mount. This is done by running rclone rc.\nSync your Rclone mount with this command:\nrclone rc vfs/refresh -v --fast-list recursive=true This recursively refreshes the root directory of the Rclone mount to pick up the newly uploaded files in Storj.\nTip: You can even run rclone move and rclone rc vfs/refresh on a script or job to automatically keep your multimedia service up-to-date.\nWrapping Up #  Now that you\u0026rsquo;ve set up Rclone to read and write to your Storj DCS buckets, you can configure Plex to use your mounted buckets as a storage location. Some tips for configuring Plex with this setup are:\n Under Settings \u0026gt; Library, uncheck \u0026ldquo;empty trash automatically after each scan\u0026rdquo; Check \u0026ldquo;use partial scan\u0026rdquo; so that Plex doesn\u0026rsquo;t scan the entire Storj mount every time new content is discovered  These, along with the other steps we took while setting up the mount, will help optimize Plex performance when reading from Storj.\nWith Plex configured to use Storj DCS for storage, you now have a convenient streaming and organization platform backed by reliable, decentralized object storage.\n"},{"id":171,"href":"/node/resources/faq/migrate-my-node/","title":"How do I migrate my node to a new device?","first":"/node/","section":"FAQ's","content":"How do I migrate my node to a new device? #  To migrate your Node to a new drive or computer, you first need to copy both the contents of your storage folder, as well as your identity folder to the new location.\nWindows Your default identity folder is located in: %APPDATA%\\Storj\\Identity\\storagenode\nYour default orders folder is located in \u0026quot;%ProgramFiles%\\Storj\\Storage Node\\orders\u0026quot;\nTo migrate your Windows storage node you can follow this guide: How to migrate the Windows GUI node from one physical location to another? The only difference - you do not need to share folders, since they are available locally, just use the local paths.\nAlso, you can enable WSL, install Ubuntu from the Microsoft store and use the Migrating with rsync guide. In this case your drives are mounted automatically. For example, D: disk will be mounted to the /mnt/d.\nTo migrate from the Docker installation to a Windows GUI, please, follow this guide: Migrating from Docker CLI to a GUI Install on Windows.\nTo migrate from the Windows GUI to the Docker installation, please, follow this guide: Migrating from Windows GUI installation to a Docker CLI.\nLinux Your default identity folder is located in: ~/.local/share/storj/identity/storagenode\nYour default orders folder is located in the data location.\n Migrating with rsync\nARM-based OS On Raspberry Pi, by default your identity folder is located in (the path may be different for other ARM platforms): /home/pi/.local/share/storj/identity/storagenode\nYour default orders folder is located in the data location.\n Migrating with rsync\nmacOS Your default identity folder is located in: \u0026quot;/Users/USER/Library/Application Support/Storj/identity/storagenode\u0026quot;\nYour default orders folder is located in the data location.\n Migrating with rsync\n Migrating with rsync #  We will assume that your parameters look like this:\n the source folder where the existing identity is located is /mnt/storj/identity/storagenode; the source folder where the existing stored data is located is /mnt/storj/storagenode/storage; the source folder where the existing orders folder is located is /mnt/storj/storagenode/orders; the destination folder the existing identity will be copied to is/mnt/storj2/storagenode-new/identity; the destination folder the existing stored data will be copied to is /mnt/storj2/storagenode-new/storage. the destination folder the existing orders will be copied to is /mnt/storj2/storagenode-new/orders.  To migrate your identity, orders and data to the new location, you can use the rsync command (please, replace the example paths mentioned above to your own!):\n Open a new terminal Keep your original storage node running Copy the identity:  rsync -aP /mnt/storj/identity/storagenode/ /mnt/storj2/storagenode-new/identity/ 4. Copy the orders\nrsync -aP /mnt/storj/storagenode/orders/ /mnt/storj2/storagenode-new/orders/ 5. Copy the data\nrsync -aP /mnt/storj/storagenode/storage/ /mnt/storj2/storagenode-new/storage/ 6. Repeat running the orders (step 4.) and data copying command (step 5.) a few more times until the difference would be negligible, then\n7. Stop the storage node (see How do I shutdown my node for system maintenance?)\n8. Remove the old container\ndocker rm storagenode 9. Run the copying command with a --delete parameter to remove deleted files from the destination:\nrsync -aP --delete /mnt/storj/storagenode/orders/ /mnt/storj2/storagenode-new/orders/ rsync -aP --delete /mnt/storj/storagenode/storage/ /mnt/storj2/storagenode-new/storage/ 10. Now you can copy config.yaml file and revocation.db to the new location:\ncp /mnt/storj/storagenode/config.yaml /mnt/storj2/storagenode-new/ cp /mnt/storj/storagenode/revocation.db /mnt/storj2/storagenode-new/revocation.db 11. After you copied over all the necessary files, update your --mount parameters in your docker run command. For our example, it will look like this (we only show a partial example of the new--mount parameter lines, not the entire docker run command!):\n--mount type=bind,source=/mnt/storj2/storagenode-new/identity,destination=/app/identity \\ --mount type=bind,source=/mnt/storj2/storagenode-new,destination=/app/config \\ The network-attached storage location could work, but it is neither supported nor recommended!  Please, note - we intentionally specified/mnt/storj2/storagenode-new as the data source in the --mount parameter and not /mnt/storj2/storagenode-new/storage because the storagenode docker container will add a subfolder calledstorage to the path automatically. So please, make sure that your data folder contains a storage subfolder with all the data inside (blobs folder, database files, etc.), otherwise the node will start from scratch since it can\u0026rsquo;t find the data in the right subfolder and will be disqualified in a few hours.  "},{"id":172,"href":"/dcs/how-tos/how-to-use-cyberduck-and-storj-dcs/","title":"How to use Cyberduck and Storj DCS","first":"/dcs/","section":"How To's","content":"How to use Cyberduck and Storj DCS #  Cyberduck is a free open-source libre server - a small server system which enables you to run your own internet services independently - cloud storage browser for macOS, Windows and Linux that supports FTP and SFTP, WebDAV, and cloud storage such as Storj DCS and other cloud storage providers.\nUsers can leverage the Cyberduck services via the user interface (GUI) or CLI (for Linux), including file transfer by drag and drop and notifications via Growl. It is also able to open some files in external text editors.\nUsers can choose Storj DCS to act as a decentralized cloud storage network target to send files to via the Cyberduck file manager interface, available via Storj\u0026rsquo;s hosted multitenant gateway ( Gateway MT) that is backward compatible with S3. This means you’ll be able to integrate with the Storj network via HTTP, and you won’t have to run anything extra on your end.\nIn this brief tutorial, we\u0026rsquo;ll go over downloading and setting up Cyberduck to integrate with Storj DCS, facilitating easy and intuitive drag-and-drop file transfer to Storj DCS.\nDownloading Cyberduck #  As a free solution, Cyberduck gives users the freedom to run, copy, distribute, study, change and improve the software. Those who wish to pay for Cyberduck will receive a registration key as a contributor. Becoming a contributor registers the installed application to your name, disabling donation prompts after downloading or updating.\nAs noted, Cyberduck supports Windows, macOS as well as Linux. Users can download Cyberduck by navigating to https://cyberduck.io/download/. Here, you can download the given installer for both Windows and macOS.\nFor those who wish to download via CLI:\nWindows #  GUI choco install cyberduck CLI choco install duck  Requires Chocolatey. See other installation options to download the MSI installer for Windows.\nmacOS #  brew install duck Requires Homebrew. See other installation options to download an OS X installer package.\nLinux #  RPM Package\necho -e \u0026#34;[duck-stable]\\nname=duck-stable\\nbaseurl=https://repo.cyberduck.io/stable/\\$basearch/\\nenabled=1\\ngpgcheck=0\u0026#34; | sudo tee /etc/yum.repos.d/duck-stable.repo sudo yum install duck Requires Yum Package Manager. See other installation options to download DEB and RPM packages.\nDEB Package\necho -e \u0026#34;deb https://s3.amazonaws.com/repo.deb.cyberduck.io stable main\u0026#34; | sudo tee /etc/apt/sources.list.d/cyberduck.list \u0026gt; /dev/null sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys FE7097963FEFBE72 sudo apt-get update sudo apt-get install duck Requires APT. See other installation options to download DEB and RPM packages.\nUsing CyberDuck with Storj - Windows #  Once the download is complete you\u0026rsquo;ll be able to open the CyberDuck client. By selecting Open Connection to the top left, you’ll be able to establish a connection via Cyberduck. By selecting Amazon S3 from the dropdown, you’ll be prompted to fill out the following:\n Server: Port: Access Key ID: Secret Access Key:  To configure Storj DCS as the decentralized cloud storage network target you’ll need to generate Storj credentials. Let’s take a look.\nUsing CyberDuck with Storj - macOS #  Once the download is complete you\u0026rsquo;ll be able to open the CyberDuck client. By selecting the + button in the bottom left-hand corner of the client, you\u0026rsquo;ll be able to add a connection bookmark, facilitating the connection between CyberDuck and Storj DCS. Select Amazon S3 from the drop-down.\nThis is where you will add Server, Access Key ID, and the Secret Access Key for Storj Gateway MT .\nTo configure Storj DCS as the decentralized cloud storage network target you’ll need to generate Storj credentials. Let’s take a look.\nGenerate Credentials to the Gateway MT #  One of the most versatile ways to get up and running with Storj DCS is through the hosted multi tenant S3 Gateway known as GatewayMT.\nGateway MT offers the following:\n Encryption, erasure coding, and upload to nodes occur server-side Supports parallelism for upload and multi transfer for download 1GB upload will result in 1GB of data being uploaded to storage nodes across the network Based on S3 standard  Navigate to the Access page within your project and then click on Create Access Grant +. A modal window will pop up where you should enter a name for this access grant.\nAssign the permissions you want this access grant to have, then click on Continue in Browser:\nIf you do not feel comfortable entering this sensitive information into your browser, we understand. Storj does not know or store your encryption passphrase. However, if you are still reluctant to enter your passphrase into our web application, please select “Continue in CLI” and follow these instructions instead.  Generate and Save the Encryption Passphrase. If this is your first access grant, we strongly encourage you to use a mnemonic phrase as your encryption passphrase (The GUI automatically generates one on the client-side for you if you choose \u0026ldquo;Generate Phrase.\u0026rdquo;) You will need this passphrase later if you want to again access files uploaded with this encryption phrase.\nBe sure to download the Access Grant to save it and then click on the Generate S3 Gateway Credentials link.\nNow click on the Generate Credentials button.\nCopy your Access Key, ** **Secret Key, and ** **Endpoint to a safe location.\nConfiguring Storj + Cyberduck #  Whether using Windows or macOS, you’ll simply add the Storj Gateway S3 credentials into the CyberDuck client to establish the connection. Click the Open Connection button to create a new connection.\n First start by selecting S3 from the drop-down menu Enter your S3 Gateway Credentials Endpoint for the Server selection (without https://) Enter your S3 Gateway Credentials Access Key into the Access Key ID selection Enter your S3 Gateway Credentials Secret Key into the Secret Access Key selection  Click Connect\nUse endpoint without https://, i.e. gateway.us1.storjshare.io in the Cyberduck Server entry above, otherwise Cyberduck will revert to WEBDAV (HTTPS) causing a connection error.\nAs seen here:\n For macOS:\nBack to the open connection in Cyberduck as we referenced above in Using CyberDuck with Storj - macOS you now have all the information you need to send files to your Storj DCS network.\n  select your saved bookmark Here, you\u0026rsquo;ll see the Amazon S3 server window reopen. To move forward, you\u0026rsquo;ll simply just add in your Storj Gateway S3 credentials that we previously configured.\n  Enter your S3 Gateway Credentials Endpoint for the Server selection\n  Enter your S3 Gateway Credentials Access Key into the Access Key ID selection\n  Enter your S3 Gateway Credentials Secret Key into the Secret Access Key selection\n  Close the modal window and click the modified bookmark.\nIf you’ve added in your S3 Gateway Credentials properly, you’ll see your Storj DCS buckets and you can now drag and drop files to your Storj DCS network seamlessly and easily via the Cyberduck GUI. Congrats!\n"},{"id":173,"href":"/dcs/how-tos/storj-ipfs-pinning-service-beta/","title":"Storj IPFS Pinning Service (Beta)","first":"/dcs/","section":"How To's","content":"Storj IPFS Pinning Service (Beta) #  Prerequisites #  You should have received an email that you have been invited to the beta which will include credentials to access the service. If you have not signed up yet, you can Join the beta.  The Storj IPFS Pinning service consists of an HTTP endpoint for uploading and pinning content, and an IPFS Gateway that serves the pinned content over IPFS and HTTP. Details on smart contract pinning will be made available in the future.\nHow to pin with Storj IPFS #  All content uploaded to the Storj IPFS service via the HTTP endpoint below is pinned. Examples are given in curl, but could be done from any programming language or with existing IPFS client bindings for a given programming language, such as the IPFS HTTP Client library for npm.\nHTTP Upload endpoint #  Uploading content follows the IPFS HTTP RPC for /api/v0/add with two small differences:\n The only optional argument supported is wrap-with-directory You must specify the credentials given when invited to participate in the beta as HTTP basic authentication.  This is not the same as your Storj DCS username and password. Do not use your Storj DCS username and password to try and use the IPFS Pinning Service.  Example using Curl #  For example, this is how it would work with curl and a file you wanted to pin called /path/file.extension. Please replace ipfs_beta_user and ipfs_beta_password with the beta credentials you received when accepted into the beta.\ncurl -u ipfs_beta_user:ipfs_beta_password -X POST -F file=@/path/file.extension https://www.storj-ipfs.com/api/v0/add The \u0026lsquo;@\u0026rsquo; before the file path is required for the upload to work properly. For example, if the file you wanted to upload was /home/hello/hi.jpg, the curl argument would be file=@/home/hello/hi.jpg.  How to retrieve pinned Objects #  Any content uploaded is automatically pinned and retrievable through any software that supports IPFS natively via its CID like IPFS Desktop or IPFS CLI. Some browsers like Brave include support, as well as some IPFS programs.\nFor those applications that do not support IPFS natively, you can use any public IPFS gateway, or the Storj IPFS Gateway as described below.\nHTTP via Storj IPFS Gateway #  For best performance, we have provided a Storj IPFS Gateway. This gateway will only host content pinned to Storj DCS, so it is not like other public IPFS gateways.\nYou can construct a link like this:\nhttps://www.storj-ipfs.com/ipfs/\u0026lt;cid\u0026gt; In cases where the gateway is unable to retrieve a given CID (e.g., returns a 404 not found error), please double check that you are using the correct CID and that it was uploaded to the Storj IPFS service.\n"},{"id":174,"href":"/node/resources/faq/remote-connection/","title":"What if I'm using a remote connection?","first":"/node/","section":"FAQ's","content":"What if I\u0026rsquo;m using a remote connection? #  If you must use a remote connection, due to the length of time it takes for some of the steps, it\u0026rsquo;s highly recommended to run them inside a virtual console like TMUX or SCREEN.\nIt is recommended to perform the next steps local to the machine, and not via a remote connection.\n"},{"id":175,"href":"/node/resources/faq/how-to-remote-access-the-web-dashboard/","title":"How to remote access the web dashboard","first":"/node/","section":"FAQ's","content":"How to remote access the web dashboard #  Enable the web dashboard for your setup #  Windows See Dashboard for Windows GUI or Dashboard for docker versionLinux See Dashboard for docker versionmacOS See Dashboard for docker version Install a ssh server on your PC with the storagenode #  Windows Get started with OpenSSHLinux sudo apt update \u0026amp;\u0026amp; sudo apt install ssh -y macOS How to Enable SSH on a Mac from the Command Line Install a ssh client on your device #  Windows Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0 Install OpenSSH using Windows Settings\nLinux sudo apt update \u0026amp;\u0026amp; sudo apt install ssh -y macOS How to Enable SSH on a Mac from the Command LineAndroid Install Termius SSH client from Google PlayiOS Install Termius SSH client from Apple Store Check connection #  Here we will use a user as user on the remote ssh server, and server as hostname or IP of the remote ssh server.\nWindows ssh user@server Password:\nuser@server:~$\nLinux ssh user@server Password:\nuser@server:~$\nmacOS ssh user@server Password:\nuser@server:~$\nAndroid Launch the Termius application\nNavigate to the Keychain via hamburger menu\nKeychain #  Tap on plus sign and select New Identity.\nNew Identity\nSpecify a name of the Identity in the field Name, provide a username for your PC with storagenode in the Username fieled, specify a password in the Password field, confirm creation.\nNavigate to the Hosts in the hamburger menu.\nHosts #  Create a new host with the plus sign.\nNew host\nSpecify alias for the host, i.e. MyHomePC. Provide the local or public hostname or IP address in the Hostname or IP Address field. Tap on Username icon and select the Identity, created earlier. In our example - user. Confirm creation of the host.\nTap on your host, the MyHomePC in our example, it should request a confirmation to add the fingerprint of the host to the list of known host on your smartphone.\nConfirm the connection with a tap on Connect button, the Terminal to your host should appear.\niOS We use the same application, so all steps are similar to Android version, please take a look on Android tab. Port forwarding #  Windows ssh -L 14002:localhost:14002 user@server Linux ssh -L 14002:localhost:14002 user@server macOS ssh -L 14002:localhost:14002 user@server Android Navigate to the Port forwarding screen in the hamburger menu.\nPort forwarding #  Tap on plus sign to create a new port forwarding rule.\nTap on Select host and select the host created earlier, in our case MyHomePC.\nSelect it and fill other parameters for the port forwarding rule:\n Port forward from with 14002 (dashboard port) Host to with 127.0.0.1 Port to with 14002 Address with 127.0.0.1  Confirm creation.\nCheck that it\u0026rsquo;s working - tap on your port forwarding rule, it should connect immediately.\nYou should see that connection is established and the button Close is appear.\nNow, open a web dashboard in your mobile browser: http://localhost:14002\nCongratulation!\niOS We use the same application, so all steps are similar to Android version, please take a look on Android tab. Navigate to http://localhost:14002 in your browser, you should see a web dashboard of your storagenode.\nThe connection can be established to your ssh server at that stage from the LAN, however, to publish your ssh server to the internet we need to secure it first. We should enable a key-only way to log in to your server. To be able to do so we need to generate and export your ssh public key to your ssh server before disabling the password login.\nGenerate ssh keys #  Windows ssh-keygen Linux ssh-keygen macOS ssh-keygen Android Navigate to the Keychain in the hamburger menu, select created Identity by tap on Identity icon then tap on pencil icon to edit an Identity, tap on Key icon, you will see a screen Select key.\nSelect key\nSelect existing key or tap on plus sign, select the option Generate key to generate a new key.\nGenerate Key\nSpecify a name for the key in the field Name, provide a passphrase (optional) in the field Passphrase and confirm.\nSelect the key to return to the edit of the Identity screen, confirm modification.\niOS We use the same application, so all steps are similar to Android version, please take a look on Android tab. Export public key from the ssh client to the ssh server #  Windows If the ssh server is a Windows machine, then you can use this guide: Deploying the public key. Otherwise, use the Powershell:\ncat ~/.ssh/id_rsa.pub | ssh user@server \u0026#34;umask 077; test -d .ssh || mkdir .ssh ; cat \u0026gt;\u0026gt; .ssh/authorized_keys\u0026#34; Linux ssh-copy-id -i ~/.ssh/id_rsa.pub user@server macOS ssh-copy-id -i ~/.ssh/id_rsa.pub user@server Android Select Keychain from the hamburger menu\nSelect the key by tap on key icon, tap on three vertical dots and select Export to host.\nTap on Select host and select the needed host, MyHomePC in our example\nConfirm the export to the selected host.\niOS We use the same application, so all steps are similar to Android version, please take a look on Android tab. Disable the password login on your ssh server with storagenode #  Windows We need to specify options PubkeyAuthentication yes and PasswordAuthentication no in the config file for the ssh daemon. So, return back to your server with storagenode.\nYou can use a ssh terminal to make these modifications, but be careful - if you have not added your key to the .ssh/authorized_keys file on your ssh server on previous steps, you will lose an access via ssh to your server.  Open the config file %programdata%\\ssh\\sshd_config with Notepad++ and set options PubkeyAuthentication yes and PasswordAuthentication no, save changes and restart the sshd service either from the Services applet or from the elevated Powershell:\nRestart-Service sshd Windows Configurations in sshd_config\nLinux Open the config file /etc/ssh/sshd_config with a text editor, for example nano, and set the PubkeyAuthentication yes and PasswordAuthentication no options, save the config file and restart the ssh daemon\nsudo service ssh restart macOS Edit the configuration file /private/etc/ssh/sshd_config with a plain text editor, for example - with nano, set the PubkeyAuthentication yes and PasswordAuthentication no options, save the config file and restart the ssh daemon\nsudo launchctl stop com.openssh.sshd sudo launchctl start com.openssh.sshd How do I configure SSH on OS X?\n Now check your connection: try to connect from your ssh client again, it should now use the ssh key for authentication instead of a password.\nTo add more security you can install applications such as fail2ban to your Linux or macOS server.\nNow, you can make a port forwarding rule on your router for the 22 TCP port (default ssh port) to your ssh server. For more security we recommend to forward an unusual port to the 22 port of the PC with ssh.\n"},{"id":176,"href":"/dcs/how-tos/simplify-file-management-with-s3-browser-and-storj-dcs/","title":"Simplify File Management with S3 Browser and Storj DCS","first":"/dcs/","section":"How To's","content":"Simplify File Management with S3 Browser and Storj DCS #  S3 Browser is a Windows-based client that provides simple and reliable file management for AWS S3 storage and AWS S3 compatible storage such as Storj DCS. Via the intuitive web file management interface, users can store and retrieve files from their Storj DCS bucket anytime and anywhere.\nWhile S3 Browser is free for personal use, users who wish to utilize the S3 Browser in commercial, business, government, or military institutions, or for any other profit activity, must purchase a pro license.\nKeep in mind that to get the best performance in using S3 Browser to manage your Storj buckets, you’ll want to apply some additional configurations to your S3 Browser instance.\nS3 Browser can be configured to download large files via multiple parallel threads. By default, S3 Browsers will download everything using 5MB chunks, whereas, you have the configuration option to increase that download size to 64MB, the segment size for Storj. We suggest configuring your S3 Bucket instance to Enable Multipart downloads with part size (in megabytes) of 64. You can find more on configuring this option here.\nOne license allows you to install one instance of S3 Browser on a single computer. Your license can be transferred if you change your PC. The license is a lifetime license and includes one year of free upgrades and support. Users are also limited to two accounts added within the free version of S3 Browser.  Downloading S3 Browser #  As noted, S3 Browser is only available for Windows, supporting Windows XP/Vista/7/8/10/11 and Windows Server 2003/2008/2012/2016/2019/2022.\nUsers can download the S3 Browser client by navigating to the S3 Browser homepage at https://s3browser.com/ and selecting the Download Now icon, or at https://s3browser.com/download.aspx.\nSome stats for the S3 Browser Download:\nS3 Browser Version 10.3.1\nSize of file: 5.37 MB (5 631 160 bytes)\nSHA256: 0b813e6f4d5cc9d2898fd9045f577d0f5e750dd960408abf3894b447033143e2\nOperating System: Windows XP/Vista/7/8/10/11 and Windows Server 2003/2008/2012/2016/2019/2022\nThere is no option to download S3 Browser via CLI  Generate Credentials to the Gateway MT #  Users interested in accessing their Storj DCS bucket(s) via S3 Browser can do so via the hosted AWS multitenant gateway known as Gateway MT. This backward-compatible hosted gateway is one of the most versatile ways to get up and running with Storj DCS when using platforms such as S3 Browser or other file manager platforms that support Storj DCS.\nGateway MT offers the following:\n Encryption, erasure coding, and upload to nodes occur server side Supports parallelism for upload and multi transfer for download A 1GB upload will result in 1GB of data being uploaded to storage nodes across the network based on the S3 standard  Navigate to the Access page within your project and then click on Create Access Grant +.\nOnce you’ve selected Create Access Grant, a new window will pop up where you can enter a name for this access grant.\nAssign the permissions you want this access grant to have, then click on Continue in Browser:\nIf you click Continue in Browser, our client-side javascript will finalize your access grant with your encryption passphrase. Your data will remain end-to-end encrypted until you explicitly register your access grant with Gateway MT for S3 compatibility. Only then will your access grant be shared with our servers.\nIf you already have created other access grants before, please enter the Encryption Passphrase you used for your other access grants in the Enter Phrase tab. If this is your first access grant, we strongly encourage you to use a mnemonic phrase as your encryption passphrase. The GUI will automatically generate one on the client-side for you if you open the Generate Phrase tab, as shown in the screenshot below. Be sure to save the encryption phrase in a safe place.\nTo generate S3 Gateway Credentials, click on the Generate S3 Gateway Credentials link:\nCopy and save the Access Grant in a safe place and then click on the Generate Credentials button.\nKeep your Access Key, Secret Key, and Endpoint in a safe location. We’ll be using this shortly!\nStorj does not know or store your encryption passphrase. However, if you are still reluctant to enter your passphrase into our web application, that’s completely understandable, and you should select Continue in CLI and follow these instructions.  Configuring Storj + S3 Browser #  Now that your S3 Browser client is downloaded and installed and you’ve generated and saved your S3 Gateway Credentials, it’s time to configure S3 Browser to interface with your Storj DCS bucket.\nSelect the Accounts menu item at the top left of the S3 Browser client. Select Add New Account. Add any name to your account in the Display Name selection. In the dropdown menu titled Account type select S3 Compatible Storage.\nIn the REST Endpoint section enter the S3 Gateway Credentials End Point without https:// and trailing /.\nIn the Access Key ID section enter the S3 Gateway Credentials Access Key.\nIn the Secret Access Key section enter the S3 Gateway Credentials Secret Key.\nOptionally, you’ll be able to protect your Access Keys with a master password by selecting the Encrypt Access Keys with a Password checkbox.\nCheck the box Use secure transfer (SSL/TSL) to secure all transfers via (SSL/TLS).\nFinallly, hit Connect.\nIf you’ve added in your S3 Gateway Credentials properly, you’ll see the following:\nUploading Files to Storj DCS Through S3 Browser #  Within the S3 Browser, you’ll be able to upload files directly to your Storj DCS bucket once you’ve effectively tied in StorJ DCS to S3 Browser.\nStart by selecting which Storj bucket you wish to upload data into by selecting the bucket at the top left. Once you’ve selected your bucket, select the Upload icon.\nHere, you’ll be prompted to select whether you’d like to upload file(s) or upload folder(s).\nFollowing a selection of upload file(s) or upload folder(s) you’ll be prompted with a file navigator to select the file or folder you wish to upload.\nDownloading Files From Storj DCS Through S3 Browser #  Within the S3 Browser, you’ll be able to download files directly from your Storj DCS bucket once you’ve effectively tied in StorJ DCS to S3 Browser.\nStart by selecting which Storj bucket you wish to download data from by selecting the bucket at the top left.\nOnce you’ve selected your bucket, select the Download icon.\nHere, you’ll be prompted to select whether you’d like to download file(s) or download folder(s)\nFollowing a selection of download file(s) or download folder(s) you’ll be prompted with a file navigator to select the file or folder you wish to download.\n"},{"id":177,"href":"/node/resources/faq/cli-on-windows-mac/","title":"What if I'm using the CLI Install on Windows / MacOS?","first":"/node/","section":"FAQ's","content":"What if I\u0026rsquo;m using the CLI Install on Windows / MacOS? #  Your Node may require extra monitoring. You may have to frequently restart Docker from the Docker desktop application when your Last Contact shown in the dashboard gets OFFLINE.\n"},{"id":178,"href":"/node/resources/faq/how-to-add-an-additional-drive/","title":"How to add an additional drive?","first":"/node/","section":"FAQ's","content":"How to add an additional drive? #  We recommend to run a new node, if you want to add an additional hard drive.\nAll nodes behind the same subnet /24 of a public IP are treated as a single node for ingress traffic and as separate ones for egress traffic (to the customers, repair and audit) - this is because we want to be decentralized as much as possible.\nAdding more drives/nodes will not increase the ingress traffic, only usage by customers can do this.\n You can, of course, use RAID instead, but this reduncancy not required for the network\u0026rsquo;s operation - the network itself has a built-in redundancy due to the usage of erasure codes: the customer needs only 29 pieces out of 80 to reconstruct the whole file.\nIf you would like to use RAID anyway, please note - your node will not receive more customer data only because you use RAID, this will not affect the node selection.\nUsing RAID0 (LVM, spanned drives, JBOD, etc.) is not recommended - with only one disk failure the whole node is lost.\nEven using RAID5/6 with today\u0026rsquo;s disks is too risky due to bit failure rate on disks with high capacity: High risk to lose a RAID5 volume during rebuild.\nYou can also read the discussion RAID vs No RAID on our forum.\nEach node must have its own generated unique identity signed with a new authorization token. Using a copy of the same identity but different tokens will result in disqualification as it is the same identity but with missing data.  General rules to run multiple nodes in the same network #  If you have only one external address, then you need to have a unique port for each storagenode. The default port for storagenode is 28967, protocols TCP+UDP.\n# the public address of the node, useful for nodes behind NAT contact.external-address: \u0026#34;my.ddns.tld:28967\u0026#34; If you want to add an additional new node, you can use the next free port or simply increase the previous used port by 1, for example - 28968.\nIf you run multiple nodes on the same device, you will need a unique port for the dashboard too. The default port for the dashboard is 14002 TCP.\n# server address of the api gateway and frontend app # console.address: 127.0.0.1:14002 You can use the same rule as for the node\u0026rsquo;s port to select any free port or simply increase the port number by 1, for example - 14003. Don\u0026rsquo;t forget to uncomment the parameter in your config (remove # with space after, each option should be started without spaces)!\nYou need to forward only the external node\u0026rsquo;s port. Please, do not forward the dashboard\u0026rsquo;s port - it has no protection and anyone on the internet can see your private information. See How to remote access the web dashboard guide to connect to your dashboard remotely.  If you run a binary version of storagenode ( Windows GUI for example) or a docker version with --network host option, you also need to have unique internal ports.\nFor the node\u0026rsquo;s CLI dashboard, the default internal address and port is 127.0.0.1:7778, it is specified in the server.private-address option.\n# private address to listen on server.private-address: 127.0.0.1:7778 For the second and any further nodes, you need to specify a unique port for each node there, too.\nThe next parameter to change the port is server.address, it contains an internal listen node\u0026rsquo;s address and port, specified as :28967 (this is equivalent to 0.0.0.0:28967, i.e. it will bind to any local IP) by default.\n# public address to listen on server.address: :28967 You need to make these changes for the second and any further additional nodes as well.\nForwarding options #  For example, you decided to configure your second node with port 28968. Then you can forward this port to the internal 28967 port, if the destination local IP is different from the first node. If both nodes are running on the same host, then you need to forward 28968 to 28968 and use this port in the contact.external-address: option of the config.yaml (in case of binary) or -e ADDRESS parameter of docker run in case of docker. If you use a binary version (or docker run with --network host), you also need to specify this port in the server.address: too.\nIn summary:\n you need to change the contact.external-address: option in config.yaml in case of binary or -e ADDRESS parameter of docker run in case of docker to use the external address and port, i.e. my.ddns.tld:28968. if the second node is running on another device, you can forward 28968 to 28967, nothing else is needed to change in the second node configuration except the external address above; if the second node is running on the same device as the first one, you need to forward 28968 to 28968, change the server.address: option in the config.yaml (in case of binary or docker run with --network host) or -p 28968:28967 parameter in docker run command (in case of docker without --network host).  See KB article Single and multi-node Port forwarding setup for details.\nSee Port Forwarding section for general port forwarding configuration.\n Docker version #  We assume that the second node would be run on the same device as a first one.\n Go to your router and forward port 28968 to 28968 and the IP of your device. docker run will contain:  docker run .... -p 28968:28967 -p 172.0.0.1:14003:14002 -e ADDRESS=my.ddns.tld:28968 .... Please, use the full command from CLI - run Storage Node, the above is an excerpt showing only the changed parts.\nHow do I change values like wallet address or storage capacity?  Binary version #  We assume that the second node would be run on the same device as the first one.\nThe Windows GUI installer can install only one node on the same Windows PC. For the second and next nodes we support only the docker version: CLI Install.\nThe alternative is to use the unofficial, Community supported Windows Toolbox made by @Vadim: https://forum.storj.io/t/win-gui-storj-node-toolbox/4381, it allows you to set up more than one Windows node on the same PC.\n  Go to your router and forward port 28968 to 28968 and IP of your device. Change parameters in your config.yaml (see Where can I find the config.yaml?)):  # the public address of the node, useful for nodes behind NAT contact.external-address: \u0026#34;my.ddns.tld:28968\u0026#34; # public address to listen on server.address: :28968 # private address to listen on server.private-address: 127.0.0.1:7779 # server address of the api gateway and frontend app console.address: 127.0.0.1:14003 3. Save the config.yaml and restart the storagenode service (see How do I shutdown my node for system maintenance?).\n"},{"id":179,"href":"/dcs/concepts/file-repair/","title":"File Repair","first":"/dcs/","section":"Concepts","content":"File Repair #  All of the nodes on the network are operated by third party volunteers. One key to the durability of the network is the distribution of data over a heterogeneous network of statistically uncorrelated nodes. Since the storage nodes are operated by different people, in different regions, on different power supplies, with different internet connections, etc., the risk of failure is highly distributed. While the risk of failure for any individual node is very low, the risk of 51 out of 80 nodes failing simultaneously is astronomically low.\nHard drives don’t last forever, and we expect and plan for nodes to fail, as well as to voluntarily leave the network. While we provide a way for nodes to leave the network via Graceful Exit, without impacting the availability of pieces, we also have tools to address situations where storage nodes fail or leave the network without triggering Graceful Exit. (Graceful Exit is a command that allows a Storage Node to upload the pieces it is holding peer-to-peer to other Storage Nodes. When that Storage Node then exits the network, it does so without loss of any pieces.\nIf a Storage Node does fail or leave the network without completing Graceful Exit, file availability isn’t impacted by the loss of one piece. You only need any 29 of the 80 pieces to retrieve a file. Over time, though, if enough nodes were to fail or leave the network without intervention, it’s possible that a file would eventually be lost. For this reason, we have a function that can rebuild missing pieces and store them on healthy storage nodes called File Repair.\nFile Repair works in conjunction with the Audit and Uptime Checker services that are constantly sampling the network to ensure that the network is monitoring the health and availability of Storage Nodes. The network constantly audits Storage Nodes by requesting a tiny byte range from a piece the node should be storing. If a Storage Node is healthy, it will be able to return the appropriate data. Even though the data is end-to-end encrypted and neither the Storage Node nor the Satellite have the encryption keys, the satellite can determine the validity of the response and determine if the node is storing the data it’s supposed to be storing. Other data elements (uptime, for example) are combined with audits to determine a Storage Node’s reputation, the statistical model by which nodes are allowed to operate or disqualified.\nAll of this analysis of Storage Nodes is fed into the data science engine which keeps track of all the objects on the network on a segment by segment basis. If, through the failure, loss or unavailability of Storage Nodes, the number of available pieces of a segment reaches the Repair Threshold, Repair Workers download 29 pieces of that object, re-encode the object, regenerate the missing pieces, then upload pieces to healthy storage nodes so that sufficient pieces are available to guarantee the availability of the object.\nThere are a number of great blog posts on the math behind the redundancy model using erasure codes and how the repair system works. Probably the most important takeaway is that erasure codes are an alternative to replication that deliver much higher durability at a much lower expansion factor. As a result, an approach whereby objects are erasure coded and distributed over dozens, hundreds, or thousands of endpoints will provide superior durability to making multiple replications in a small number of endpoint locations or using erasure codes in a single location. If you want the details, check out:\n  Replication is bad for decentralized storage, part 1: Erasure codes for fun and profit  Why (Proof-of-) Replication is Bad for Decentralized Storage, Part 2: Churn and Burn  Reputation Matters When it Comes to Storage Nodes  Decentralized Auditing and Repair! The Low-key Life of Data Resurrection  "},{"id":180,"href":"/dcs/how-tos/kubernetes-backup-via-velero/","title":"Kubernetes Backup via Velero","first":"/dcs/","section":"How To's","content":"Kubernetes Backup via Velero #  Background #  This guide walks through the process of backing up a Kubernetes cluster using Restic with Velero. Velero is a command-line tool that backs up Kubernetes clusters and restores them in case of loss. Velero includes a command-line interface and server-side component that runs inside of your Kubernetes cluster.\nVelero lets you:\n Take backups of your cluster and restore them in case of loss. Migrate cluster resources to other clusters. Replicate your production cluster to development and testing clusters.  The Storj DCS Velero plugin does not support volume snapshots for now.  Prerequisites #    Complete Velero Prerequisites and install the CLI  Create a Storj DCS account  Create an Access Grant for the project or create an access grant with the Uplink CLI  Create a Storj bucket where Velero will store the backups  Instructions #  Install Velero with Storj plugin\n$ velero install --provider tardigrade \\ --plugins storjlabs/velero-plugin:latest \\ --bucket $BUCKET \\ --backup-location-config accessGrant=$ACCESS \\ --no-secret To generate an access grant ($ACCESS) for the configuration, follow the guide here: Sharing Your First Object  Backup/Restore\nPerform a backup:\n$ velero backup create $BACKUP_NAME Perform a restore:\n$ velero restore create $RESTORE_NAME --from-backup $BACKUP_NAME Note: it\u0026rsquo;s possible to overwrite the backup location or access grant by editing the backupstoragelocations.velero.io CR and restarting the Velero Pod on the cluster in case of any mistake with the configuration.  The complete Velero documentation is located here.\nMulticloud Architecture for Disaster Prevention and Migration #  Because Storj DCS is a globally distributed hot object store, you can store and recover your Kubernetes volumes from anywhere in the world, instantly, without having to replicate data across regions. For DevOps managers, this can mean better resilience for your cluster, reduced global RTO (recovery time objective), cost-savings, and improved native security over centralized alternatives.\nIn the example below, we can see a Disaster Recovery scenario where we might need to migrate into a new cluster:\\\nIn this scenario, Cluster B is restored from the backup of A and reconstituting the Kubernetes volume in the cluster.\nGithub Source: #  https://github.com/storj/velero-plugin #  "},{"id":181,"href":"/dcs/api-reference/uplink-cli/share-command/","title":"share","first":"/dcs/","section":"Uplink CLI","content":"share #  Usage #  Windows ./uplink.exe share [ALLOWED_PATH_PREFIX]... [flags] Linux uplink share [ALLOWED_PATH_PREFIX]... [flags] macOS uplink share [ALLOWED_PATH_PREFIX]... [flags]  An access generated using uplink share with no arguments creates an access to your entire project with read permissions.  Flags #     Flag Description     --access string the serialized access, or name of the access to use   --auth-service string url for shared auth service\nAsia https://auth.ap1.storjshare.io\nUS https://auth.us1.storjshare.io\nEU https://auth.eu1.storjshare.io\n   --base-url string the base url for link sharing Asia https://link.ap1.storjshare.io\nUS https://link.us1.storjshare.io\nEU https://link.eu1.storjshare.io\n   --disallow-deletes if true, disallow deletes   --disallow-lists if true, disallow lists   --disallow-reads if true, disallow reads   --disallow-writes if true, disallow writes   --dns string specify your custom hostname. if set, returns dns settings for web hosting. implies --register and --public   --export-to string path to export the shared access to   --help, -h help for share   --not-after disallow access after this time (e.g. \u0026lsquo;+2h\u0026rsquo;, \u0026lsquo;2020-01-02T15:01:01-01:00\u0026rsquo;)   --not-before disallow access before this time (e.g. \u0026lsquo;+2h\u0026rsquo;, \u0026lsquo;2020-01-02T15:01:01-01:00\u0026rsquo;)   --public if true, the access will be public. --dnsand --urloverride this   --readonly implies --disallow-writes and --disallow-deletes   --register if true, creates and registers access grant   --url if true, returns a url for the shared path. implies --register and --public   --writeonly implies --disallow-reads and --disallow-lists    Examples #  Share a single object #  Windows ./uplink.exe share sj://cakes/cheesecake.jpg Linux uplink share sj://cakes/cheesecake.jpg macOS uplink share sj://cakes/cheesecake.jpg  Notice that by default, only download (read) and list operations are allowed.\n=========== ACCESS RESTRICTIONS ========================================================== Download : Allowed Upload : Disallowed Lists : Allowed Deletes : Disallowed NotBefore : No restriction NotAfter : No restriction Paths : sj://cakes/cheesecake.jpg =========== SERIALIZED ACCESS WITH THE ABOVE RESTRICTIONS TO SHARE WITH OTHERS =========== Access : 12yUGNqdsKX1Xky2qVoGwdpL... Share a bucket with all permissions #  Windows ./uplink.exe share sj://cakes/ --readonly=false Linux uplink share sj://cakes/ --readonly=false macOS uplink share sj://cakes/ --readonly=false  As the --readonly flag is set to false, uploads and deletes are allowed.\n=========== ACCESS RESTRICTIONS ========================================================== Download : Allowed Upload : Allowed Lists : Allowed Deletes : Allowed NotBefore : No restriction NotAfter : No restriction Paths : sj://cakes/ =========== SERIALIZED ACCESS WITH THE ABOVE RESTRICTIONS TO SHARE WITH OTHERS =========== Access : 12BncZWg9xc4GyXCgCi3YvBg... Register with Gateway MT #  Generate credentials to use with our S3 multitenant gateway (beta): GatewayMT\nWindows ./uplink.exe share sj://cakes/ --register --auth-service=https://auth.us1.storjshare.io Linux uplink share sj://cakes/ --register --auth-service=https://auth.us1.storjshare.io macOS uplink share sj://cakes/ --register --auth-service=https://auth.us1.storjshare.io  Notice the endpoint generated for Gateway MT gateway.us1.storjshare.io corresponds to the auth service used auth.us1.storjshare.io\n=========== ACCESS RESTRICTIONS ========================================================== Download : Allowed Upload : Disallowed Lists : Allowed Deletes : Disallowed NotBefore : No restriction NotAfter : No restriction Paths : sj://cakes/ (entire bucket) =========== SERIALIZED ACCESS WITH THE ABOVE RESTRICTIONS TO SHARE WITH OTHERS =========== Access : 1Q74vfxunqiAQ15WPxPqreya... ========== CREDENTIALS =================================================================== Access Key ID: jvguri... Secret Key : j3nj4x... Endpoint : https://gateway.us1.storjshare.io Link Sharing #  You can also generate a URL to share your projects/buckets/objects\nWindows ./uplink.exe share sj://cakes/ --url --not-after=none --base-url=https://link.us1.storjshare.io Linux uplink share sj://cakes/ --url --not-after=none --base-url=https://link.us1.storjshare.io macOS uplink share sj://cakes/ --url --not-after=none --base-url=https://link.us1.storjshare.io  Note that specifying --base-url is optional, but the --not-after is mandatory. If you do not want to specify date or offset - you can specify --not-after=none.\nSee flags of share command for details.\n =========== ACCESS RESTRICTIONS ========================================================== Download : Allowed Upload : Disallowed Lists : Allowed Deletes : Disallowed NotBefore : No restriction NotAfter : No restriction Paths : sj://cakes/ (entire bucket) =========== SERIALIZED ACCESS WITH THE ABOVE RESTRICTIONS TO SHARE WITH OTHERS =========== Access : 1Q74vfxunqiAQ15WPxPqreya... ========== CREDENTIALS =================================================================== Access Key ID: jvguri... Secret Key : j3nj4x... Endpoint : https://gateway.us1.storjshare.io Public Access: true =========== BROWSER URL ================================================================== REMINDER : Object key must end in \u0026#39;/\u0026#39; when trying to share recursively URL : https://link.us1.storjshare.io/s/jvguri.../cakes Also note that the URL uses the same Gateway MT access key, so if you have that already, you don\u0026rsquo;t necessarily need to run this command to generate a shareable link.\nhttps://link.\u0026lt;region\u0026gt;.storjshare.io/s/\u0026lt;access key\u0026gt;/\u0026lt;object path\u0026gt;\nTo download content directly, use /raw/ in Linkshare URL ex: https://link.us1.storjshare.io/raw/ju34skavohcqezr6vlfgshg5nmjq/dwebdemo/isthataquestion.mp4\nTo view the object location map, use /s/ in Linkshare URL ex: https://link.us1.storjshare.io/s/ju34skavohcqezr6vlfgshg5nmjq/dwebdemo/isthataquestion.mp4\nWebhosting #  For more detail, visit the documentation on hosting a static website with the Uplink CLI.\nWhile you may share individual objects with the above linksharing instructions, you must share a bucket or object prefix for webhosting. Your web address will render the index.html file.\nWindows ./uplink.exe share --dns www.mysite.com sj://cakes/ --base-url https://link.us1.storjshare.io Linux uplink share --dns www.mysite.com sj://cakes/ --base-url https://link.us1.storjshare.io macOS uplink share --dns www.mysite.com sj://cakes/ --base-url https://link.us1.storjshare.io  =========== ACCESS RESTRICTIONS ========================================================== Download : Allowed Upload : Disallowed Lists : Allowed Deletes : Disallowed NotBefore : No restriction NotAfter : No restriction Paths : sj://cakes/ (entire bucket) =========== SERIALIZED ACCESS WITH THE ABOVE RESTRICTIONS TO SHARE WITH OTHERS =========== Access : 12BncZWg9xc4GyXCDX73... ========== CREDENTIALS =================================================================== Access Key ID: ju3ga56lfk7x... Secret Key : j2psszecoqtc... Endpoint : https://gateway.us1.storjshare.io Public Access: true =========== DNS INFO ===================================================================== Remember to update the $ORIGIN with your domain name. You may also change the $TTL. $ORIGIN example.com. $TTL 3600 www.mysite.com IN\tCNAME\tlink.us1.storjshare.io. txt-www.mysite.com\tIN\tTXT storj-root:cakes txt-www.mysite.com\tIN\tTXT storj-access:ju3ga56lfk7x... Use the generated DNS info to connect your domain name to your shared objects.\nNote you can use any hostname in place of www.mysite.com in the example. The base-url is also optional.\n"},{"id":182,"href":"/node/resources/faq/where-can-i-find-a-config-yaml/","title":"Where can I find the config.yaml?","first":"/node/","section":"FAQ's","content":"Where can I find the config.yaml? #  Docker The config.yaml is created in your storage location when you did the initial setup of the storagenode.\nFor example, if your --mount option in your docker run command looks like --mount type=bind,source=/mnt/storj/storagenode,destination=/app/config, then the config.yaml will be in the /mnt/storj/storagenode location.  Options and parameters specified in the docker run command have a precedence over options in the config.yaml - Only options not also specified in the docker run command will be taken from the config.yamlfile.  Windows GUI By default, the config.yaml will be created in the \u0026quot;C:\\Program Files\\Storj\\Storage Node\\\u0026quot; folder. "},{"id":183,"href":"/dcs/resources/whitepaper/","title":"Whitepaper","first":"/dcs/","section":"Resources","content":"The whitepaper can be found here https://www.storj.io/storjv3.pdf\n"}]